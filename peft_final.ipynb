{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [],
      "dockerImageVersionId": 30747,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nnilayy/MedGPT/blob/main/peft_final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the read-only \"../input/\" directory\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n",
        "\n",
        "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n",
        "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "id": "hxPJLCbvlck4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install bitsandbytes evaluate datasets transformers peft"
      ],
      "metadata": {
        "trusted": true,
        "id": "wKualxMwlck5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "hugging_face_token = \"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\"\n",
        "notebook_login()"
      ],
      "metadata": {
        "trusted": true,
        "id": "wK-dPjCmlck6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "wandb_api_token = \"1a6a95ba4f084dedd64528953348896560a68bfe\"\n",
        "wandb.login(key = wandb_api_token)"
      ],
      "metadata": {
        "trusted": true,
        "id": "GXI9NxUPlck6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import (BertTokenizer,\n",
        "                          BitsAndBytesConfig,\n",
        "                          BertForSequenceClassification,\n",
        "                          Trainer,\n",
        "                          TrainingArguments,\n",
        "                          DataCollatorWithPadding,\n",
        "                          AutoModelForSequenceClassification,\n",
        "                          AutoTokenizer\n",
        "                         )\n",
        "from peft import (get_peft_model,\n",
        "                  LoraConfig,\n",
        "                  TaskType,\n",
        "                  prepare_model_for_kbit_training\n",
        "                 )\n",
        "from datasets import load_dataset\n",
        "import torch\n",
        "\n",
        "checkpoint = [\"bert-base-uncased\",\n",
        "             \"BioMistral/BioMistral-7B\",\n",
        "             \"\",\n",
        "             ]\n",
        "# bnb_config = BitsAndBytesConfig(load_in_8bit=True)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(checkpoint[0],\n",
        "#                                                            device_map=\"auto\",\n",
        "                                                           num_labels=2,\n",
        "#                                                            torch_dtype=torch.float16, #This reduces the gpu onboard vram usage\n",
        "#                                                            quantization_config = bnb_config,\n",
        "                                                      )\n",
        "\n",
        "peft_config = LoraConfig(\n",
        "                         inference_mode=False,\n",
        "                         r=16,\n",
        "                         lora_alpha = 32,\n",
        "                         lora_dropout = 0.1,\n",
        "                         bias=\"none\",\n",
        "                         peft_type = \"SEQ_CLS\",\n",
        "                         )\n",
        "\n",
        "# model = prepare_model_for_kbit_training(model)\n",
        "model = get_peft_model(model, peft_config)\n",
        "model.print_trainable_parameters()\n",
        "\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    logits = torch.from_numpy(logits)\n",
        "    labels = torch.from_numpy(labels)\n",
        "\n",
        "    predictions = torch.argmax(logits, dim=-1)\n",
        "    accuracy = (predictions == labels).float().mean()\n",
        "    return {'accuracy': accuracy.item()}\n",
        "\n",
        "# Preprocess the dataset\n",
        "def encode(examples):\n",
        "    outputs = tokenizer(examples['sentence1'], examples['sentence2'], truncation=True, padding='max_length', max_length=128)\n",
        "    outputs['labels'] = examples['label']\n",
        "    return outputs\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint[0])\n",
        "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
        "# model.config.pad_token_id = model.config.eos_token_id\n",
        "# tokenizer.pad_token = tokenizer.eos_token\n",
        "# Dataset\n",
        "dataset = load_dataset('glue', 'mrpc')\n",
        "dataset = dataset.map(encode, batched=True, num_proc=12)\n",
        "dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
        "label_names = dataset['train'].features['label'].names\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "\n",
        "# Training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    logging_dir='./logs_rslora',\n",
        "    # run_name='run_8',\n",
        "    do_train=True,\n",
        "    do_eval=True,\n",
        "    num_train_epochs=10,\n",
        "    learning_rate=2e-5,\n",
        "    logging_strategy='epoch',\n",
        "    per_device_train_batch_size=200,\n",
        "    per_device_eval_batch_size=200,\n",
        "    save_total_limit=1,\n",
        "    save_strategy=\"epoch\",\n",
        "    eval_strategy=\"epoch\",\n",
        "    label_names = [\"labels\"], #Without this Validation Accuracy and Validation Loss wouldn't be logged\n",
        "    fp16=True,\n",
        "#     fsdp=\"full_shard\"\n",
        ")\n",
        "\n",
        "# Initialize Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=dataset['train'],\n",
        "    eval_dataset=dataset['validation'],\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "trainer.train()\n",
        "trainer.evaluate()"
      ],
      "metadata": {
        "trusted": true,
        "id": "PXZNcSH5lck6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer,BitsAndBytesConfig,BertForSequenceClassification,Trainer,TrainingArguments,DataCollatorWithPadding,AutoModelForSequenceClassification,AutoTokenizer\n",
        "from peft import get_peft_model,LoraConfig,TaskType,prepare_model_for_kbit_training\n",
        "from datasets import load_dataset\n",
        "import torch\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "def main():\n",
        "    checkpoint = [\"bert-base-uncased\",\n",
        "                 \"BioMistral/BioMistral-7B\",\n",
        "                 \"\",\n",
        "                 ]\n",
        "    # bnb_config = BitsAndBytesConfig(load_in_8bit=True)\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(checkpoint[0],\n",
        "    #                                                            device_map=\"auto\",\n",
        "                                                               num_labels=2,\n",
        "    #                                                            torch_dtype=torch.float16, #This reduces the gpu onboard vram usage\n",
        "    #                                                            quantization_config = bnb_config,\n",
        "                                                          )\n",
        "\n",
        "    peft_config = LoraConfig(\n",
        "                             inference_mode=False,\n",
        "                             r=16,\n",
        "                             lora_alpha = 32,\n",
        "                             lora_dropout = 0.1,\n",
        "                             bias=\"none\",\n",
        "                             peft_type = \"SEQ_CLS\",\n",
        "                             )\n",
        "\n",
        "    # model = prepare_model_for_kbit_training(model)\n",
        "    model = get_peft_model(model, peft_config)\n",
        "    model.print_trainable_parameters()\n",
        "\n",
        "\n",
        "    def compute_metrics(eval_pred):\n",
        "        logits, labels = eval_pred\n",
        "        logits = torch.from_numpy(logits)\n",
        "        labels = torch.from_numpy(labels)\n",
        "\n",
        "        predictions = torch.argmax(logits, dim=-1)\n",
        "        accuracy = (predictions == labels).float().mean()\n",
        "        return {'accuracy': accuracy.item()}\n",
        "\n",
        "    # Preprocess the dataset\n",
        "    def encode(examples):\n",
        "        outputs = tokenizer(examples['sentence1'], examples['sentence2'], truncation=True, padding='max_length', max_length=128)\n",
        "        outputs['labels'] = examples['label']\n",
        "        return outputs\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(checkpoint[0])\n",
        "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
        "    # model.config.pad_token_id = model.config.eos_token_id\n",
        "    # tokenizer.pad_token = tokenizer.eos_token\n",
        "    # Dataset\n",
        "    dataset = load_dataset('glue', 'mrpc')\n",
        "    dataset = dataset.map(encode, batched=True, num_proc=12)\n",
        "    dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
        "    label_names = dataset['train'].features['label'].names\n",
        "    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "\n",
        "    # Training arguments\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir='./results',\n",
        "        logging_dir='./logs_rslora',\n",
        "        # run_name='run_8',\n",
        "        do_train=True,\n",
        "        do_eval=True,\n",
        "        num_train_epochs=10,\n",
        "        learning_rate=2e-5,\n",
        "        logging_strategy='epoch',\n",
        "        per_device_train_batch_size=200,\n",
        "        per_device_eval_batch_size=200,\n",
        "        save_total_limit=1,\n",
        "        save_strategy=\"epoch\",\n",
        "        eval_strategy=\"epoch\",\n",
        "        label_names = [\"labels\"], #Without this Validation Accuracy and Validation Loss wouldn't be logged\n",
        "        fp16=True,\n",
        "#         fsdp=\"full_shard\",\n",
        "#         fsdp_auto_wrap_policy=\"TRANSFORMER_BASED_WRAP\",\n",
        "#         fsdp_transformer_layer_cls_to_wrap = \"BertLayer\",\n",
        "    )\n",
        "\n",
        "    # Initialize Trainer\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=dataset['train'],\n",
        "        eval_dataset=dataset['validation'],\n",
        "        data_collator=data_collator,\n",
        "        compute_metrics=compute_metrics\n",
        "    )\n",
        "\n",
        "    # Train the model\n",
        "    trainer.train()\n",
        "    trainer.evaluate()\n",
        "if __name__ ==\"__main__\":\n",
        "    from accelerate import notebook_launcher\n",
        "    notebook_launcher(main, num_processes=2, mixed_precision=\"fp16\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "1RxON-hylck6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from accelerate import notebook_launcher\n",
        "notebook_launcher?\n",
        "# notebook_launcher(main, num_processes=2, mixed_precision=\"fp16\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-28T17:15:56.157901Z",
          "iopub.execute_input": "2024-07-28T17:15:56.158906Z",
          "iopub.status.idle": "2024-07-28T17:15:58.597521Z",
          "shell.execute_reply.started": "2024-07-28T17:15:56.158864Z",
          "shell.execute_reply": "2024-07-28T17:15:58.596144Z"
        },
        "trusted": true,
        "id": "jVrkXDCblck7",
        "outputId": "86643825-f108-4a7a-f4d3-4a791efffa0f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "\u001b[0;31mSignature:\u001b[0m\n\u001b[0mnotebook_launcher\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mfunction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mnum_processes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mmixed_precision\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'no'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0muse_port\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'29500'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mmaster_addr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'127.0.0.1'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mnode_rank\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mnum_nodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mrdzv_backend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'static'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mrdzv_endpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mrdzv_conf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mrdzv_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'none'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mmax_restarts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mmonitor_interval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mlog_line_prefix_template\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;31mDocstring:\u001b[0m\nLaunches a training function, using several processes or multiple nodes if it's possible in the current environment\n(TPU with multiple cores for instance).\n\n<Tip warning={true}>\n\nTo use this function absolutely zero calls to a CUDA device must be made in the notebook session before calling. If\nany have been made, you will need to restart the notebook and make sure no cells use any CUDA capability.\n\nSetting `ACCELERATE_DEBUG_MODE=\"1\"` in your environment will run a test before truly launching to ensure that none\nof those calls have been made.\n\n</Tip>\n\nArgs:\n    function (`Callable`):\n        The training function to execute. If it accepts arguments, the first argument should be the index of the\n        process run.\n    args (`Tuple`):\n        Tuple of arguments to pass to the function (it will receive `*args`).\n    num_processes (`int`, *optional*):\n        The number of processes to use for training. Will default to 8 in Colab/Kaggle if a TPU is available, to\n        the number of GPUs available otherwise.\n    mixed_precision (`str`, *optional*, defaults to `\"no\"`):\n        If `fp16` or `bf16`, will use mixed precision training on multi-GPU.\n    use_port (`str`, *optional*, defaults to `\"29500\"`):\n        The port to use to communicate between processes when launching a multi-GPU training.\n    master_addr (`str`, *optional*, defaults to `\"127.0.0.1\"`):\n        The address to use for communication between processes.\n    node_rank (`int`, *optional*, defaults to 0):\n        The rank of the current node.\n    num_nodes (`int`, *optional*, defaults to 1):\n        The number of nodes to use for training.\n    rdzv_backend (`str`, *optional*, defaults to `\"static\"`):\n        The rendezvous method to use, such as 'static' (the default) or 'c10d'\n    rdzv_endpoint (`str`, *optional*, defaults to `\"\"`):\n        The endpoint of the rdzv sync. storage.\n    rdzv_conf (`Dict`, *optional*, defaults to `None`):\n        Additional rendezvous configuration.\n    rdzv_id (`str`, *optional*, defaults to `\"none\"`):\n        The unique run id of the job.\n    max_restarts (`int`, *optional*, defaults to 0):\n        The maximum amount of restarts that elastic agent will conduct on workers before failure.\n    monitor_interval (`float`, *optional*, defaults to 0.1):\n        The interval in seconds that is used by the elastic_agent as a period of monitoring workers.\n    log_line_prefix_template (`str`, *optional*, defaults to `None`):\n        The prefix template for elastic launch logging. Available from PyTorch 2.2.0.\n\nExample:\n\n```python\n# Assume this is defined in a Jupyter Notebook on an instance with two GPUs\nfrom accelerate import notebook_launcher\n\n\ndef train(*args):\n    # Your training function here\n    ...\n\n\nnotebook_launcher(train, args=(arg1, arg2), num_processes=2, mixed_precision=\"fp16\")\n```\n\u001b[0;31mFile:\u001b[0m      /opt/conda/lib/python3.10/site-packages/accelerate/launchers.py\n\u001b[0;31mType:\u001b[0m      function"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "TrainingArguments?"
      ],
      "metadata": {
        "trusted": true,
        "id": "yhjuBI60lck7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loads Model on CPUs RAM\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "model = AutoModelForCausalLM.from_pretrained(\"bigscience/bloom-3b\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bigscience/bloom-3b\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "YcIEWS9rlck7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "![image.png](attachment:7fd8447b-0b84-4e61-b299-d8e3e6445655.png)"
      ],
      "metadata": {
        "id": "sbryBN90lck8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loads Model on 1 GPU Vram\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "model = AutoModelForCausalLM.from_pretrained(\"bigscience/bloom-3b\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bigscience/bloom-3b\")\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)"
      ],
      "metadata": {
        "trusted": true,
        "id": "L3g6TED8lck8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loads Model on 1 GPU Vram, in 16 precision\n",
        "# Reduce memory footprint by half\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "model = AutoModelForCausalLM.from_pretrained(\"bigscience/bloom-3b\", torch_dtype=torch.float16)\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bigscience/bloom-3b\")\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)"
      ],
      "metadata": {
        "trusted": true,
        "id": "BDKogk0rlck8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "![image.png](attachment:dc397578-5333-4d21-aebd-35f81079b26d.png)"
      ],
      "metadata": {
        "id": "-s8MAi0flck9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM\n",
        "torch.set_default_dtype(torch.float16)\n",
        "if torch.cuda.is_available():\n",
        "    torch.set_default_device('cuda:1')\n",
        "model = AutoModelForCausalLM.from_pretrained(\"bigscience/bloom-3b\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "hi-vv9Z_lck9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "![image.png](attachment:286a2579-627c-4800-8bea-2ce4ba70f1f1.png)"
      ],
      "metadata": {
        "id": "KanPVDD6lck9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Parallelism: Same Model Gets Loaded On One GPU\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\"bigscience/bloom-3b\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bigscience/bloom-3b\")\n",
        "model = torch.nn.DataParallel(model)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "BB5tg3c4lck9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\"bigscience/bloom-3b\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bigscience/bloom-3b\")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    # Use all available GPUs\n",
        "    device_ids = list(range(torch.cuda.device_count()))\n",
        "    model = torch.nn.DataParallel(model, device_ids=device_ids)\n",
        "    model.to('cuda')  # Move model to the default device\n",
        "else:\n",
        "    print(\"CUDA is not available.\")\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "dyiZ2WlMlck9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "![image.png](attachment:62708457-46e7-4840-94be-ad0da40584e7.png)"
      ],
      "metadata": {
        "id": "asEesmrmlck9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "dataset = load_dataset('nnilayy/pubmedqa_artificial_128')"
      ],
      "metadata": {
        "trusted": true,
        "id": "orDbiRMjlck9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_content=\"\"\"\n",
        "from transformers import BertTokenizer,BitsAndBytesConfig,BertForSequenceClassification,Trainer,TrainingArguments,DataCollatorWithPadding,AutoModelForSequenceClassification,AutoTokenizer\n",
        "from peft import get_peft_model,LoraConfig,TaskType,prepare_model_for_kbit_training\n",
        "from datasets import load_dataset\n",
        "import torch\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "def main():\n",
        "    checkpoint = [\"bert-base-uncased\",\n",
        "                 \"BioMistral/BioMistral-7B\",\n",
        "                 \"\",\n",
        "                 ]\n",
        "    # bnb_config = BitsAndBytesConfig(load_in_8bit=True)\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(checkpoint[0],\n",
        "    #                                                            device_map=\"auto\",\n",
        "                                                               num_labels=2,\n",
        "    #                                                            torch_dtype=torch.float16, #This reduces the gpu onboard vram usage\n",
        "    #                                                            quantization_config = bnb_config,\n",
        "                                                          )\n",
        "\n",
        "    peft_config = LoraConfig(\n",
        "                             inference_mode=False,\n",
        "                             r=16,\n",
        "                             lora_alpha = 32,\n",
        "                             lora_dropout = 0.1,\n",
        "                             bias=\"none\",\n",
        "                             peft_type = \"SEQ_CLS\",\n",
        "                             )\n",
        "\n",
        "    # model = prepare_model_for_kbit_training(model)\n",
        "    model = get_peft_model(model, peft_config)\n",
        "    model.print_trainable_parameters()\n",
        "\n",
        "\n",
        "    def compute_metrics(eval_pred):\n",
        "        logits, labels = eval_pred\n",
        "        logits = torch.from_numpy(logits)\n",
        "        labels = torch.from_numpy(labels)\n",
        "\n",
        "        predictions = torch.argmax(logits, dim=-1)\n",
        "        accuracy = (predictions == labels).float().mean()\n",
        "        return {'accuracy': accuracy.item()}\n",
        "\n",
        "    # Preprocess the dataset\n",
        "    def encode(examples):\n",
        "        outputs = tokenizer(examples['sentence1'], examples['sentence2'], truncation=True, padding='max_length', max_length=128)\n",
        "        outputs['labels'] = examples['label']\n",
        "        return outputs\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(checkpoint[0])\n",
        "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
        "    # model.config.pad_token_id = model.config.eos_token_id\n",
        "    # tokenizer.pad_token = tokenizer.eos_token\n",
        "    # Dataset\n",
        "    dataset = load_dataset('glue', 'mrpc')\n",
        "    dataset = dataset.map(encode, batched=True, num_proc=12)\n",
        "    dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
        "    label_names = dataset['train'].features['label'].names\n",
        "    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "\n",
        "    # Training arguments\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir='./results',\n",
        "        logging_dir='./logs_rslora',\n",
        "        # run_name='run_8',\n",
        "        do_train=True,\n",
        "        do_eval=True,\n",
        "        num_train_epochs=10,\n",
        "        learning_rate=2e-5,\n",
        "        logging_strategy='epoch',\n",
        "        per_device_train_batch_size=200,\n",
        "        per_device_eval_batch_size=200,\n",
        "        save_total_limit=1,\n",
        "        save_strategy=\"epoch\",\n",
        "        eval_strategy=\"epoch\",\n",
        "        label_names = [\"labels\"], #Without this Validation Accuracy and Validation Loss wouldn't be logged\n",
        "        fp16=True,\n",
        "#         fsdp=\"full_shard\",\n",
        "#         fsdp_auto_wrap_policy=\"TRANSFORMER_BASED_WRAP\",\n",
        "#         fsdp_transformer_layer_cls_to_wrap = \"BertLayer\",\n",
        "    )\n",
        "\n",
        "    # Initialize Trainer\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=dataset['train'],\n",
        "        eval_dataset=dataset['validation'],\n",
        "        data_collator=data_collator,\n",
        "        compute_metrics=compute_metrics\n",
        "    )\n",
        "\n",
        "    # Train the model\n",
        "    trainer.train()\n",
        "    trainer.evaluate()\n",
        "if __name__ ==\"__main__\":\n",
        "    main()\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "# Create and write to the file in the /kaggle/working/ directory\n",
        "file_path = \"/kaggle/working/trainer.py\"\n",
        "with open(file_path, \"w\") as file:\n",
        "    file.write(file_content)\n",
        "\n",
        "print(\"File created successfully in /kaggle/working/\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-28T17:12:55.259289Z",
          "iopub.execute_input": "2024-07-28T17:12:55.259782Z",
          "iopub.status.idle": "2024-07-28T17:12:55.277199Z",
          "shell.execute_reply.started": "2024-07-28T17:12:55.259750Z",
          "shell.execute_reply": "2024-07-28T17:12:55.276005Z"
        },
        "trusted": true,
        "id": "bUvrSYWflck9",
        "outputId": "eb9bc5e4-da96-4451-a4f5-6535f13c9c2f"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "File created successfully in /kaggle/working/\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !python -m torch.distributed.launch --nproc_per_node=2 trainer.py\n",
        "# !torchrun --nproc_per_node=2 load_ddp_model.py\n",
        "# !accelerate launch --multi_gpu --mixed_precision=\"fp16\" --num_processes=2 trainer.py\n",
        "!python trainer.py"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-28T17:20:03.685843Z",
          "iopub.execute_input": "2024-07-28T17:20:03.686234Z",
          "iopub.status.idle": "2024-07-28T17:21:04.551928Z",
          "shell.execute_reply.started": "2024-07-28T17:20:03.686200Z",
          "shell.execute_reply": "2024-07-28T17:21:04.550725Z"
        },
        "trusted": true,
        "id": "2NoW94eqlck-",
        "outputId": "bd9e60da-2d18-479a-b91c-ae90b8b1b7e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "2024-07-28 17:20:07.650981: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-07-28 17:20:07.651038: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-07-28 17:20:07.652506: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\ntrainable params: 589,824 || all params: 110,073,602 || trainable%: 0.5358\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mnnilayy\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.17.5 is available!  To upgrade, please run:\n\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.17.4\n\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/wandb/run-20240728_172017-itp1g15m\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33m./results\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/nnilayy/huggingface\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/nnilayy/huggingface/runs/itp1g15m\u001b[0m\n{'loss': 0.6961, 'grad_norm': 9914.470703125, 'learning_rate': 1.8e-05, 'epoch': 1.0}\n 10%|‚ñà‚ñà‚ñà‚ñà‚ñè                                     | 10/100 [00:26<03:09,  2.10s/it]\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 0.6876705288887024, 'eval_accuracy': 0.5465686321258545, 'eval_runtime': 1.4795, 'eval_samples_per_second': 275.773, 'eval_steps_per_second': 1.352, 'epoch': 1.0}\n 10%|‚ñà‚ñà‚ñà‚ñà‚ñè                                     | 10/100 [00:28<03:09,  2.10s/it]\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:00<00:00, 21.08it/s]\u001b[A\n                                                                                \u001b[A^C\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import gc\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()"
      ],
      "metadata": {
        "trusted": true,
        "id": "LgxTfKd-lck-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}