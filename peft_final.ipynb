{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [],
      "dockerImageVersionId": 30747,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nnilayy/MedGPT/blob/main/peft_final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the read-only \"../input/\" directory\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n",
        "\n",
        "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n",
        "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "id": "YEohkbzxbpk4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install bitsandbytes evaluate datasets transformers peft"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-27T16:54:41.964906Z",
          "iopub.execute_input": "2024-07-27T16:54:41.965258Z",
          "iopub.status.idle": "2024-07-27T16:55:01.813958Z",
          "shell.execute_reply.started": "2024-07-27T16:54:41.965230Z",
          "shell.execute_reply": "2024-07-27T16:55:01.813059Z"
        },
        "trusted": true,
        "id": "YcoxB0Qibpk5",
        "outputId": "20cfd0a8-4291-44e9-ae2e-4a735276899a"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Collecting bitsandbytes\n  Downloading bitsandbytes-0.43.2-py3-none-manylinux_2_24_x86_64.whl.metadata (3.5 kB)\nCollecting evaluate\n  Downloading evaluate-0.4.2-py3-none-any.whl.metadata (9.3 kB)\nRequirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (2.20.0)\nRequirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.42.3)\nCollecting peft\n  Downloading peft-0.12.0-py3-none-any.whl.metadata (13 kB)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from bitsandbytes) (2.1.2)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from bitsandbytes) (1.26.4)\nRequirement already satisfied: dill in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.2.2)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.32.3)\nRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from evaluate) (4.66.4)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from evaluate) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.70.16)\nRequirement already satisfied: fsspec>=2021.05.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.5.0)\nRequirement already satisfied: huggingface-hub>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.23.4)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from evaluate) (21.3)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets) (3.13.1)\nRequirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (16.1.0)\nRequirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.10/site-packages (from datasets) (0.6)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.9.1)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets) (6.0.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.12.25)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.3)\nRequirement already satisfied: tokenizers<0.20,>=0.19 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.19.1)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from peft) (5.9.3)\nRequirement already satisfied: accelerate>=0.21.0 in /opt/conda/lib/python3.10/site-packages (from peft) (0.32.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.3)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.7.0->evaluate) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->evaluate) (3.1.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (2024.7.4)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (1.13.0)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.1.2)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2023.4)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->bitsandbytes) (2.1.3)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->bitsandbytes) (1.3.0)\nDownloading bitsandbytes-0.43.2-py3-none-manylinux_2_24_x86_64.whl (137.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.5/137.5 MB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading evaluate-0.4.2-py3-none-any.whl (84 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading peft-0.12.0-py3-none-any.whl (296 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m296.4/296.4 kB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: bitsandbytes, peft, evaluate\nSuccessfully installed bitsandbytes-0.43.2 evaluate-0.4.2 peft-0.12.0\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "hugging_face_token = \"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\"\n",
        "notebook_login()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-27T16:55:14.438943Z",
          "iopub.execute_input": "2024-07-27T16:55:14.439867Z",
          "iopub.status.idle": "2024-07-27T16:55:14.733925Z",
          "shell.execute_reply.started": "2024-07-27T16:55:14.439828Z",
          "shell.execute_reply": "2024-07-27T16:55:14.733077Z"
        },
        "trusted": true,
        "id": "dyrWlyE6bpk6",
        "outputId": "562268ee-e359-4b13-e00d-bc98c503001c",
        "colab": {
          "referenced_widgets": [
            "e2dd61cc5a5943d6b649114a9a4edae3"
          ]
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e2dd61cc5a5943d6b649114a9a4edae3"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "wandb_api_token = \"1a6a95ba4f084dedd64528953348896560a68bfe\"\n",
        "wandb.login(key = wandb_api_token)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-27T16:55:22.351324Z",
          "iopub.execute_input": "2024-07-27T16:55:22.351675Z",
          "iopub.status.idle": "2024-07-27T16:55:25.071427Z",
          "shell.execute_reply.started": "2024-07-27T16:55:22.351648Z",
          "shell.execute_reply": "2024-07-27T16:55:25.070499Z"
        },
        "trusted": true,
        "id": "RAmEcf9hbpk7",
        "outputId": "f29b56ab-3449-44d1-8fea-4720eb70a06b"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "text": "\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
          "output_type": "stream"
        },
        {
          "execution_count": 3,
          "output_type": "execute_result",
          "data": {
            "text/plain": "True"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import (BertTokenizer,\n",
        "                          BitsAndBytesConfig,\n",
        "                          BertForSequenceClassification,\n",
        "                          Trainer,\n",
        "                          TrainingArguments,\n",
        "                          DataCollatorWithPadding,\n",
        "                          AutoModelForSequenceClassification,\n",
        "                          AutoTokenizer\n",
        "                         )\n",
        "from peft import (get_peft_model,\n",
        "                  LoraConfig,\n",
        "                  TaskType,\n",
        "                  prepare_model_for_kbit_training\n",
        "                 )\n",
        "from datasets import load_dataset\n",
        "import torch\n",
        "\n",
        "checkpoint = [\"bert-base-uncased\",\n",
        "             \"BioMistral/BioMistral-7B\",\n",
        "             \"\",\n",
        "             ]\n",
        "# bnb_config = BitsAndBytesConfig(load_in_8bit=True)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(checkpoint[1],\n",
        "                                                           device_map=\"auto\",\n",
        "                                                           num_labels=2,\n",
        "                                                           torch_dtype=torch.float16, #This reduces the gpu onboard vram usage\n",
        "#                                                            quantization_config = bnb_config,\n",
        "                                                      )\n",
        "\n",
        "peft_config = LoraConfig(\n",
        "                         inference_mode=False,\n",
        "                         r=16,\n",
        "                         lora_alpha = 32,\n",
        "                         lora_dropout = 0.1,\n",
        "                         bias=\"none\",\n",
        "                         peft_type = \"SEQ_CLS\",\n",
        "                         )\n",
        "\n",
        "# model = prepare_model_for_kbit_training(model)\n",
        "model = get_peft_model(model, peft_config)\n",
        "model.print_trainable_parameters()\n",
        "\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    logits = torch.from_numpy(logits)\n",
        "    labels = torch.from_numpy(labels)\n",
        "\n",
        "    predictions = torch.argmax(logits, dim=-1)\n",
        "    accuracy = (predictions == labels).float().mean()\n",
        "    return {'accuracy': accuracy.item()}\n",
        "\n",
        "# Preprocess the dataset\n",
        "def encode(examples):\n",
        "    outputs = tokenizer(examples['sentence1'], examples['sentence2'], truncation=True, padding='max_length', max_length=128)\n",
        "    outputs['labels'] = examples['label']\n",
        "    return outputs\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint[1])\n",
        "model.config.pad_token_id = model.config.eos_token_id\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "# Dataset\n",
        "dataset = load_dataset('glue', 'mrpc')\n",
        "dataset = dataset.map(encode, batched=True, num_proc=12)\n",
        "dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
        "label_names = dataset['train'].features['label'].names\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "\n",
        "# Training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    logging_dir='./logs_rslora',\n",
        "    # run_name='run_8',\n",
        "    do_train=True,\n",
        "    do_eval=True,\n",
        "    num_train_epochs=10,\n",
        "    learning_rate=2e-5,\n",
        "    logging_strategy='epoch',\n",
        "    per_device_train_batch_size=4,\n",
        "    per_device_eval_batch_size=4,\n",
        "    save_total_limit=1,\n",
        "    save_strategy=\"epoch\",\n",
        "    eval_strategy=\"epoch\",\n",
        "    label_names = [\"labels\"], #Without this Validation Accuracy and Validation Loss wouldn't be logged\n",
        "    fp16=True,\n",
        "    fsdp=\"full_shard\"\n",
        ")\n",
        "\n",
        "# Initialize Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=dataset['train'],\n",
        "    eval_dataset=dataset['validation'],\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "trainer.train()\n",
        "trainer.evaluate()"
      ],
      "metadata": {
        "trusted": true,
        "id": "33goKfg5bpk7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    from transformers import BertTokenizer,BitsAndBytesConfig,BertForSequenceClassification,Trainer,TrainingArguments,DataCollatorWithPadding,AutoModelForSequenceClassification,AutoTokenizer\n",
        "    from peft import get_peft_model,LoraConfig,TaskType,prepare_model_for_kbit_training\n",
        "    from datasets import load_dataset\n",
        "    import torch\n",
        "    import warnings\n",
        "    warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "    checkpoint = [\"bert-base-uncased\",\n",
        "                 \"BioMistral/BioMistral-7B\",\n",
        "                 \"\",\n",
        "                 ]\n",
        "    # bnb_config = BitsAndBytesConfig(load_in_8bit=True)\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(checkpoint[1],\n",
        "                                                               num_labels=2,\n",
        "                                                               torch_dtype=torch.float16, #This reduces the gpu onboard vram usage\n",
        "    #                                                            quantization_config = bnb_config,\n",
        "                                                          )\n",
        "\n",
        "    peft_config = LoraConfig(\n",
        "                             inference_mode=False,\n",
        "                             r=16,\n",
        "                             lora_alpha = 32,\n",
        "                             lora_dropout = 0.1,\n",
        "                             bias=\"none\",\n",
        "                             peft_type = \"SEQ_CLS\",\n",
        "                             )\n",
        "\n",
        "    # model = prepare_model_for_kbit_training(model)\n",
        "    model = get_peft_model(model, peft_config)\n",
        "    model.print_trainable_parameters()\n",
        "\n",
        "    def compute_metrics(eval_pred):\n",
        "        logits, labels = eval_pred\n",
        "        logits = torch.from_numpy(logits)\n",
        "        labels = torch.from_numpy(labels)\n",
        "\n",
        "        predictions = torch.argmax(logits, dim=-1)\n",
        "        accuracy = (predictions == labels).float().mean()\n",
        "        return {'accuracy': accuracy.item()}\n",
        "\n",
        "    # Preprocess the dataset\n",
        "    def encode(examples):\n",
        "        outputs = tokenizer(examples['sentence1'], examples['sentence2'], truncation=True, padding='max_length', max_length=128)\n",
        "        outputs['labels'] = examples['label']\n",
        "        return outputs\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(checkpoint[1])\n",
        "    model.config.pad_token_id = model.config.eos_token_id\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    # Dataset\n",
        "    dataset = load_dataset('glue', 'mrpc')\n",
        "    dataset = dataset.map(encode, batched=True, num_proc=12)\n",
        "    dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
        "    label_names = dataset['train'].features['label'].names\n",
        "    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "\n",
        "    # Training arguments\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir='./results',\n",
        "        logging_dir='./logs_rslora',\n",
        "        # run_name='run_8',\n",
        "        do_train=True,\n",
        "        do_eval=True,\n",
        "        num_train_epochs=10,\n",
        "        learning_rate=2e-5,\n",
        "        logging_strategy='epoch',\n",
        "        per_device_train_batch_size=4,\n",
        "        per_device_eval_batch_size=4,\n",
        "        save_total_limit=1,\n",
        "        save_strategy=\"epoch\",\n",
        "        eval_strategy=\"epoch\",\n",
        "        label_names = [\"labels\"], #Without this Validation Accuracy and Validation Loss wouldn't be logged\n",
        "        fp16=True,\n",
        "        fsdp=\"full_shard\"\n",
        "    )\n",
        "\n",
        "    # Initialize Trainer\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=dataset['train'],\n",
        "        eval_dataset=dataset['validation'],\n",
        "        data_collator=data_collator,\n",
        "        compute_metrics=compute_metrics\n",
        "    )\n",
        "\n",
        "    # Train the model\n",
        "    trainer.train()\n",
        "    trainer.evaluate()\n",
        "if __name__ ==\"__main__\":\n",
        "    from accelerate import notebook_launcher\n",
        "    notebook_launcher(main, num_processes=2, mixed_precision=\"fp16\")\n",
        ""
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-27T16:55:46.824378Z",
          "iopub.execute_input": "2024-07-27T16:55:46.824921Z",
          "iopub.status.idle": "2024-07-27T16:58:08.643626Z",
          "shell.execute_reply.started": "2024-07-27T16:55:46.824887Z",
          "shell.execute_reply": "2024-07-27T16:58:08.640773Z"
        },
        "trusted": true,
        "id": "GlvtPpCqbpk7",
        "outputId": "b961fcb5-34f1-45cb-e6c4-5bfd2efe6366",
        "colab": {
          "referenced_widgets": [
            "b6e03f002b584213a53114b50a1713d8",
            "a14e44482e244d2494ceaa598927cc5c",
            "121c321e4ab34e1a9b20d7be66060556",
            "c7d27fa9ba4448ce93ce12770cefc5cd"
          ]
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Launching training on 2 GPUs.\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "2024-07-27 16:55:53.080282: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-07-27 16:55:53.080280: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-07-27 16:55:53.080351: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-07-27 16:55:53.080383: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-07-27 16:55:53.217402: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n2024-07-27 16:55:53.217395: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nSome weights of MistralForSequenceClassification were not initialized from the model checkpoint at BioMistral/BioMistral-7B and are newly initialized: ['score.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nSome weights of MistralForSequenceClassification were not initialized from the model checkpoint at BioMistral/BioMistral-7B and are newly initialized: ['score.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "trainable params: 6,815,744 || all params: 7,117,484,032 || trainable%: 0.0958\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "tokenizer_config.json:   0%|          | 0.00/1.42k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b6e03f002b584213a53114b50a1713d8"
            }
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "trainable params: 6,815,744 || all params: 7,117,484,032 || trainable%: 0.0958\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a14e44482e244d2494ceaa598927cc5c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "tokenizer.json:   0%|          | 0.00/1.80M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "121c321e4ab34e1a9b20d7be66060556"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "special_tokens_map.json:   0%|          | 0.00/72.0 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c7d27fa9ba4448ce93ce12770cefc5cd"
            }
          },
          "metadata": {}
        },
        {
          "name": "stderr",
          "text": "[2024-07-27 16:58:08,213] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 0 (pid: 108) of fn: main (start_method: fork)\n",
          "output_type": "stream"
        },
        {
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mChildFailedError\u001b[0m                          Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[4], line 93\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     92\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01maccelerate\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m notebook_launcher\n\u001b[0;32m---> 93\u001b[0m     \u001b[43mnotebook_launcher\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_processes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmixed_precision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfp16\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/accelerate/launchers.py:245\u001b[0m, in \u001b[0;36mnotebook_launcher\u001b[0;34m(function, args, num_processes, mixed_precision, use_port, master_addr, node_rank, num_nodes, rdzv_backend, rdzv_endpoint, rdzv_conf, rdzv_id, max_restarts, monitor_interval, log_line_prefix_template)\u001b[0m\n\u001b[1;32m    243\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_torch_version(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m>=\u001b[39m\u001b[38;5;124m\"\u001b[39m, ELASTIC_LOG_LINE_PREFIX_TEMPLATE_PYTORCH_VERSION):\n\u001b[1;32m    244\u001b[0m         launch_config_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlog_line_prefix_template\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m log_line_prefix_template\n\u001b[0;32m--> 245\u001b[0m     \u001b[43melastic_launch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mLaunchConfig\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mlaunch_config_kwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mentrypoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    246\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ProcessRaisedException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    247\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot re-initialize CUDA in forked subprocess\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m e\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;241m0\u001b[39m]:\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py:134\u001b[0m, in \u001b[0;36melastic_launch.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs):\n\u001b[0;32m--> 134\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlaunch_agent\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_entrypoint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py:264\u001b[0m, in \u001b[0;36mlaunch_agent\u001b[0;34m(config, entrypoint, args)\u001b[0m\n\u001b[1;32m    257\u001b[0m     events\u001b[38;5;241m.\u001b[39mrecord(agent\u001b[38;5;241m.\u001b[39mget_event_succeeded())\n\u001b[1;32m    259\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result\u001b[38;5;241m.\u001b[39mis_failed():\n\u001b[1;32m    260\u001b[0m         \u001b[38;5;66;03m# ChildFailedError is treated specially by @record\u001b[39;00m\n\u001b[1;32m    261\u001b[0m         \u001b[38;5;66;03m# if the error files for the failed children exist\u001b[39;00m\n\u001b[1;32m    262\u001b[0m         \u001b[38;5;66;03m# @record will copy the first error (root cause)\u001b[39;00m\n\u001b[1;32m    263\u001b[0m         \u001b[38;5;66;03m# to the error file of the launcher process.\u001b[39;00m\n\u001b[0;32m--> 264\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ChildFailedError(\n\u001b[1;32m    265\u001b[0m             name\u001b[38;5;241m=\u001b[39mentrypoint_name,\n\u001b[1;32m    266\u001b[0m             failures\u001b[38;5;241m=\u001b[39mresult\u001b[38;5;241m.\u001b[39mfailures,\n\u001b[1;32m    267\u001b[0m         )\n\u001b[1;32m    269\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\u001b[38;5;241m.\u001b[39mreturn_values\n\u001b[1;32m    270\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ChildFailedError:\n",
            "\u001b[0;31mChildFailedError\u001b[0m: \n============================================================\nmain FAILED\n------------------------------------------------------------\nFailures:\n  <NO_OTHER_FAILURES>\n------------------------------------------------------------\nRoot Cause (first observed failure):\n[0]:\n  time      : 2024-07-27_16:58:07\n  host      : 32c2ba5bd8eb\n  rank      : 0 (local_rank: 0)\n  exitcode  : 1 (pid: 108)\n  error_file: /tmp/torchelastic_3j9mvljg/none_85jyjw8v/attempt_0/0/error.json\n  traceback : Traceback (most recent call last):\n    File \"/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 346, in wrapper\n      return f(*args, **kwargs)\n    File \"/tmp/ipykernel_34/2438671895.py\", line 52, in main\n      dataset = load_dataset('glue', 'mrpc')\n    File \"/opt/conda/lib/python3.10/site-packages/datasets/load.py\", line 2594, in load_dataset\n      builder_instance = load_dataset_builder(\n    File \"/opt/conda/lib/python3.10/site-packages/datasets/load.py\", line 2266, in load_dataset_builder\n      dataset_module = dataset_module_factory(\n    File \"/opt/conda/lib/python3.10/site-packages/datasets/load.py\", line 1914, in dataset_module_factory\n      raise e1 from None\n    File \"/opt/conda/lib/python3.10/site-packages/datasets/load.py\", line 1834, in dataset_module_factory\n      dataset_info = hf_api.dataset_info(\n    File \"/opt/conda/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py\", line 114, in _inner_fn\n      return fn(*args, **kwargs)\n    File \"/opt/conda/lib/python3.10/site-packages/huggingface_hub/hf_api.py\", line 2364, in dataset_info\n      hf_raise_for_status(r)\n    File \"/opt/conda/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py\", line 371, in hf_raise_for_status\n      raise HfHubHTTPError(str(e), response=response) from e\n  huggingface_hub.utils._errors.HfHubHTTPError: 500 Server Error: Internal Server Error for url: https://huggingface.co/api/datasets/nyu-mll/glue (Request ID: Root=1-66a5271e-348f01b03561eb134b5c34a6;faef9856-8210-4a8d-8c85-52a23bb2957c)\n  \n  Internal Error - We're working hard to fix this as soon as possible!\n  \n============================================================"
          ],
          "ename": "ChildFailedError",
          "evalue": "\n============================================================\nmain FAILED\n------------------------------------------------------------\nFailures:\n  <NO_OTHER_FAILURES>\n------------------------------------------------------------\nRoot Cause (first observed failure):\n[0]:\n  time      : 2024-07-27_16:58:07\n  host      : 32c2ba5bd8eb\n  rank      : 0 (local_rank: 0)\n  exitcode  : 1 (pid: 108)\n  error_file: /tmp/torchelastic_3j9mvljg/none_85jyjw8v/attempt_0/0/error.json\n  traceback : Traceback (most recent call last):\n    File \"/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 346, in wrapper\n      return f(*args, **kwargs)\n    File \"/tmp/ipykernel_34/2438671895.py\", line 52, in main\n      dataset = load_dataset('glue', 'mrpc')\n    File \"/opt/conda/lib/python3.10/site-packages/datasets/load.py\", line 2594, in load_dataset\n      builder_instance = load_dataset_builder(\n    File \"/opt/conda/lib/python3.10/site-packages/datasets/load.py\", line 2266, in load_dataset_builder\n      dataset_module = dataset_module_factory(\n    File \"/opt/conda/lib/python3.10/site-packages/datasets/load.py\", line 1914, in dataset_module_factory\n      raise e1 from None\n    File \"/opt/conda/lib/python3.10/site-packages/datasets/load.py\", line 1834, in dataset_module_factory\n      dataset_info = hf_api.dataset_info(\n    File \"/opt/conda/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py\", line 114, in _inner_fn\n      return fn(*args, **kwargs)\n    File \"/opt/conda/lib/python3.10/site-packages/huggingface_hub/hf_api.py\", line 2364, in dataset_info\n      hf_raise_for_status(r)\n    File \"/opt/conda/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py\", line 371, in hf_raise_for_status\n      raise HfHubHTTPError(str(e), response=response) from e\n  huggingface_hub.utils._errors.HfHubHTTPError: 500 Server Error: Internal Server Error for url: https://huggingface.co/api/datasets/nyu-mll/glue (Request ID: Root=1-66a5271e-348f01b03561eb134b5c34a6;faef9856-8210-4a8d-8c85-52a23bb2957c)\n  \n  Internal Error - We're working hard to fix this as soon as possible!\n  \n============================================================",
          "output_type": "error"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "TrainingArguments?"
      ],
      "metadata": {
        "trusted": true,
        "id": "dqPdOuDobpk8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}