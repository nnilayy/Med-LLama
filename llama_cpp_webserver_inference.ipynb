{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "68TRdp9NbnMC",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "pip install llama-cpp-python flask-ngrok"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv()\n",
        "ngrok_api_key = os.getenv(\"NGROK_API_KEY\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!ngrok authtoken ngrok_api_key"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-07-06T11:33:05.734431Z",
          "iopub.status.busy": "2024-07-06T11:33:05.733565Z",
          "iopub.status.idle": "2024-07-06T11:33:11.326895Z",
          "shell.execute_reply": "2024-07-06T11:33:11.325936Z",
          "shell.execute_reply.started": "2024-07-06T11:33:05.734395Z"
        },
        "id": "fzAyz1phbnMC",
        "outputId": "7f5d6b18-87bf-40b5-ea33-6ecfd680de4c",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "from llama_cpp import Llama\n",
        "\n",
        "llm = Llama.from_pretrained(\n",
        "    repo_id=\"Qwen/Qwen2-0.5B-Instruct-GGUF\",\n",
        "    filename=\"*q8_0.gguf\",\n",
        "    verbose=False,\n",
        "    n_ctx=2048,\n",
        "#     n_gpu_layers=-1\n",
        ")\n",
        "\n",
        "\n",
        "prompt = \"\"\"\n",
        "You are a helpful Doctor, Help the patient below with their queries and provide a detail ansewr to them\n",
        "Patient: Why DOes my nose hurt so bad Doctor, its like there is severe suffocation and my thorat is also hurting, Can you devise a full Weekly prescription as to what i shoudl take and also tell me safety\n",
        "precautions to take, please, i would be forever thankful?\n",
        "Doctor:\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "output = llm(\n",
        "      prompt, # Prompt\n",
        "      max_tokens=None, # Generate up to 32 tokens, set to None to generate up to the end of the context window\n",
        "      stop=[\"Q:\", \"\\n\"],# Stop generating just before the model would generate a new question\n",
        "      echo=True # Echo the prompt back in the output\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "print(output['choices'][0]['text'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O17OvpcpbnMD"
      },
      "outputs": [],
      "source": [
        "from flask import Flask, jsonify, request\n",
        "from flask_ngrok import run_with_ngrok\n",
        "\n",
        "app = Flask(__name__)\n",
        "run_with_ngrok(app)\n",
        "\n",
        "@app.route(“/”)\n",
        "def hello():\n",
        "    return “Hello World!! from Google Colab”\n",
        "\n",
        "if __name__ == ‘__main__’:\n",
        "app.run()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-07-06T11:57:27.350344Z",
          "iopub.status.busy": "2024-07-06T11:57:27.349391Z",
          "iopub.status.idle": "2024-07-06T12:13:38.864486Z",
          "shell.execute_reply": "2024-07-06T12:13:38.863594Z",
          "shell.execute_reply.started": "2024-07-06T11:57:27.350301Z"
        },
        "id": "VxNLfe1jbnME",
        "outputId": "5ddde8e1-a828-442d-c925-c3a208e05c36",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "from flask import Flask, request, jsonify\n",
        "from flask_ngrok import run_with_ngrok\n",
        "from llama_cpp import Llama\n",
        "\n",
        "app = Flask(__name__)\n",
        "run_with_ngrok(app)\n",
        "# Load the model\n",
        "checkpoint = \"Qwen/Qwen2-0.5B-Instruct-GGUF\"\n",
        "filename = \"*q8_0.gguf\"\n",
        "llm = Llama.from_pretrained(\n",
        "    repo_id=checkpoint,\n",
        "    filename=filename,\n",
        "    verbose=False,\n",
        "    n_ctx=2048,\n",
        "#     n_gpu_layers=-1\n",
        ")\n",
        "\n",
        "\n",
        "@app.route('/predict', methods=['POST'])\n",
        "def predict():\n",
        "    data = request.json\n",
        "    prompt = data['prompt']\n",
        "    max_tokens = data.get('max_tokens', 32)\n",
        "\n",
        "    # Generate text\n",
        "    response = llm(prompt,\n",
        "                   max_tokens=max_tokens,\n",
        "                   stop=[\"Q:\", \"\\n\"],\n",
        "                   echo=True)\n",
        "\n",
        "    return jsonify(response)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    app.run()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cUsJ4GWfbnME"
      },
      "outputs": [],
      "source": [
        "from llama_cpp import Llama\n",
        "from llama_cpp.llama_chat_format import MoondreamChatHandler\n",
        "\n",
        "chat_handler = MoondreamChatHandler.from_pretrained(\n",
        "  repo_id=\"vikhyatk/moondream2\",\n",
        "  filename=\"*mmproj*\",\n",
        ")\n",
        "\n",
        "llm = Llama.from_pretrained(\n",
        "  repo_id=\"vikhyatk/moondream2\",\n",
        "  filename=\"*text-model*\",\n",
        "  chat_handler=chat_handler,\n",
        "  n_ctx=2048, # n_ctx should be increased to accommodate the image embedding\n",
        ")\n",
        "\n",
        "response = llm.create_chat_completion(\n",
        "    messages = [\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": [\n",
        "                {\"type\" : \"text\", \"text\": \"What's in this image?\"},\n",
        "                {\"type\": \"image_url\", \"image_url\": {\"url\": \"https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg\" } }\n",
        "\n",
        "            ]\n",
        "        }\n",
        "    ]\n",
        ")\n",
        "print(response[\"choices\"][0][\"text\"])"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "name": "llama_cpp_webserver_inference",
      "provenance": []
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [],
      "dockerImageVersionId": 30733,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
