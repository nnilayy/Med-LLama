{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [],
      "dockerImageVersionId": 30747,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nnilayy/MedGPT/blob/main/peft_deepspeed.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the read-only \"../input/\" directory\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n",
        "\n",
        "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n",
        "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "id": "CxbHJQRF4ooo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U bitsandbytes evaluate datasets transformers peft deepspeed triton"
      ],
      "metadata": {
        "scrolled": true,
        "trusted": true,
        "id": "PyliySf44oor"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install flash-attn"
      ],
      "metadata": {
        "trusted": true,
        "id": "J5pv-STI4oos"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "sudo apt-get update\n",
        "sudo apt-get install libaio-dev -y"
      ],
      "metadata": {
        "scrolled": true,
        "trusted": true,
        "id": "zuPB7nB74oot"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "hugging_face_token = \"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\"\n",
        "notebook_login()"
      ],
      "metadata": {
        "trusted": true,
        "id": "ydLyQhVJ4oou"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "wandb_api_token = \"1a6a95ba4f084dedd64528953348896560a68bfe\"\n",
        "wandb.login(key = wandb_api_token)"
      ],
      "metadata": {
        "trusted": true,
        "id": "xMNLULpo4oov"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "cat <<'EOT' > ds_config_zero3.json\n",
        "{\n",
        "    \"fp16\": {\n",
        "        \"enabled\": \"auto\",\n",
        "        \"loss_scale\": 0,\n",
        "        \"loss_scale_window\": 1000,\n",
        "        \"initial_scale_power\": 16,\n",
        "        \"hysteresis\": 2,\n",
        "        \"min_loss_scale\": 1\n",
        "    },\n",
        "\n",
        "    \"optimizer\": {\n",
        "        \"type\": \"AdamW\",\n",
        "        \"params\": {\n",
        "            \"lr\": \"auto\",\n",
        "            \"betas\": \"auto\",\n",
        "            \"eps\": \"auto\",\n",
        "            \"weight_decay\": \"auto\"\n",
        "        }\n",
        "    },\n",
        "\n",
        "    \"scheduler\": {\n",
        "        \"type\": \"WarmupLR\",\n",
        "        \"params\": {\n",
        "            \"warmup_min_lr\": \"auto\",\n",
        "            \"warmup_max_lr\": \"auto\",\n",
        "            \"warmup_num_steps\": \"auto\"\n",
        "        }\n",
        "    },\n",
        "\n",
        "    \"zero_optimization\": {\n",
        "        \"stage\": 3,\n",
        "        \"offload_optimizer\": {\n",
        "            \"device\": \"cpu\",\n",
        "            \"pin_memory\": true\n",
        "        },\n",
        "\n",
        "        \"offload_param\": {\n",
        "            \"device\": \"cpu\",\n",
        "            \"pin_memory\": true\n",
        "        },\n",
        "\n",
        "        \"overlap_comm\": true,\n",
        "        \"contiguous_gradients\": true,\n",
        "        \"sub_group_size\": 1e9,\n",
        "        \"reduce_bucket_size\": \"auto\",\n",
        "        \"stage3_prefetch_bucket_size\": \"auto\",\n",
        "        \"stage3_param_persistence_threshold\": \"auto\",\n",
        "        \"stage3_max_live_parameters\": 1e9,\n",
        "        \"stage3_max_reuse_distance\": 1e9,\n",
        "        \"stage3_gather_16bit_weights_on_model_save\": true\n",
        "    },\n",
        "\n",
        "    \"sparse_attention\": {\n",
        "        \"mode\": \"fixed\",\n",
        "        \"block\": 16,\n",
        "        \"different_layout_per_head\": true,\n",
        "        \"num_local_blocks\": 4,\n",
        "        \"num_global_blocks\": 1,\n",
        "        \"attention\": \"bidirectional\",\n",
        "        \"horizontal_global_attention\": false,\n",
        "        \"num_different_global_patterns\": 4,\n",
        "        \"num_random_blocks\": 0,\n",
        "        \"local_window_blocks\": [4],\n",
        "        \"global_block_indices\": [0],\n",
        "        \"num_sliding_window_blocks\": 3\n",
        "  },\n",
        "\n",
        "    \"gradient_accumulation_steps\": \"auto\",\n",
        "    \"gradient_clipping\": \"auto\",\n",
        "    \"steps_per_print\": 2000,\n",
        "    \"train_batch_size\": \"auto\",\n",
        "    \"train_micro_batch_size_per_gpu\": \"auto\",\n",
        "    \"wall_clock_breakdown\": false\n",
        "}\n",
        "EOT"
      ],
      "metadata": {
        "trusted": true,
        "id": "E2I9hNcx4oow"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "cat <<'EOT' > trainer.py\n",
        "\n",
        "# --------------------------------------------------------------------------------------------------------------------------------------------\n",
        "# --------------------------------------------------------------------------------------------------------------------------------------------\n",
        "import os\n",
        "import multiprocessing\n",
        "from transformers import BertTokenizer, BitsAndBytesConfig, BertForSequenceClassification, Trainer, TrainingArguments, DataCollatorWithPadding, AutoModelForSequenceClassification, AutoTokenizer\n",
        "from peft import get_peft_model, LoraConfig, TaskType, prepare_model_for_kbit_training\n",
        "from datasets import load_dataset\n",
        "import torch\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Set OMP_NUM_THREADS\n",
        "num_physical_cores = multiprocessing.cpu_count() // 2  # Assuming hyperthreading is enabled\n",
        "os.environ[\"OMP_NUM_THREADS\"] = str(num_physical_cores)\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n",
        "print(f\"Setting OMP_NUM_THREADS to {num_physical_cores}\")\n",
        "\n",
        "def main():\n",
        "    checkpoint = [\"bert-base-uncased\",\n",
        "                 \"BioMistral/BioMistral-7B\",\n",
        "                 \"bigscience/bloom-3b\",\n",
        "                 ]\n",
        "\n",
        "    index = 2\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(checkpoint[index],\n",
        "                                                               num_labels=2,\n",
        "                                                               torch_dtype=torch.float16, #This reduces the gpu onboard vram usage\n",
        "                                                          )\n",
        "\n",
        "    peft_config = LoraConfig(\n",
        "                             inference_mode=False,\n",
        "                             r=64,\n",
        "                             lora_alpha = 2048,\n",
        "                             lora_dropout = 0.1,\n",
        "                             bias=\"none\",\n",
        "                             peft_type = \"SEQ_CLS\",\n",
        "                             )\n",
        "\n",
        "    model = get_peft_model(model, peft_config)\n",
        "    model.print_trainable_parameters()\n",
        "\n",
        "    def compute_metrics(eval_pred):\n",
        "        logits, labels = eval_pred\n",
        "        logits = torch.from_numpy(logits)\n",
        "        labels = torch.from_numpy(labels)\n",
        "\n",
        "        predictions = torch.argmax(logits, dim=-1)\n",
        "        accuracy = (predictions == labels).float().mean()\n",
        "        return {'accuracy': accuracy.item()}\n",
        "\n",
        "    # Preprocess the dataset\n",
        "    def encode(examples):\n",
        "        outputs = tokenizer(examples['sentence1'], examples['sentence2'], truncation=True, padding='max_length', max_length=128)\n",
        "        outputs['labels'] = examples['label']\n",
        "        return outputs\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(checkpoint[index])\n",
        "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
        "\n",
        "    # Dataset\n",
        "    dataset = load_dataset('glue', 'mrpc')\n",
        "    dataset = dataset.map(encode, batched=True, num_proc=max(1, num_physical_cores - 2))\n",
        "    dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
        "    label_names = dataset['train'].features['label'].names\n",
        "    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "\n",
        "    # Training arguments\n",
        "    torch.set_grad_enabled(True)\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir='./results',\n",
        "        logging_dir='./logs_rslora',\n",
        "        do_train=True,\n",
        "        do_eval=True,\n",
        "        num_train_epochs=10,\n",
        "        learning_rate=2e-4,\n",
        "        logging_strategy='epoch',\n",
        "        per_device_train_batch_size=16,\n",
        "        per_device_eval_batch_size=16,\n",
        "        save_total_limit=1,\n",
        "        dataloader_num_workers = 8,\n",
        "        dataloader_pin_memory = True,\n",
        "        dataloader_prefetch_factor = 4,\n",
        "        save_strategy=\"epoch\",\n",
        "        eval_strategy=\"epoch\",\n",
        "        label_names = [\"labels\"], #Without this Validation Accuracy and Validation Loss wouldn't be logged\n",
        "        fp16=True,\n",
        "\n",
        "#         gradient_checkpointing=True,\n",
        "#         gradient_accumulation_steps=4,\n",
        "#         deepspeed = \"/kaggle/working/ds_config_zero3.json\",\n",
        "    )\n",
        "\n",
        "    # Initialize Trainer\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=dataset['train'],\n",
        "        eval_dataset=dataset['validation'],\n",
        "        data_collator=data_collator,\n",
        "        compute_metrics=compute_metrics\n",
        "    )\n",
        "\n",
        "    # Train the model\n",
        "    trainer.train()\n",
        "    trainer.evaluate()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "# --------------------------------------------------------------------------------------------------------------------------------------------\n",
        "# --------------------------------------------------------------------------------------------------------------------------------------------\n",
        "EOT"
      ],
      "metadata": {
        "trusted": true,
        "id": "jKK5Waxe4oox"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments\n",
        "TrainingArguments?"
      ],
      "metadata": {
        "trusted": true,
        "id": "sjMK00HD4oox"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !torchrun --nproc_per_node=2 load_ddp_model.py\n",
        "!accelerate launch --multi_gpu \\\n",
        "--num_processes=2 \\\n",
        "--mixed_precision=\"fp16\" \\\n",
        "trainer.py\n",
        "\n",
        "# --multi_gpu\n",
        "# --use_deepspeed\n",
        "\n",
        "# Training Config\n",
        "# deepspeed: no\n",
        "# lora: no\n",
        "# time: cuda error\n",
        "\n",
        "# Training Config\n",
        "# deepspeed: no\n",
        "# lora: yes\n",
        "# time: 26 mins\n",
        "\n",
        "# Training Config\n",
        "# deepspeed: yes\n",
        "# lora: yes\n",
        "# time: 48 mins\n",
        "\n",
        "# Training Config\n",
        "# deepspeed: yes\n",
        "# lora: yes\n",
        "# time: 48 mins"
      ],
      "metadata": {
        "scrolled": true,
        "execution": {
          "iopub.status.busy": "2024-07-30T15:18:57.416159Z",
          "iopub.execute_input": "2024-07-30T15:18:57.416584Z",
          "iopub.status.idle": "2024-07-30T15:48:35.076760Z",
          "shell.execute_reply.started": "2024-07-30T15:18:57.416548Z",
          "shell.execute_reply": "2024-07-30T15:48:35.075377Z"
        },
        "trusted": true,
        "id": "jnbRxQo94oox",
        "outputId": "8c7bf42d-8146-4b84-bb58-6e1a6aab4ed7"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "2024-07-30 15:19:06.646675: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-07-30 15:19:06.646675: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-07-30 15:19:06.646747: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-07-30 15:19:06.646748: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-07-30 15:19:06.648766: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n2024-07-30 15:19:06.648767: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nSetting OMP_NUM_THREADS to 2\nSetting OMP_NUM_THREADS to 2\nSome weights of BloomForSequenceClassification were not initialized from the model checkpoint at bigscience/bloom-3b and are newly initialized: ['score.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nSome weights of BloomForSequenceClassification were not initialized from the model checkpoint at bigscience/bloom-3b and are newly initialized: ['score.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\ntrainable params: 19,660,800 || all params: 3,022,223,360 || trainable%: 0.6505\ntrainable params: 19,660,800 || all params: 3,022,223,360 || trainable%: 0.6505\n[2024-07-30 15:19:20,581] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n\u001b[93m [WARNING] \u001b[0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH\n[2024-07-30 15:19:20,917] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n\u001b[93m [WARNING] \u001b[0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH\n\u001b[93m [WARNING] \u001b[0m NVIDIA Inference is only supported on Ampere and newer architectures\n\u001b[93m [WARNING] \u001b[0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1\n\u001b[93m [WARNING] \u001b[0m using untested triton version (3.0.0), only 1.0.0 is known to be compatible\n\u001b[93m [WARNING] \u001b[0m NVIDIA Inference is only supported on Ampere and newer architectures\n\u001b[93m [WARNING] \u001b[0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1\n\u001b[93m [WARNING] \u001b[0m using untested triton version (3.0.0), only 1.0.0 is known to be compatible\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mnnilayy\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n[W reducer.cpp:1346] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.17.5 is available!  To upgrade, please run:\n\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.17.4\n\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/wandb/run-20240730_151925-9e9c3110\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33m./results\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/nnilayy/huggingface\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/nnilayy/huggingface/runs/9e9c3110\u001b[0m\n  0%|                                                  | 0/1150 [00:00<?, ?it/s][W reducer.cpp:1346] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n{'loss': 3.5383, 'grad_norm': 5.702698707580566, 'learning_rate': 0.00018086956521739132, 'epoch': 1.0}\n 10%|████                                    | 115/1150 [02:40<25:08,  1.46s/it]\n  0%|                                                    | 0/13 [00:00<?, ?it/s]\u001b[A\n 15%|██████▊                                     | 2/13 [00:00<00:03,  3.07it/s]\u001b[A\n 23%|██████████▏                                 | 3/13 [00:01<00:04,  2.21it/s]\u001b[A\n 31%|█████████████▌                              | 4/13 [00:01<00:04,  1.91it/s]\u001b[A\n 38%|████████████████▉                           | 5/13 [00:02<00:04,  1.76it/s]\u001b[A\n 46%|████████████████████▎                       | 6/13 [00:03<00:04,  1.71it/s]\u001b[A\n 54%|███████████████████████▋                    | 7/13 [00:03<00:03,  1.66it/s]\u001b[A\n 62%|███████████████████████████                 | 8/13 [00:04<00:03,  1.63it/s]\u001b[A\n 69%|██████████████████████████████▍             | 9/13 [00:05<00:02,  1.63it/s]\u001b[A\n 77%|█████████████████████████████████          | 10/13 [00:05<00:01,  1.60it/s]\u001b[A\n 85%|████████████████████████████████████▍      | 11/13 [00:06<00:01,  1.60it/s]\u001b[A\n 92%|███████████████████████████████████████▋   | 12/13 [00:06<00:00,  1.59it/s]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 0.857702374458313, 'eval_accuracy': 0.3848039209842682, 'eval_runtime': 8.7826, 'eval_samples_per_second': 46.455, 'eval_steps_per_second': 1.48, 'epoch': 1.0}\n 10%|████                                    | 115/1150 [02:49<25:08,  1.46s/it]\n100%|███████████████████████████████████████████| 13/13 [00:07<00:00,  1.48it/s]\u001b[A\n{'loss': 0.7738, 'grad_norm': 2.336444616317749, 'learning_rate': 0.00016086956521739132, 'epoch': 2.0}\n 20%|████████                                | 230/1150 [05:32<22:23,  1.46s/it]\n  0%|                                                    | 0/13 [00:00<?, ?it/s]\u001b[A\n 15%|██████▊                                     | 2/13 [00:00<00:03,  3.05it/s]\u001b[A\n 23%|██████████▏                                 | 3/13 [00:01<00:04,  2.21it/s]\u001b[A\n 31%|█████████████▌                              | 4/13 [00:01<00:04,  1.93it/s]\u001b[A\n 38%|████████████████▉                           | 5/13 [00:02<00:04,  1.79it/s]\u001b[A\n 46%|████████████████████▎                       | 6/13 [00:03<00:04,  1.73it/s]\u001b[A\n 54%|███████████████████████▋                    | 7/13 [00:03<00:03,  1.67it/s]\u001b[A\n 62%|███████████████████████████                 | 8/13 [00:04<00:03,  1.64it/s]\u001b[A\n 69%|██████████████████████████████▍             | 9/13 [00:05<00:02,  1.63it/s]\u001b[A\n 77%|█████████████████████████████████          | 10/13 [00:05<00:01,  1.61it/s]\u001b[A\n 85%|████████████████████████████████████▍      | 11/13 [00:06<00:01,  1.61it/s]\u001b[A\n 92%|███████████████████████████████████████▋   | 12/13 [00:06<00:00,  1.61it/s]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 0.6668379306793213, 'eval_accuracy': 0.6936274766921997, 'eval_runtime': 8.7413, 'eval_samples_per_second': 46.675, 'eval_steps_per_second': 1.487, 'epoch': 2.0}\n 20%|████████                                | 230/1150 [05:41<22:23,  1.46s/it]\n100%|███████████████████████████████████████████| 13/13 [00:07<00:00,  1.49it/s]\u001b[A\n{'loss': 0.6782, 'grad_norm': 56.083438873291016, 'learning_rate': 0.00014086956521739132, 'epoch': 3.0}\n 30%|████████████                            | 345/1150 [08:24<19:41,  1.47s/it]\n  0%|                                                    | 0/13 [00:00<?, ?it/s]\u001b[A\n 15%|██████▊                                     | 2/13 [00:00<00:03,  3.25it/s]\u001b[A\n 23%|██████████▏                                 | 3/13 [00:01<00:04,  2.24it/s]\u001b[A\n 31%|█████████████▌                              | 4/13 [00:01<00:04,  1.94it/s]\u001b[A\n 38%|████████████████▉                           | 5/13 [00:02<00:04,  1.81it/s]\u001b[A\n 46%|████████████████████▎                       | 6/13 [00:03<00:04,  1.72it/s]\u001b[A\n 54%|███████████████████████▋                    | 7/13 [00:03<00:03,  1.69it/s]\u001b[A\n 62%|███████████████████████████                 | 8/13 [00:04<00:03,  1.65it/s]\u001b[A\n 69%|██████████████████████████████▍             | 9/13 [00:05<00:02,  1.63it/s]\u001b[A\n 77%|█████████████████████████████████          | 10/13 [00:05<00:01,  1.62it/s]\u001b[A\n 85%|████████████████████████████████████▍      | 11/13 [00:06<00:01,  1.61it/s]\u001b[A\n 92%|███████████████████████████████████████▋   | 12/13 [00:06<00:00,  1.61it/s]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 0.658586859703064, 'eval_accuracy': 0.656862735748291, 'eval_runtime': 8.7825, 'eval_samples_per_second': 46.456, 'eval_steps_per_second': 1.48, 'epoch': 3.0}\n 30%|████████████                            | 345/1150 [08:33<19:41,  1.47s/it]\n100%|███████████████████████████████████████████| 13/13 [00:07<00:00,  1.50it/s]\u001b[A\n{'loss': 0.6557, 'grad_norm': 15.760964393615723, 'learning_rate': 0.00012086956521739131, 'epoch': 4.0}\n 40%|████████████████                        | 460/1150 [11:15<16:45,  1.46s/it]\n  0%|                                                    | 0/13 [00:00<?, ?it/s]\u001b[A\n 15%|██████▊                                     | 2/13 [00:00<00:03,  3.10it/s]\u001b[A\n 23%|██████████▏                                 | 3/13 [00:01<00:04,  2.23it/s]\u001b[A\n 31%|█████████████▌                              | 4/13 [00:01<00:04,  1.94it/s]\u001b[A\n 38%|████████████████▉                           | 5/13 [00:02<00:04,  1.79it/s]\u001b[A\n 46%|████████████████████▎                       | 6/13 [00:03<00:04,  1.73it/s]\u001b[A\n 54%|███████████████████████▋                    | 7/13 [00:03<00:03,  1.67it/s]\u001b[A\n 62%|███████████████████████████                 | 8/13 [00:04<00:03,  1.64it/s]\u001b[A\n 69%|██████████████████████████████▍             | 9/13 [00:05<00:02,  1.63it/s]\u001b[A\n 77%|█████████████████████████████████          | 10/13 [00:05<00:01,  1.61it/s]\u001b[A\n 85%|████████████████████████████████████▍      | 11/13 [00:06<00:01,  1.61it/s]\u001b[A\n 92%|███████████████████████████████████████▋   | 12/13 [00:06<00:00,  1.61it/s]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 0.624422550201416, 'eval_accuracy': 0.686274528503418, 'eval_runtime': 8.7864, 'eval_samples_per_second': 46.435, 'eval_steps_per_second': 1.48, 'epoch': 4.0}\n 40%|████████████████                        | 460/1150 [11:24<16:45,  1.46s/it]\n100%|███████████████████████████████████████████| 13/13 [00:07<00:00,  1.48it/s]\u001b[A\n{'loss': 0.6244, 'grad_norm': 12.99903678894043, 'learning_rate': 0.00010121739130434784, 'epoch': 5.0}\n 50%|████████████████████                    | 575/1150 [14:07<13:59,  1.46s/it]\n  0%|                                                    | 0/13 [00:00<?, ?it/s]\u001b[A\n 15%|██████▊                                     | 2/13 [00:00<00:03,  3.03it/s]\u001b[A\n 23%|██████████▏                                 | 3/13 [00:01<00:04,  2.20it/s]\u001b[A\n 31%|█████████████▌                              | 4/13 [00:01<00:04,  1.93it/s]\u001b[A\n 38%|████████████████▉                           | 5/13 [00:02<00:04,  1.79it/s]\u001b[A\n 46%|████████████████████▎                       | 6/13 [00:03<00:04,  1.73it/s]\u001b[A\n 54%|███████████████████████▋                    | 7/13 [00:03<00:03,  1.67it/s]\u001b[A\n 62%|███████████████████████████                 | 8/13 [00:04<00:03,  1.64it/s]\u001b[A\n 69%|██████████████████████████████▍             | 9/13 [00:05<00:02,  1.63it/s]\u001b[A\n 77%|█████████████████████████████████          | 10/13 [00:05<00:01,  1.61it/s]\u001b[A\n 85%|████████████████████████████████████▍      | 11/13 [00:06<00:01,  1.61it/s]\u001b[A\n 92%|███████████████████████████████████████▋   | 12/13 [00:06<00:00,  1.61it/s]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 0.6443966031074524, 'eval_accuracy': 0.6838235259056091, 'eval_runtime': 8.8046, 'eval_samples_per_second': 46.34, 'eval_steps_per_second': 1.477, 'epoch': 5.0}\n 50%|████████████████████                    | 575/1150 [14:15<13:59,  1.46s/it]\n100%|███████████████████████████████████████████| 13/13 [00:07<00:00,  1.45it/s]\u001b[A\n{'loss': 0.6315, 'grad_norm': 23.02299690246582, 'learning_rate': 8.121739130434783e-05, 'epoch': 6.0}\n 60%|████████████████████████                | 690/1150 [16:58<11:10,  1.46s/it]\n  0%|                                                    | 0/13 [00:00<?, ?it/s]\u001b[A\n 15%|██████▊                                     | 2/13 [00:00<00:03,  3.05it/s]\u001b[A\n 23%|██████████▏                                 | 3/13 [00:01<00:04,  2.21it/s]\u001b[A\n 31%|█████████████▌                              | 4/13 [00:01<00:04,  1.94it/s]\u001b[A\n 38%|████████████████▉                           | 5/13 [00:02<00:04,  1.78it/s]\u001b[A\n 46%|████████████████████▎                       | 6/13 [00:03<00:04,  1.72it/s]\u001b[A\n 54%|███████████████████████▋                    | 7/13 [00:03<00:03,  1.66it/s]\u001b[A\n 62%|███████████████████████████                 | 8/13 [00:04<00:03,  1.64it/s]\u001b[A\n 69%|██████████████████████████████▍             | 9/13 [00:05<00:02,  1.63it/s]\u001b[A\n 77%|█████████████████████████████████          | 10/13 [00:05<00:01,  1.61it/s]\u001b[A\n 85%|████████████████████████████████████▍      | 11/13 [00:06<00:01,  1.60it/s]\u001b[A\n 92%|███████████████████████████████████████▋   | 12/13 [00:06<00:00,  1.61it/s]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 0.6668325066566467, 'eval_accuracy': 0.6200980544090271, 'eval_runtime': 8.7709, 'eval_samples_per_second': 46.517, 'eval_steps_per_second': 1.482, 'epoch': 6.0}\n 60%|████████████████████████                | 690/1150 [17:07<11:10,  1.46s/it]\n100%|███████████████████████████████████████████| 13/13 [00:07<00:00,  1.48it/s]\u001b[A\n{'loss': 0.618, 'grad_norm': 21.040218353271484, 'learning_rate': 6.121739130434783e-05, 'epoch': 7.0}\n 70%|████████████████████████████            | 805/1150 [19:50<08:23,  1.46s/it]\n  0%|                                                    | 0/13 [00:00<?, ?it/s]\u001b[A\n 15%|██████▊                                     | 2/13 [00:00<00:03,  3.10it/s]\u001b[A\n 23%|██████████▏                                 | 3/13 [00:01<00:04,  2.24it/s]\u001b[A\n 31%|█████████████▌                              | 4/13 [00:01<00:04,  1.94it/s]\u001b[A\n 38%|████████████████▉                           | 5/13 [00:02<00:04,  1.78it/s]\u001b[A\n 46%|████████████████████▎                       | 6/13 [00:03<00:04,  1.72it/s]\u001b[A\n 54%|███████████████████████▋                    | 7/13 [00:03<00:03,  1.66it/s]\u001b[A\n 62%|███████████████████████████                 | 8/13 [00:04<00:03,  1.64it/s]\u001b[A\n 69%|██████████████████████████████▍             | 9/13 [00:05<00:02,  1.63it/s]\u001b[A\n 77%|█████████████████████████████████          | 10/13 [00:05<00:01,  1.61it/s]\u001b[A\n 85%|████████████████████████████████████▍      | 11/13 [00:06<00:01,  1.61it/s]\u001b[A\n 92%|███████████████████████████████████████▋   | 12/13 [00:06<00:00,  1.61it/s]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 0.6427440047264099, 'eval_accuracy': 0.6764705777168274, 'eval_runtime': 8.7132, 'eval_samples_per_second': 46.826, 'eval_steps_per_second': 1.492, 'epoch': 7.0}\n 70%|████████████████████████████            | 805/1150 [19:59<08:23,  1.46s/it]\n100%|███████████████████████████████████████████| 13/13 [00:07<00:00,  1.50it/s]\u001b[A\n{'loss': 0.6021, 'grad_norm': 99.39067077636719, 'learning_rate': 4.1217391304347827e-05, 'epoch': 8.0}\n 80%|████████████████████████████████        | 920/1150 [22:42<05:35,  1.46s/it]\n  0%|                                                    | 0/13 [00:00<?, ?it/s]\u001b[A\n 15%|██████▊                                     | 2/13 [00:00<00:03,  3.07it/s]\u001b[A\n 23%|██████████▏                                 | 3/13 [00:01<00:04,  2.23it/s]\u001b[A\n 31%|█████████████▌                              | 4/13 [00:01<00:04,  1.94it/s]\u001b[A\n 38%|████████████████▉                           | 5/13 [00:02<00:04,  1.79it/s]\u001b[A\n 46%|████████████████████▎                       | 6/13 [00:03<00:04,  1.73it/s]\u001b[A\n 54%|███████████████████████▋                    | 7/13 [00:03<00:03,  1.67it/s]\u001b[A\n 62%|███████████████████████████                 | 8/13 [00:04<00:03,  1.64it/s]\u001b[A\n 69%|██████████████████████████████▍             | 9/13 [00:05<00:02,  1.64it/s]\u001b[A\n 77%|█████████████████████████████████          | 10/13 [00:05<00:01,  1.62it/s]\u001b[A\n 85%|████████████████████████████████████▍      | 11/13 [00:06<00:01,  1.61it/s]\u001b[A\n 92%|███████████████████████████████████████▋   | 12/13 [00:06<00:00,  1.61it/s]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 0.6347541213035583, 'eval_accuracy': 0.6789215803146362, 'eval_runtime': 8.6906, 'eval_samples_per_second': 46.947, 'eval_steps_per_second': 1.496, 'epoch': 8.0}\n 80%|████████████████████████████████        | 920/1150 [22:51<05:35,  1.46s/it]\n100%|███████████████████████████████████████████| 13/13 [00:07<00:00,  1.50it/s]\u001b[A\n{'loss': 0.6095, 'grad_norm': 22.382253646850586, 'learning_rate': 2.1217391304347828e-05, 'epoch': 9.0}\n 90%|███████████████████████████████████    | 1035/1150 [25:33<02:48,  1.46s/it]\n  0%|                                                    | 0/13 [00:00<?, ?it/s]\u001b[A\n 15%|██████▊                                     | 2/13 [00:00<00:03,  3.06it/s]\u001b[A\n 23%|██████████▏                                 | 3/13 [00:01<00:04,  2.21it/s]\u001b[A\n 31%|█████████████▌                              | 4/13 [00:01<00:04,  1.93it/s]\u001b[A\n 38%|████████████████▉                           | 5/13 [00:02<00:04,  1.78it/s]\u001b[A\n 46%|████████████████████▎                       | 6/13 [00:03<00:04,  1.72it/s]\u001b[A\n 54%|███████████████████████▋                    | 7/13 [00:03<00:03,  1.66it/s]\u001b[A\n 62%|███████████████████████████                 | 8/13 [00:04<00:03,  1.63it/s]\u001b[A\n 69%|██████████████████████████████▍             | 9/13 [00:05<00:02,  1.63it/s]\u001b[A\n 77%|█████████████████████████████████          | 10/13 [00:05<00:01,  1.61it/s]\u001b[A\n 85%|████████████████████████████████████▍      | 11/13 [00:06<00:01,  1.60it/s]\u001b[A\n 92%|███████████████████████████████████████▋   | 12/13 [00:06<00:00,  1.61it/s]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 0.6332511305809021, 'eval_accuracy': 0.686274528503418, 'eval_runtime': 8.7594, 'eval_samples_per_second': 46.579, 'eval_steps_per_second': 1.484, 'epoch': 9.0}\n 90%|███████████████████████████████████    | 1035/1150 [25:42<02:48,  1.46s/it]\n100%|███████████████████████████████████████████| 13/13 [00:07<00:00,  1.49it/s]\u001b[A\n{'loss': 0.5903, 'grad_norm': 5.86713981628418, 'learning_rate': 1.217391304347826e-06, 'epoch': 10.0}\n100%|███████████████████████████████████████| 1150/1150 [28:26<00:00,  1.46s/it]\n  0%|                                                    | 0/13 [00:00<?, ?it/s]\u001b[A\n 15%|██████▊                                     | 2/13 [00:00<00:03,  3.21it/s]\u001b[A\n 23%|██████████▏                                 | 3/13 [00:01<00:04,  2.24it/s]\u001b[A\n 31%|█████████████▌                              | 4/13 [00:01<00:04,  1.94it/s]\u001b[A\n 38%|████████████████▉                           | 5/13 [00:02<00:04,  1.81it/s]\u001b[A\n 46%|████████████████████▎                       | 6/13 [00:03<00:04,  1.72it/s]\u001b[A\n 54%|███████████████████████▋                    | 7/13 [00:03<00:03,  1.68it/s]\u001b[A\n 62%|███████████████████████████                 | 8/13 [00:04<00:03,  1.65it/s]\u001b[A\n 69%|██████████████████████████████▍             | 9/13 [00:05<00:02,  1.63it/s]\u001b[A\n 77%|█████████████████████████████████          | 10/13 [00:05<00:01,  1.62it/s]\u001b[A\n 85%|████████████████████████████████████▍      | 11/13 [00:06<00:01,  1.61it/s]\u001b[A\n 92%|███████████████████████████████████████▋   | 12/13 [00:06<00:00,  1.61it/s]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 0.630031943321228, 'eval_accuracy': 0.6813725233078003, 'eval_runtime': 8.7686, 'eval_samples_per_second': 46.529, 'eval_steps_per_second': 1.483, 'epoch': 10.0}\n100%|███████████████████████████████████████| 1150/1150 [28:34<00:00,  1.46s/it]\n100%|███████████████████████████████████████████| 13/13 [00:07<00:00,  1.50it/s]\u001b[A\n{'train_runtime': 1733.8537, 'train_samples_per_second': 21.155, 'train_steps_per_second': 0.663, 'train_loss': 0.9321899347719939, 'epoch': 10.0}\n100%|███████████████████████████████████████| 1150/1150 [28:35<00:00,  1.49s/it]\n100%|███████████████████████████████████████████| 13/13 [00:07<00:00,  1.69it/s]\n\u001b[34m\u001b[1mwandb\u001b[0m: \\ 0.001 MB of 0.007 MB uploaded\n\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n\u001b[34m\u001b[1mwandb\u001b[0m:           eval/accuracy ▁█▇██▆█████\n\u001b[34m\u001b[1mwandb\u001b[0m:               eval/loss █▂▂▁▂▂▂▁▁▁▁\n\u001b[34m\u001b[1mwandb\u001b[0m:            eval/runtime ▇▄▇▇█▆▂▁▅▆▇\n\u001b[34m\u001b[1mwandb\u001b[0m: eval/samples_per_second ▂▅▂▂▁▃▇█▄▃▂\n\u001b[34m\u001b[1mwandb\u001b[0m:   eval/steps_per_second ▂▅▂▂▁▃▇█▄▃▂\n\u001b[34m\u001b[1mwandb\u001b[0m:             train/epoch ▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇████\n\u001b[34m\u001b[1mwandb\u001b[0m:       train/global_step ▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇████\n\u001b[34m\u001b[1mwandb\u001b[0m:         train/grad_norm ▁▁▅▂▂▂▂█▂▁\n\u001b[34m\u001b[1mwandb\u001b[0m:     train/learning_rate █▇▆▆▅▄▃▃▂▁\n\u001b[34m\u001b[1mwandb\u001b[0m:              train/loss █▁▁▁▁▁▁▁▁▁\n\u001b[34m\u001b[1mwandb\u001b[0m: \n\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n\u001b[34m\u001b[1mwandb\u001b[0m:            eval/accuracy 0.68137\n\u001b[34m\u001b[1mwandb\u001b[0m:                eval/loss 0.63003\n\u001b[34m\u001b[1mwandb\u001b[0m:             eval/runtime 8.7903\n\u001b[34m\u001b[1mwandb\u001b[0m:  eval/samples_per_second 46.415\n\u001b[34m\u001b[1mwandb\u001b[0m:    eval/steps_per_second 1.479\n\u001b[34m\u001b[1mwandb\u001b[0m:               total_flos 6.726368146489344e+16\n\u001b[34m\u001b[1mwandb\u001b[0m:              train/epoch 10.0\n\u001b[34m\u001b[1mwandb\u001b[0m:        train/global_step 1150\n\u001b[34m\u001b[1mwandb\u001b[0m:          train/grad_norm 5.86714\n\u001b[34m\u001b[1mwandb\u001b[0m:      train/learning_rate 0.0\n\u001b[34m\u001b[1mwandb\u001b[0m:               train/loss 0.5903\n\u001b[34m\u001b[1mwandb\u001b[0m:               train_loss 0.93219\n\u001b[34m\u001b[1mwandb\u001b[0m:            train_runtime 1733.8537\n\u001b[34m\u001b[1mwandb\u001b[0m: train_samples_per_second 21.155\n\u001b[34m\u001b[1mwandb\u001b[0m:   train_steps_per_second 0.663\n\u001b[34m\u001b[1mwandb\u001b[0m: \n\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33m./results\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/nnilayy/huggingface/runs/9e9c3110\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/nnilayy/huggingface\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240730_151925-9e9c3110/logs\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information.\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!accelerate launch -h"
      ],
      "metadata": {
        "trusted": true,
        "id": "H5YOPPe04ooz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import gc\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()"
      ],
      "metadata": {
        "trusted": true,
        "id": "dlreg0zQ4ooz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loads Model on CPUs RAM\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "model = AutoModelForCausalLM.from_pretrained(\"bigscience/bloom-3b\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bigscience/bloom-3b\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "euOcQ0A_4ooz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loads Model on 1 GPU Vram\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "model = AutoModelForCausalLM.from_pretrained(\"bigscience/bloom-3b\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bigscience/bloom-3b\")\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)"
      ],
      "metadata": {
        "trusted": true,
        "id": "pVM0xKJv4ooz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loads Model on 1 GPU Vram, in 16 precision\n",
        "# Reduce memory footprint by half\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "model = AutoModelForCausalLM.from_pretrained(\"bigscience/bloom-3b\", torch_dtype=torch.float16)\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bigscience/bloom-3b\")\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)"
      ],
      "metadata": {
        "trusted": true,
        "id": "TnSYwkbJ4oo0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM\n",
        "torch.set_default_dtype(torch.float16)\n",
        "if torch.cuda.is_available():\n",
        "    torch.set_default_device('cuda:1')\n",
        "model = AutoModelForCausalLM.from_pretrained(\"bigscience/bloom-3b\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "wPKTnAPN4oo0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Parallelism: Same Model Gets Loaded On One GPU\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\"bigscience/bloom-3b\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bigscience/bloom-3b\")\n",
        "model = torch.nn.DataParallel(model)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "pOKaXaKR4oo0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\"bigscience/bloom-3b\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bigscience/bloom-3b\")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    # Use all available GPUs\n",
        "    device_ids = list(range(torch.cuda.device_count()))\n",
        "    model = torch.nn.DataParallel(model, device_ids=device_ids)\n",
        "    model.to('cuda')  # Move model to the default device\n",
        "else:\n",
        "    print(\"CUDA is not available.\")\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "XUZ1oz094oo0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "dataset = load_dataset('nnilayy/pubmedqa_artificial_128')"
      ],
      "metadata": {
        "trusted": true,
        "id": "iW_2XBOr4oo0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}