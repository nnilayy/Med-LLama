{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tfZUpYZZMISz",
        "scrolled": true,
        "trusted": true
      },
      "outputs": [],
      "source": [
        "!pip install -U bitsandbytes evaluate datasets transformers peft deepspeed triton wandb\n",
        "# !pip install flash-attn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8b9C1N4pMIS0",
        "scrolled": true,
        "trusted": true
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "sudo apt-get update\n",
        "sudo apt-get install libaio-dev -y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tcN-2Jw2MIS1",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import wandb\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv()\n",
        "\n",
        "hugging_face_api_key = os.getenv(\"HUGGING_FACE_API_KEY\")\n",
        "wandb_api_key = os.getenv(\"WANDB_API_KEY\")\n",
        "wandb.login(key = wandb_api_key)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, BitsAndBytesConfig, AutoModelForCausalLM\n",
        "from transformers import BertTokenizer, BertForMaskedLM\n",
        "# Loading the model in 8-bit and 4-bit\n",
        "# checkpoint = \"TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T\"\n",
        "\n",
        "checkpoint = \"bert-base-uncased\"\n",
        "tokenizer = BertTokenizer.from_pretrained(checkpoint, padding_side=\"right\")\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "   load_in_8bit=True,\n",
        "#    bnb_4bit_quant_type=\"nf4\",\n",
        "#    bnb_4bit_use_double_quant=True,\n",
        "   bnb_8bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "model = BertForMaskedLM.from_pretrained(checkpoint,\n",
        "                                            #  device_map = \"auto\",\n",
        "                                             quantization_config = bnb_config,\n",
        "                                             torch_dtype=torch.float16,\n",
        "                                             )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# MERGE LORA WEIGHTS WITH BASE MODEL\n",
        "from transformers import AutoModelForCausalLM\n",
        "from peft import PeftModel\n",
        "\n",
        "# Assuming 'base_model' is your pre-trained model's name or path\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\"base_model_name_or_path\")\n",
        "peft_model = PeftModel.from_pretrained(base_model, \"path_to_trained_adapter\")\n",
        "merged_model = peft_model.merge_and_unload()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Loads Model on 1 GPU Vram, in 16 precision\n",
        "# Reduce memory footprint by half\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "model = AutoModelForCausalLM.from_pretrained(\"bigscience/bloom-3b\", torch_dtype=torch.float16)\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bigscience/bloom-3b\")\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM\n",
        "torch.set_default_dtype(torch.float16)\n",
        "if torch.cuda.is_available():\n",
        "    torch.set_default_device('cuda:1')\n",
        "model = AutoModelForCausalLM.from_pretrained(\"bigscience/bloom-3b\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Data Parallelism: Same Model Gets Loaded On One GPU\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\"bigscience/bloom-3b\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bigscience/bloom-3b\")\n",
        "model = torch.nn.DataParallel(model)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\"bigscience/bloom-3b\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bigscience/bloom-3b\")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    # Use all available GPUs\n",
        "    device_ids = list(range(torch.cuda.device_count()))\n",
        "    model = torch.nn.DataParallel(model, device_ids=device_ids)\n",
        "    model.to('cuda')  # Move model to the default device\n",
        "else:\n",
        "    print(\"CUDA is not available.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# !python -m torch.distributed.launch --nproc_per_node=2 trainer.py\n",
        "# !torchrun --nproc_per_node=2 load_ddp_model.py\n",
        "# !accelerate launch --multi_gpu --mixed_precision=\"fp16\" --num_processes=2 trainer.py\n",
        "!python trainer.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sGuHC3EFMIS1",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "cat <<'EOT' > ds_config_zero3.json\n",
        "{\n",
        "    \"fp16\": {\n",
        "        \"enabled\": \"auto\",\n",
        "        \"loss_scale\": 0,\n",
        "        \"loss_scale_window\": 1000,\n",
        "        \"initial_scale_power\": 16,\n",
        "        \"hysteresis\": 2,\n",
        "        \"min_loss_scale\": 1\n",
        "    },\n",
        "\n",
        "    \"optimizer\": {\n",
        "        \"type\": \"AdamW\",\n",
        "        \"params\": {\n",
        "            \"lr\": \"auto\",\n",
        "            \"betas\": \"auto\",\n",
        "            \"eps\": \"auto\",\n",
        "            \"weight_decay\": \"auto\"\n",
        "        }\n",
        "    },\n",
        "\n",
        "    \"scheduler\": {\n",
        "        \"type\": \"WarmupLR\",\n",
        "        \"params\": {\n",
        "            \"warmup_min_lr\": \"auto\",\n",
        "            \"warmup_max_lr\": \"auto\",\n",
        "            \"warmup_num_steps\": \"auto\"\n",
        "        }\n",
        "    },\n",
        "\n",
        "    \"zero_optimization\": {\n",
        "        \"stage\": 3,\n",
        "        \"offload_optimizer\": {\n",
        "            \"device\": \"cpu\",\n",
        "            \"pin_memory\": true\n",
        "        },\n",
        "\n",
        "        \"offload_param\": {\n",
        "            \"device\": \"cpu\",\n",
        "            \"pin_memory\": true\n",
        "        },\n",
        "\n",
        "        \"overlap_comm\": true,\n",
        "        \"contiguous_gradients\": true,\n",
        "        \"sub_group_size\": 1e9,\n",
        "        \"reduce_bucket_size\": \"auto\",\n",
        "        \"stage3_prefetch_bucket_size\": \"auto\",\n",
        "        \"stage3_param_persistence_threshold\": \"auto\",\n",
        "        \"stage3_max_live_parameters\": 1e9,\n",
        "        \"stage3_max_reuse_distance\": 1e9,\n",
        "        \"stage3_gather_16bit_weights_on_model_save\": true\n",
        "    },\n",
        "\n",
        "    \"sparse_attention\": {\n",
        "        \"mode\": \"fixed\",\n",
        "        \"block\": 16,\n",
        "        \"different_layout_per_head\": true,\n",
        "        \"num_local_blocks\": 4,\n",
        "        \"num_global_blocks\": 1,\n",
        "        \"attention\": \"bidirectional\",\n",
        "        \"horizontal_global_attention\": false,\n",
        "        \"num_different_global_patterns\": 4,\n",
        "        \"num_random_blocks\": 0,\n",
        "        \"local_window_blocks\": [4],\n",
        "        \"global_block_indices\": [0],\n",
        "        \"num_sliding_window_blocks\": 3\n",
        "  },\n",
        "\n",
        "    \"gradient_accumulation_steps\": \"auto\",\n",
        "    \"gradient_clipping\": \"auto\",\n",
        "    \"steps_per_print\": 2000,\n",
        "    \"train_batch_size\": \"auto\",\n",
        "    \"train_micro_batch_size_per_gpu\": \"auto\",\n",
        "    \"wall_clock_breakdown\": false\n",
        "}\n",
        "EOT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "feOvPiKjMIS2",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "cat <<'EOT' > trainer.py\n",
        "\n",
        "# --------------------------------------------------------------------------------------------------------------------------------------------\n",
        "# --------------------------------------------------------------------------------------------------------------------------------------------\n",
        "import os\n",
        "import multiprocessing\n",
        "from transformers import BertTokenizer, BitsAndBytesConfig, BertForSequenceClassification, Trainer, TrainingArguments, DataCollatorWithPadding, AutoModelForSequenceClassification, AutoTokenizer, AutoConfig\n",
        "from peft import get_peft_model, LoraConfig, TaskType, prepare_model_for_kbit_training\n",
        "from datasets import load_dataset\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "import numpy as np\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "from accelerate import PartialState\n",
        "from accelerate.logging import get_logger\n",
        "\n",
        "class SparseAttention(nn.Module):\n",
        "    def __init__(self, hidden_size, num_heads, sparsity_factor=2):\n",
        "        super().__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.hidden_size = hidden_size\n",
        "        self.head_dim = hidden_size // num_heads\n",
        "        self.sparsity_factor = sparsity_factor\n",
        "\n",
        "        self.query = nn.Linear(hidden_size, hidden_size)\n",
        "        self.key = nn.Linear(hidden_size, hidden_size)\n",
        "        self.value = nn.Linear(hidden_size, hidden_size)\n",
        "\n",
        "    def forward(self, hidden_states):\n",
        "        batch_size, seq_length, _ = hidden_states.size()\n",
        "\n",
        "        query = self.query(hidden_states).view(batch_size, seq_length, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "        key = self.key(hidden_states).view(batch_size, seq_length, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "        value = self.value(hidden_states).view(batch_size, seq_length, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "\n",
        "        scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
        "\n",
        "        sparse_mask = torch.ones_like(scores, dtype=torch.bool)\n",
        "        sparse_mask[:, :, :, ::self.sparsity_factor] = False\n",
        "\n",
        "        scores = scores.masked_fill(sparse_mask, float('-inf'))\n",
        "\n",
        "        attn_weights = F.softmax(scores, dim=-1)\n",
        "        attn_output = torch.matmul(attn_weights, value)\n",
        "\n",
        "        return attn_output.transpose(1, 2).contiguous().view(batch_size, seq_length, self.hidden_size), attn_weights\n",
        "\n",
        "class CustomModelWithSparseAttention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.embeddings = nn.Embedding(config.vocab_size, config.hidden_size)\n",
        "        self.sparse_attention = SparseAttention(config.hidden_size, config.num_attention_heads)\n",
        "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
        "        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n",
        "\n",
        "    def forward(self, input_ids=None, attention_mask=None, token_type_ids=None, position_ids=None,\n",
        "                inputs_embeds=None, labels=None, output_attentions=None, output_hidden_states=None,\n",
        "                return_dict=None):\n",
        "        if input_ids is not None:\n",
        "            input_shape = input_ids.size()\n",
        "            embeddings = self.embeddings(input_ids)\n",
        "        elif inputs_embeds is not None:\n",
        "            input_shape = inputs_embeds.size()[:-1]\n",
        "            embeddings = inputs_embeds\n",
        "        else:\n",
        "            raise ValueError(\"You have to specify either input_ids or inputs_embeds\")\n",
        "\n",
        "        attention_output, attention_weights = self.sparse_attention(embeddings)\n",
        "        hidden_states = self.dense(attention_output)\n",
        "        logits = self.classifier(hidden_states[:, 0, :])  # Use [CLS] token for classification\n",
        "\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            loss_fct = nn.CrossEntropyLoss()\n",
        "            loss = loss_fct(logits.view(-1, self.config.num_labels), labels.view(-1))\n",
        "\n",
        "        if not return_dict:\n",
        "            output = (logits,)\n",
        "            if output_hidden_states:\n",
        "                output += (hidden_states,)\n",
        "            if output_attentions:\n",
        "                output += (attention_weights,)\n",
        "            return ((loss,) + output) if loss is not None else output\n",
        "\n",
        "        return {'loss': loss, 'logits': logits, 'hidden_states': hidden_states, 'attentions': attention_weights}\n",
        "\n",
        "def main():\n",
        "    checkpoint = [\"bert-base-uncased\",\n",
        "                 \"BioMistral/BioMistral-7B\",\n",
        "                 \"bigscience/bloom-3b\",\n",
        "                 ]\n",
        "\n",
        "    index = 0  # Using BERT base uncased\n",
        "    config = AutoConfig.from_pretrained(checkpoint[index])\n",
        "    config.num_labels = 2\n",
        "\n",
        "    model = CustomModelWithSparseAttention(config)\n",
        "\n",
        "    peft_config = LoraConfig(\n",
        "        task_type=TaskType.SEQ_CLS,\n",
        "        inference_mode=False,\n",
        "        r=1,\n",
        "        lora_alpha=2048,\n",
        "        lora_dropout=0.1,\n",
        "        target_modules=[\"dense\", \"classifier\"]\n",
        "    )\n",
        "\n",
        "    model = get_peft_model(model, peft_config)\n",
        "    model.print_trainable_parameters()\n",
        "\n",
        "    def compute_metrics(eval_pred):\n",
        "        logits, labels = eval_pred\n",
        "        if isinstance(logits, tuple):  # If model returns a tuple, take the first element as logits\n",
        "            logits = logits[0]\n",
        "        predictions = np.argmax(logits, axis=-1)\n",
        "        accuracy = (predictions == labels).mean()\n",
        "        return {'accuracy': accuracy}\n",
        "\n",
        "    def encode(examples):\n",
        "        outputs = tokenizer(examples['sentence1'], examples['sentence2'], truncation=True, padding='max_length', max_length=128)\n",
        "        outputs['labels'] = examples['label']\n",
        "        return outputs\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(checkpoint[index])\n",
        "    if tokenizer.pad_token is None:\n",
        "        tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
        "\n",
        "    dataset = load_dataset('glue', 'mrpc')\n",
        "    dataset = dataset.map(encode, batched=True, num_proc=12)\n",
        "    dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'token_type_ids', 'labels'])\n",
        "    label_names = dataset['train'].features['label'].names\n",
        "    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "\n",
        "    torch.set_grad_enabled(True)\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir='./results',\n",
        "        logging_dir='./logs_rslora',\n",
        "        do_train=True,\n",
        "        do_eval=True,\n",
        "        num_train_epochs=10,\n",
        "        learning_rate=2e-4,\n",
        "        logging_strategy='epoch',\n",
        "        per_device_train_batch_size=20,\n",
        "        per_device_eval_batch_size=20,\n",
        "        save_total_limit=1,\n",
        "        dataloader_num_workers=8,\n",
        "        dataloader_pin_memory=True,\n",
        "        dataloader_prefetch_factor=4,\n",
        "        save_strategy=\"epoch\",\n",
        "        evaluation_strategy=\"epoch\",\n",
        "        label_names=[\"labels\"],\n",
        "        fp16=True,\n",
        "        ddp_find_unused_parameters=False,\n",
        "    )\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=dataset['train'],\n",
        "        eval_dataset=dataset['validation'],\n",
        "        data_collator=data_collator,\n",
        "        compute_metrics=compute_metrics\n",
        "    )\n",
        "\n",
        "    trainer.train()\n",
        "    trainer.evaluate()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "# --------------------------------------------------------------------------------------------------------------------------------------------\n",
        "# --------------------------------------------------------------------------------------------------------------------------------------------\n",
        "EOT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d4Zgvv4TMIS3",
        "scrolled": true,
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# !torchrun --nproc_per_node=2 load_ddp_model.py\n",
        "!accelerate launch --multi_gpu \\\n",
        "--num_processes=2 \\\n",
        "--mixed_precision=\"fp16\" \\\n",
        "working_trainer.py\n",
        "\n",
        "# --multi_gpu\n",
        "# --use_deepspeed\n",
        "\n",
        "# Training Config\n",
        "# deepspeed: no\n",
        "# lora: no\n",
        "# time: cuda error\n",
        "\n",
        "# Training Config\n",
        "# deepspeed: no\n",
        "# lora: yes\n",
        "# time: 26 mins\n",
        "\n",
        "# Training Config\n",
        "# deepspeed: yes\n",
        "# lora: yes\n",
        "# time: 48 mins\n",
        "\n",
        "# Training Config\n",
        "# deepspeed: yes\n",
        "# lora: yes\n",
        "# time: 48 mins"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [],
      "dockerImageVersionId": 30747,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
