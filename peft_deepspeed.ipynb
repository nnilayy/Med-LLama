{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [],
      "dockerImageVersionId": 30747,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nnilayy/MedGPT/blob/main/peft_deepspeed.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the read-only \"../input/\" directory\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n",
        "\n",
        "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n",
        "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "id": "vhQeoV5UMISy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U bitsandbytes evaluate datasets transformers peft deepspeed triton wandb"
      ],
      "metadata": {
        "scrolled": true,
        "trusted": true,
        "id": "tfZUpYZZMISz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "sudo apt-get update\n",
        "sudo apt-get install libaio-dev -y"
      ],
      "metadata": {
        "scrolled": true,
        "trusted": true,
        "id": "8b9C1N4pMIS0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install flash-attn"
      ],
      "metadata": {
        "trusted": true,
        "id": "kuF2FrscMIS0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "hugging_face_token = \"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\"\n",
        "notebook_login()"
      ],
      "metadata": {
        "trusted": true,
        "id": "tcN-2Jw2MIS1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "wandb_api_token = \"1a6a95ba4f084dedd64528953348896560a68bfe\"\n",
        "wandb.login(key = wandb_api_token)"
      ],
      "metadata": {
        "trusted": true,
        "id": "8SWE88VGMIS1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "cat <<'EOT' > ds_config_zero3.json\n",
        "{\n",
        "    \"fp16\": {\n",
        "        \"enabled\": \"auto\",\n",
        "        \"loss_scale\": 0,\n",
        "        \"loss_scale_window\": 1000,\n",
        "        \"initial_scale_power\": 16,\n",
        "        \"hysteresis\": 2,\n",
        "        \"min_loss_scale\": 1\n",
        "    },\n",
        "\n",
        "    \"optimizer\": {\n",
        "        \"type\": \"AdamW\",\n",
        "        \"params\": {\n",
        "            \"lr\": \"auto\",\n",
        "            \"betas\": \"auto\",\n",
        "            \"eps\": \"auto\",\n",
        "            \"weight_decay\": \"auto\"\n",
        "        }\n",
        "    },\n",
        "\n",
        "    \"scheduler\": {\n",
        "        \"type\": \"WarmupLR\",\n",
        "        \"params\": {\n",
        "            \"warmup_min_lr\": \"auto\",\n",
        "            \"warmup_max_lr\": \"auto\",\n",
        "            \"warmup_num_steps\": \"auto\"\n",
        "        }\n",
        "    },\n",
        "\n",
        "    \"zero_optimization\": {\n",
        "        \"stage\": 3,\n",
        "        \"offload_optimizer\": {\n",
        "            \"device\": \"cpu\",\n",
        "            \"pin_memory\": true\n",
        "        },\n",
        "\n",
        "        \"offload_param\": {\n",
        "            \"device\": \"cpu\",\n",
        "            \"pin_memory\": true\n",
        "        },\n",
        "\n",
        "        \"overlap_comm\": true,\n",
        "        \"contiguous_gradients\": true,\n",
        "        \"sub_group_size\": 1e9,\n",
        "        \"reduce_bucket_size\": \"auto\",\n",
        "        \"stage3_prefetch_bucket_size\": \"auto\",\n",
        "        \"stage3_param_persistence_threshold\": \"auto\",\n",
        "        \"stage3_max_live_parameters\": 1e9,\n",
        "        \"stage3_max_reuse_distance\": 1e9,\n",
        "        \"stage3_gather_16bit_weights_on_model_save\": true\n",
        "    },\n",
        "\n",
        "    \"sparse_attention\": {\n",
        "        \"mode\": \"fixed\",\n",
        "        \"block\": 16,\n",
        "        \"different_layout_per_head\": true,\n",
        "        \"num_local_blocks\": 4,\n",
        "        \"num_global_blocks\": 1,\n",
        "        \"attention\": \"bidirectional\",\n",
        "        \"horizontal_global_attention\": false,\n",
        "        \"num_different_global_patterns\": 4,\n",
        "        \"num_random_blocks\": 0,\n",
        "        \"local_window_blocks\": [4],\n",
        "        \"global_block_indices\": [0],\n",
        "        \"num_sliding_window_blocks\": 3\n",
        "  },\n",
        "\n",
        "    \"gradient_accumulation_steps\": \"auto\",\n",
        "    \"gradient_clipping\": \"auto\",\n",
        "    \"steps_per_print\": 2000,\n",
        "    \"train_batch_size\": \"auto\",\n",
        "    \"train_micro_batch_size_per_gpu\": \"auto\",\n",
        "    \"wall_clock_breakdown\": false\n",
        "}\n",
        "EOT"
      ],
      "metadata": {
        "trusted": true,
        "id": "sGuHC3EFMIS1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "cat <<'EOT' > trainer.py\n",
        "\n",
        "# --------------------------------------------------------------------------------------------------------------------------------------------\n",
        "# --------------------------------------------------------------------------------------------------------------------------------------------\n",
        "import os\n",
        "import multiprocessing\n",
        "from transformers import BertTokenizer, BitsAndBytesConfig, BertForSequenceClassification, Trainer, TrainingArguments, DataCollatorWithPadding, AutoModelForSequenceClassification, AutoTokenizer\n",
        "from peft import get_peft_model, LoraConfig, TaskType, prepare_model_for_kbit_training\n",
        "from datasets import load_dataset\n",
        "import torch\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "from accelerate import PartialState\n",
        "from accelerate.logging import get_logger\n",
        "\n",
        "def main():\n",
        "    checkpoint = [\"bert-base-uncased\",\n",
        "                 \"BioMistral/BioMistral-7B\",\n",
        "                 \"bigscience/bloom-3b\",\n",
        "                 ]\n",
        "\n",
        "    index = 2\n",
        "#     bnb_config = BitsAndBytesConfig(load_in_8bit=True)\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(checkpoint[index],\n",
        "                                                               num_labels=2,\n",
        "                                                               torch_dtype=torch.float16, #This reduces the gpu onboard vram usage\n",
        "#                                                                quantization_config=bnb_config,\n",
        "                                                               low_cpu_mem_usage=True,\n",
        "#                                                                device_map={\"\": PartialState().process_index},\n",
        "                                                          )\n",
        "\n",
        "    peft_config = LoraConfig(\n",
        "                             inference_mode=False,\n",
        "                             r=64,\n",
        "                             lora_alpha = 2048,\n",
        "                             lora_dropout = 0.1,\n",
        "                             bias=\"none\",\n",
        "                             peft_type = \"SEQ_CLS\",\n",
        "                             )\n",
        "\n",
        "    model = get_peft_model(model, peft_config)\n",
        "    model.print_trainable_parameters()\n",
        "\n",
        "    def compute_metrics(eval_pred):\n",
        "        logits, labels = eval_pred\n",
        "        logits = torch.from_numpy(logits)\n",
        "        labels = torch.from_numpy(labels)\n",
        "\n",
        "        predictions = torch.argmax(logits, dim=-1)\n",
        "        accuracy = (predictions == labels).float().mean()\n",
        "        return {'accuracy': accuracy.item()}\n",
        "\n",
        "    # Preprocess the dataset\n",
        "    def encode(examples):\n",
        "        outputs = tokenizer(examples['sentence1'], examples['sentence2'], truncation=True, padding='max_length', max_length=128)\n",
        "        outputs['labels'] = examples['label']\n",
        "        return outputs\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(checkpoint[index])\n",
        "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
        "\n",
        "    # Dataset\n",
        "    dataset = load_dataset('glue', 'mrpc')\n",
        "    dataset = dataset.map(encode, batched=True, num_proc=12)\n",
        "    dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
        "    label_names = dataset['train'].features['label'].names\n",
        "    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "\n",
        "    # Training arguments\n",
        "\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir='./results',\n",
        "        logging_dir='./logs_rslora',\n",
        "        do_train=True,\n",
        "        do_eval=True,\n",
        "        num_train_epochs=10,\n",
        "        learning_rate=2e-4,\n",
        "        logging_strategy='epoch',\n",
        "        per_device_train_batch_size=20,\n",
        "        per_device_eval_batch_size=20,\n",
        "        save_total_limit=1,\n",
        "        dataloader_num_workers = 8,\n",
        "        dataloader_pin_memory = True,\n",
        "        dataloader_prefetch_factor = 4,\n",
        "        save_strategy=\"epoch\",\n",
        "        eval_strategy=\"epoch\",\n",
        "        label_names = [\"labels\"], #Without this Validation Accuracy and Validation Loss wouldn't be logged\n",
        "        fp16=True,\n",
        "        ddp_find_unused_parameters = False,\n",
        "#         gradient_checkpointing=True,\n",
        "#         gradient_accumulation_steps=4,\n",
        "#         deepspeed = \"/kaggle/working/ds_config_zero3.json\",\n",
        "    )\n",
        "\n",
        "    # Initialize Trainer\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=dataset['train'],\n",
        "        eval_dataset=dataset['validation'],\n",
        "        data_collator=data_collator,\n",
        "        compute_metrics=compute_metrics\n",
        "    )\n",
        "\n",
        "    # Train the model\n",
        "    trainer.train()\n",
        "    trainer.evaluate()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "# --------------------------------------------------------------------------------------------------------------------------------------------\n",
        "# --------------------------------------------------------------------------------------------------------------------------------------------\n",
        "EOT"
      ],
      "metadata": {
        "trusted": true,
        "id": "0UY21CU-MIS1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "cat <<'EOT' > trainer.py\n",
        "\n",
        "# --------------------------------------------------------------------------------------------------------------------------------------------\n",
        "# --------------------------------------------------------------------------------------------------------------------------------------------\n",
        "import os\n",
        "import multiprocessing\n",
        "from transformers import BertTokenizer, BitsAndBytesConfig, BertForSequenceClassification, Trainer, TrainingArguments, DataCollatorWithPadding, AutoModelForSequenceClassification, AutoTokenizer, AutoConfig\n",
        "from peft import get_peft_model, LoraConfig, TaskType, prepare_model_for_kbit_training\n",
        "from datasets import load_dataset\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "import numpy as np\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "from accelerate import PartialState\n",
        "from accelerate.logging import get_logger\n",
        "\n",
        "class SparseAttention(nn.Module):\n",
        "    def __init__(self, hidden_size, num_heads, sparsity_factor=2):\n",
        "        super().__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.hidden_size = hidden_size\n",
        "        self.head_dim = hidden_size // num_heads\n",
        "        self.sparsity_factor = sparsity_factor\n",
        "\n",
        "        self.query = nn.Linear(hidden_size, hidden_size)\n",
        "        self.key = nn.Linear(hidden_size, hidden_size)\n",
        "        self.value = nn.Linear(hidden_size, hidden_size)\n",
        "\n",
        "    def forward(self, hidden_states):\n",
        "        batch_size, seq_length, _ = hidden_states.size()\n",
        "\n",
        "        query = self.query(hidden_states).view(batch_size, seq_length, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "        key = self.key(hidden_states).view(batch_size, seq_length, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "        value = self.value(hidden_states).view(batch_size, seq_length, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "\n",
        "        scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
        "\n",
        "        sparse_mask = torch.ones_like(scores, dtype=torch.bool)\n",
        "        sparse_mask[:, :, :, ::self.sparsity_factor] = False\n",
        "\n",
        "        scores = scores.masked_fill(sparse_mask, float('-inf'))\n",
        "\n",
        "        attn_weights = F.softmax(scores, dim=-1)\n",
        "        attn_output = torch.matmul(attn_weights, value)\n",
        "\n",
        "        return attn_output.transpose(1, 2).contiguous().view(batch_size, seq_length, self.hidden_size), attn_weights\n",
        "\n",
        "class CustomModelWithSparseAttention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.embeddings = nn.Embedding(config.vocab_size, config.hidden_size)\n",
        "        self.sparse_attention = SparseAttention(config.hidden_size, config.num_attention_heads)\n",
        "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
        "        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n",
        "\n",
        "    def forward(self, input_ids=None, attention_mask=None, token_type_ids=None, position_ids=None,\n",
        "                inputs_embeds=None, labels=None, output_attentions=None, output_hidden_states=None,\n",
        "                return_dict=None):\n",
        "        if input_ids is not None:\n",
        "            input_shape = input_ids.size()\n",
        "            embeddings = self.embeddings(input_ids)\n",
        "        elif inputs_embeds is not None:\n",
        "            input_shape = inputs_embeds.size()[:-1]\n",
        "            embeddings = inputs_embeds\n",
        "        else:\n",
        "            raise ValueError(\"You have to specify either input_ids or inputs_embeds\")\n",
        "\n",
        "        attention_output, attention_weights = self.sparse_attention(embeddings)\n",
        "        hidden_states = self.dense(attention_output)\n",
        "        logits = self.classifier(hidden_states[:, 0, :])  # Use [CLS] token for classification\n",
        "\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            loss_fct = nn.CrossEntropyLoss()\n",
        "            loss = loss_fct(logits.view(-1, self.config.num_labels), labels.view(-1))\n",
        "\n",
        "        if not return_dict:\n",
        "            output = (logits,)\n",
        "            if output_hidden_states:\n",
        "                output += (hidden_states,)\n",
        "            if output_attentions:\n",
        "                output += (attention_weights,)\n",
        "            return ((loss,) + output) if loss is not None else output\n",
        "\n",
        "        return {'loss': loss, 'logits': logits, 'hidden_states': hidden_states, 'attentions': attention_weights}\n",
        "\n",
        "def main():\n",
        "    checkpoint = [\"bert-base-uncased\",\n",
        "                 \"BioMistral/BioMistral-7B\",\n",
        "                 \"bigscience/bloom-3b\",\n",
        "                 ]\n",
        "\n",
        "    index = 0  # Using BERT base uncased\n",
        "    config = AutoConfig.from_pretrained(checkpoint[index])\n",
        "    config.num_labels = 2\n",
        "\n",
        "    model = CustomModelWithSparseAttention(config)\n",
        "\n",
        "    peft_config = LoraConfig(\n",
        "        task_type=TaskType.SEQ_CLS,\n",
        "        inference_mode=False,\n",
        "        r=1,\n",
        "        lora_alpha=2048,\n",
        "        lora_dropout=0.1,\n",
        "        target_modules=[\"dense\", \"classifier\"]\n",
        "    )\n",
        "\n",
        "    model = get_peft_model(model, peft_config)\n",
        "    model.print_trainable_parameters()\n",
        "\n",
        "    def compute_metrics(eval_pred):\n",
        "        logits, labels = eval_pred\n",
        "        if isinstance(logits, tuple):  # If model returns a tuple, take the first element as logits\n",
        "            logits = logits[0]\n",
        "        predictions = np.argmax(logits, axis=-1)\n",
        "        accuracy = (predictions == labels).mean()\n",
        "        return {'accuracy': accuracy}\n",
        "\n",
        "    def encode(examples):\n",
        "        outputs = tokenizer(examples['sentence1'], examples['sentence2'], truncation=True, padding='max_length', max_length=128)\n",
        "        outputs['labels'] = examples['label']\n",
        "        return outputs\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(checkpoint[index])\n",
        "    if tokenizer.pad_token is None:\n",
        "        tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
        "\n",
        "    dataset = load_dataset('glue', 'mrpc')\n",
        "    dataset = dataset.map(encode, batched=True, num_proc=12)\n",
        "    dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'token_type_ids', 'labels'])\n",
        "    label_names = dataset['train'].features['label'].names\n",
        "    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "\n",
        "    torch.set_grad_enabled(True)\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir='./results',\n",
        "        logging_dir='./logs_rslora',\n",
        "        do_train=True,\n",
        "        do_eval=True,\n",
        "        num_train_epochs=10,\n",
        "        learning_rate=2e-4,\n",
        "        logging_strategy='epoch',\n",
        "        per_device_train_batch_size=20,\n",
        "        per_device_eval_batch_size=20,\n",
        "        save_total_limit=1,\n",
        "        dataloader_num_workers=8,\n",
        "        dataloader_pin_memory=True,\n",
        "        dataloader_prefetch_factor=4,\n",
        "        save_strategy=\"epoch\",\n",
        "        evaluation_strategy=\"epoch\",\n",
        "        label_names=[\"labels\"],\n",
        "        fp16=True,\n",
        "        ddp_find_unused_parameters=False,\n",
        "    )\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=dataset['train'],\n",
        "        eval_dataset=dataset['validation'],\n",
        "        data_collator=data_collator,\n",
        "        compute_metrics=compute_metrics\n",
        "    )\n",
        "\n",
        "    trainer.train()\n",
        "    trainer.evaluate()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "# --------------------------------------------------------------------------------------------------------------------------------------------\n",
        "# --------------------------------------------------------------------------------------------------------------------------------------------\n",
        "EOT"
      ],
      "metadata": {
        "trusted": true,
        "id": "feOvPiKjMIS2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments\n",
        "TrainingArguments?"
      ],
      "metadata": {
        "trusted": true,
        "id": "INrkT2FSMIS2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lr_scheduler_type=\"cosine\","
      ],
      "metadata": {
        "id": "912Nb7aeMIS2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install lomo-optim\n",
        "# !pip install git+https://github.com/jiaweizzhao/GaLore\n",
        "!pip install tensorly"
      ],
      "metadata": {
        "trusted": true,
        "id": "_IbmAjqSMIS2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %%bash\n",
        "# cat <<'EOT' > working_trainer.py\n",
        "import torch\n",
        "from transformers import AutoTokenizer, BitsAndBytesConfig, AutoModelForCausalLM, AutoModelForSequenceClassification, AutoTokenizer, BertForSequenceClassification\n",
        "from peft import get_peft_model, LoraConfig, TaskType, prepare_model_for_kbit_training\n",
        "from transformers import TrainingArguments, Trainer\n",
        "from evaluate import load\n",
        "from datasets import load_dataset\n",
        "from transformers import DataCollatorWithPadding\n",
        "import numpy as np\n",
        "import warnings\n",
        "import os\n",
        "from peft.utils import get_peft_model_state_dict\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
        "\n",
        "def main():\n",
        "    def compute_metrics(eval_pred):\n",
        "        logits, labels = eval_pred\n",
        "        predictions = np.argmax(logits, axis=-1)\n",
        "        return accuracy.compute(predictions=predictions, references=labels)\n",
        "\n",
        "    def encode(examples):\n",
        "        output = tokenizer(examples['sentence1'], examples['sentence2'], truncation=True, padding='max_length', max_length=128)\n",
        "        output['labels'] = examples['label']\n",
        "        return output\n",
        "\n",
        "\n",
        "    checkpoint = \"bert-base-uncased\"\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(checkpoint,\n",
        "#                                                  torch_dtype=torch.float16,\n",
        "                                                 )\n",
        "\n",
        "\n",
        "    peft_config = LoraConfig(inference_mode=False,\n",
        "                             r=32,\n",
        "                             lora_alpha = 512,\n",
        "                             lora_dropout = 0.1,\n",
        "                             bias=\"none\",\n",
        "                             peft_type = TaskType.SEQ_CLS, #\" CAUSAL_LM\"\n",
        "                             )\n",
        "\n",
        "    # model = prepare_model_for_kbit_training(model)\n",
        "    model = get_peft_model(model, peft_config)\n",
        "    model.print_trainable_parameters()\n",
        "\n",
        "    # TYPICAL TRAINING CODE\n",
        "    accuracy = load(\"accuracy\")\n",
        "    tokenizer = AutoTokenizer.from_pretrained(checkpoint, padding_side=\"right\")\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
        "\n",
        "    dataset = load_dataset(\"glue\", \"mrpc\")\n",
        "    dataset = dataset.map(encode, batched=True)\n",
        "    dataset = dataset.remove_columns(['sentence1', 'sentence2', 'label', 'idx'])\n",
        "    dataset.set_format(type='pt', columns=['input_ids', 'attention_mask', 'labels',], output_all_columns=True)\n",
        "#     label_names = dataset['train'].features['label'].names\n",
        "    data_collator = DataCollatorWithPadding(tokenizer)\n",
        "\n",
        "#     torch.set_grad_enabled(True)\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=\"your-name/bigscience/mt0-large-lora\",\n",
        "        optim=\"paged_adamw_32bit\",\n",
        "        learning_rate=1e-4,\n",
        "        lr_scheduler_type=\"cosine\",\n",
        "#         lr_scheduler_kwargs={\"power\": 2.0},\n",
        "        warmup_ratio=0.4,\n",
        "#         warmup_steps=200,\n",
        "#         ddp_backend=\"nccl\",\n",
        "        logging_strategy=\"epoch\", # Logs the Training Loss\n",
        "        label_names = ['labels'], # Logs the Validation Loss and Validation Accuracy\n",
        "        per_device_train_batch_size=32,\n",
        "        per_device_eval_batch_size=32,\n",
        "        num_train_epochs=15,\n",
        "        eval_strategy=\"epoch\", # Doesnt Evaluate the model per epoch, Reducing the training time\n",
        "        save_strategy=\"epoch\",\n",
        "        fp16=True,\n",
        "        seed=42,\n",
        "        data_seed=42,\n",
        "        dataloader_num_workers=4, # Reduces Training time by a decent percentage\n",
        "        dataloader_pin_memory=True,\n",
        "        dataloader_persistent_workers=True,\n",
        "        ddp_find_unused_parameters=False,\n",
        "#         gradient_checkpointing=True,\n",
        "#         torch_empty_cache_steps=40, #Clears vram cache during training after a few steps\n",
        "    )\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=dataset[\"train\"],\n",
        "        eval_dataset=dataset[\"test\"],\n",
        "        tokenizer=tokenizer,\n",
        "        data_collator=data_collator,\n",
        "        compute_metrics=compute_metrics,\n",
        "    )\n",
        "\n",
        "    trainer.train()\n",
        "if __name__ == \"__main__\":\n",
        "#     main()\n",
        "    from accelerate import notebook_launcher\n",
        "    notebook_launcher(main, num_processes=2)\n",
        "# EOT"
      ],
      "metadata": {
        "_kg_hide-output": true,
        "execution": {
          "iopub.status.idle": "2024-08-01T16:36:14.093527Z",
          "shell.execute_reply.started": "2024-08-01T16:32:39.936098Z",
          "shell.execute_reply": "2024-08-01T16:36:14.092181Z"
        },
        "trusted": true,
        "id": "zWUh8Ql3MIS2",
        "outputId": "ac9965f9-ec3b-4dfb-a575-6623be8869d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "[2024-08-01 16:32:46,216] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n[2024-08-01 16:32:46,261] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n\u001b[93m [WARNING] \u001b[0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH\n\u001b[93m [WARNING] \u001b[0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH\n\u001b[93m [WARNING] \u001b[0m NVIDIA Inference is only supported on Ampere and newer architectures\n\u001b[93m [WARNING] \u001b[0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1\n\u001b[93m [WARNING] \u001b[0m using untested triton version (3.0.0), only 1.0.0 is known to be compatible\n\u001b[93m [WARNING] \u001b[0m NVIDIA Inference is only supported on Ampere and newer architectures\n\u001b[93m [WARNING] \u001b[0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1\n\u001b[93m [WARNING] \u001b[0m using untested triton version (3.0.0), only 1.0.0 is known to be compatible\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mnnilayy\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "Tracking run with wandb version 0.17.5"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "Run data is saved locally in <code>/kaggle/working/wandb/run-20240801_163250-fyrzx9x0</code>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "Syncing run <strong><a href='https://wandb.ai/nnilayy/huggingface/runs/fyrzx9x0' target=\"_blank\">your-name/bigscience/mt0-large-lora</a></strong> to <a href='https://wandb.ai/nnilayy/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": " View project at <a href='https://wandb.ai/nnilayy/huggingface' target=\"_blank\">https://wandb.ai/nnilayy/huggingface</a>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": " View run at <a href='https://wandb.ai/nnilayy/huggingface/runs/fyrzx9x0' target=\"_blank\">https://wandb.ai/nnilayy/huggingface/runs/fyrzx9x0</a>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='870' max='870' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [870/870 03:04, Epoch 15/15]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.639500</td>\n      <td>0.621369</td>\n      <td>0.664928</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.592300</td>\n      <td>0.559371</td>\n      <td>0.743768</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.546300</td>\n      <td>0.509891</td>\n      <td>0.784348</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.484100</td>\n      <td>0.449037</td>\n      <td>0.821449</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.430100</td>\n      <td>0.440502</td>\n      <td>0.817971</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>0.394500</td>\n      <td>0.426874</td>\n      <td>0.828986</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>0.338300</td>\n      <td>0.415037</td>\n      <td>0.830145</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>0.265900</td>\n      <td>0.386071</td>\n      <td>0.844638</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>0.219100</td>\n      <td>0.398121</td>\n      <td>0.845797</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>0.174100</td>\n      <td>0.411007</td>\n      <td>0.845797</td>\n    </tr>\n    <tr>\n      <td>11</td>\n      <td>0.148000</td>\n      <td>0.437653</td>\n      <td>0.843478</td>\n    </tr>\n    <tr>\n      <td>12</td>\n      <td>0.125500</td>\n      <td>0.449699</td>\n      <td>0.840580</td>\n    </tr>\n    <tr>\n      <td>13</td>\n      <td>0.109900</td>\n      <td>0.470834</td>\n      <td>0.837101</td>\n    </tr>\n    <tr>\n      <td>14</td>\n      <td>0.098100</td>\n      <td>0.472312</td>\n      <td>0.838841</td>\n    </tr>\n    <tr>\n      <td>15</td>\n      <td>0.098600</td>\n      <td>0.475843</td>\n      <td>0.838261</td>\n    </tr>\n  </tbody>\n</table><p>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='870' max='870' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [870/870 03:04, Epoch 15/15]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.639500</td>\n      <td>0.621369</td>\n      <td>0.664928</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.592300</td>\n      <td>0.559371</td>\n      <td>0.743768</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.546300</td>\n      <td>0.509891</td>\n      <td>0.784348</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.484100</td>\n      <td>0.449037</td>\n      <td>0.821449</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.430100</td>\n      <td>0.440502</td>\n      <td>0.817971</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>0.394500</td>\n      <td>0.426874</td>\n      <td>0.828986</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>0.338300</td>\n      <td>0.415037</td>\n      <td>0.830145</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>0.265900</td>\n      <td>0.386071</td>\n      <td>0.844638</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>0.219100</td>\n      <td>0.398121</td>\n      <td>0.845797</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>0.174100</td>\n      <td>0.411007</td>\n      <td>0.845797</td>\n    </tr>\n    <tr>\n      <td>11</td>\n      <td>0.148000</td>\n      <td>0.437653</td>\n      <td>0.843478</td>\n    </tr>\n    <tr>\n      <td>12</td>\n      <td>0.125500</td>\n      <td>0.449699</td>\n      <td>0.840580</td>\n    </tr>\n    <tr>\n      <td>13</td>\n      <td>0.109900</td>\n      <td>0.470834</td>\n      <td>0.837101</td>\n    </tr>\n    <tr>\n      <td>14</td>\n      <td>0.098100</td>\n      <td>0.472312</td>\n      <td>0.838841</td>\n    </tr>\n    <tr>\n      <td>15</td>\n      <td>0.098600</td>\n      <td>0.475843</td>\n      <td>0.838261</td>\n    </tr>\n  </tbody>\n</table><p>"
          },
          "metadata": {}
        },
        {
          "name": "stderr",
          "text": "[2024-08-01 16:36:14,044] torch.distributed.elastic.multiprocessing.api: [WARNING] Closing process 73918 via signal SIGTERM\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!accelerate launch -h"
      ],
      "metadata": {
        "trusted": true,
        "id": "_MEmE7_bMIS3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !torchrun --nproc_per_node=2 load_ddp_model.py\n",
        "!accelerate launch --multi_gpu \\\n",
        "--num_processes=2 \\\n",
        "--mixed_precision=\"fp16\" \\\n",
        "working_trainer.py\n",
        "\n",
        "# --multi_gpu\n",
        "# --use_deepspeed\n",
        "\n",
        "# Training Config\n",
        "# deepspeed: no\n",
        "# lora: no\n",
        "# time: cuda error\n",
        "\n",
        "# Training Config\n",
        "# deepspeed: no\n",
        "# lora: yes\n",
        "# time: 26 mins\n",
        "\n",
        "# Training Config\n",
        "# deepspeed: yes\n",
        "# lora: yes\n",
        "# time: 48 mins\n",
        "\n",
        "# Training Config\n",
        "# deepspeed: yes\n",
        "# lora: yes\n",
        "# time: 48 mins"
      ],
      "metadata": {
        "scrolled": true,
        "trusted": true,
        "id": "d4Zgvv4TMIS3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "import os\n",
        "\n",
        "model_id = \"google/gemma-2b\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config, device_map={\"\":0})"
      ],
      "metadata": {
        "id": "DXNCLztFMIS3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import gc\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()"
      ],
      "metadata": {
        "trusted": true,
        "id": "pRABCryCMIS4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loads Model on CPUs RAM\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "model = AutoModelForCausalLM.from_pretrained(\"bigscience/bloom-3b\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bigscience/bloom-3b\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "J7xiAs3oMIS4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loads Model on 1 GPU Vram\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "model = AutoModelForCausalLM.from_pretrained(\"bigscience/bloom-3b\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bigscience/bloom-3b\")\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)"
      ],
      "metadata": {
        "trusted": true,
        "id": "NnCv_S7uMIS4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loads Model on 1 GPU Vram, in 16 precision\n",
        "# Reduce memory footprint by half\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "model = AutoModelForCausalLM.from_pretrained(\"bigscience/bloom-3b\", torch_dtype=torch.float16)\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bigscience/bloom-3b\")\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)"
      ],
      "metadata": {
        "trusted": true,
        "id": "87Qb7hEAMIS4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM\n",
        "torch.set_default_dtype(torch.float16)\n",
        "if torch.cuda.is_available():\n",
        "    torch.set_default_device('cuda:1')\n",
        "model = AutoModelForCausalLM.from_pretrained(\"bigscience/bloom-3b\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "vWoMi3btMIS4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Parallelism: Same Model Gets Loaded On One GPU\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\"bigscience/bloom-3b\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bigscience/bloom-3b\")\n",
        "model = torch.nn.DataParallel(model)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "zcOSkI0pMIS4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\"bigscience/bloom-3b\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bigscience/bloom-3b\")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    # Use all available GPUs\n",
        "    device_ids = list(range(torch.cuda.device_count()))\n",
        "    model = torch.nn.DataParallel(model, device_ids=device_ids)\n",
        "    model.to('cuda')  # Move model to the default device\n",
        "else:\n",
        "    print(\"CUDA is not available.\")\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "e33DU0RzMIS4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "dataset = load_dataset('nnilayy/pubmedqa_artificial_128')"
      ],
      "metadata": {
        "trusted": true,
        "id": "z2Lb2-TRMIS4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}