{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [],
      "dockerImageVersionId": 30747,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nnilayy/MedGPT/blob/main/peft_deepspeed.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the read-only \"../input/\" directory\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n",
        "\n",
        "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n",
        "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "id": "FCt22khQuQng"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install bitsandbytes evaluate datasets transformers peft"
      ],
      "metadata": {
        "trusted": true,
        "id": "CQjz0svDuQnk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers --upgrade"
      ],
      "metadata": {
        "trusted": true,
        "id": "w0VwfY0euQnl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "hugging_face_token = \"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\"\n",
        "notebook_login()"
      ],
      "metadata": {
        "trusted": true,
        "id": "Dib5Dy6BuQnm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "wandb_api_token = \"1a6a95ba4f084dedd64528953348896560a68bfe\"\n",
        "wandb.login(key = wandb_api_token)"
      ],
      "metadata": {
        "trusted": true,
        "id": "6BXs3BOduQnn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer,BitsAndBytesConfig,BertForSequenceClassification,Trainer,TrainingArguments,DataCollatorWithPadding,AutoModelForSequenceClassification,AutoTokenizer\n",
        "from peft import get_peft_model,LoraConfig,TaskType,prepare_model_for_kbit_training\n",
        "from datasets import load_dataset\n",
        "import torch\n",
        "import warnings\n",
        "\n",
        "checkpoint = [\"bert-base-uncased\",\n",
        "             \"BioMistral/BioMistral-7B\",\n",
        "             \"bigscience/bloom-3b\",\n",
        "             ]\n",
        "bnb_config = BitsAndBytesConfig(load_in_8bit=True)\n",
        "index = 2\n",
        "model = AutoModelForSequenceClassification.from_pretrained(checkpoint[index],\n",
        "#                                                            device_map=\"auto\",\n",
        "                                                           num_labels=2,\n",
        "                                                           torch_dtype=torch.float16, #This reduces the gpu onboard vram usage\n",
        "#                                                            quantization_config = bnb_config,\n",
        "                                                      )\n",
        "\n",
        "peft_config = LoraConfig(\n",
        "                         inference_mode=False,\n",
        "                         r=16,\n",
        "                         lora_alpha = 1024,\n",
        "                         lora_dropout = 0.1,\n",
        "                         bias=\"none\",\n",
        "                         peft_type = \"SEQ_CLS\",\n",
        "                         use_dora=True,\n",
        "                         )\n",
        "\n",
        "# model = prepare_model_for_kbit_training(model)\n",
        "model = get_peft_model(model, peft_config)\n",
        "model.print_trainable_parameters()\n",
        "\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    logits = torch.from_numpy(logits)\n",
        "    labels = torch.from_numpy(labels)\n",
        "\n",
        "    predictions = torch.argmax(logits, dim=-1)\n",
        "    accuracy = (predictions == labels).float().mean()\n",
        "    return {'accuracy': accuracy.item()}\n",
        "\n",
        "# Preprocess the dataset\n",
        "def encode(examples):\n",
        "    outputs = tokenizer(examples['sentence1'], examples['sentence2'], truncation=True, padding='max_length', max_length=128)\n",
        "    outputs['labels'] = examples['label']\n",
        "    return outputs\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint[index])\n",
        "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
        "# model.config.pad_token_id = model.config.eos_token_id\n",
        "# tokenizer.pad_token = tokenizer.eos_token\n",
        "# Dataset\n",
        "dataset = load_dataset('glue', 'mrpc')\n",
        "dataset = dataset.map(encode, batched=True, num_proc=12)\n",
        "dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
        "label_names = dataset['train'].features['label'].names\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "\n",
        "# Training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    logging_dir='./logs_rslora',\n",
        "    # run_name='run_8',\n",
        "    do_train=True,\n",
        "    do_eval=True,\n",
        "    num_train_epochs=10,\n",
        "    learning_rate=2e-4,\n",
        "    logging_strategy='epoch',\n",
        "    per_device_train_batch_size=4,\n",
        "    per_device_eval_batch_size=4,\n",
        "    save_total_limit=1,\n",
        "    save_strategy=\"epoch\",\n",
        "    eval_strategy=\"epoch\",\n",
        "    label_names = [\"labels\"], #Without this Validation Accuracy and Validation Loss wouldn't be logged\n",
        "    fp16=True,\n",
        "#         fsdp=\"full_shard\",\n",
        "#         fsdp_auto_wrap_policy=\"TRANSFORMER_BASED_WRAP\",\n",
        "#         fsdp_transformer_layer_cls_to_wrap = \"BertLayer\",\n",
        ")\n",
        "\n",
        "# Initialize Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=dataset['train'],\n",
        "    eval_dataset=dataset['validation'],\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "trainer.train()\n",
        "trainer.evaluate()"
      ],
      "metadata": {
        "trusted": true,
        "id": "wbPBoJiguQno"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "cat <<'EOT' > ds_config_zero3.json\n",
        "{\n",
        "    \"fp16\": {\n",
        "        \"enabled\": \"auto\",\n",
        "        \"loss_scale\": 0,\n",
        "        \"loss_scale_window\": 1000,\n",
        "        \"initial_scale_power\": 16,\n",
        "        \"hysteresis\": 2,\n",
        "        \"min_loss_scale\": 1\n",
        "    },\n",
        "\n",
        "    \"optimizer\": {\n",
        "        \"type\": \"AdamW\",\n",
        "        \"params\": {\n",
        "            \"lr\": \"auto\",\n",
        "            \"betas\": \"auto\",\n",
        "            \"eps\": \"auto\",\n",
        "            \"weight_decay\": \"auto\"\n",
        "        }\n",
        "    },\n",
        "\n",
        "    \"scheduler\": {\n",
        "        \"type\": \"WarmupLR\",\n",
        "        \"params\": {\n",
        "            \"warmup_min_lr\": \"auto\",\n",
        "            \"warmup_max_lr\": \"auto\",\n",
        "            \"warmup_num_steps\": \"auto\"\n",
        "        }\n",
        "    },\n",
        "\n",
        "    \"zero_optimization\": {\n",
        "        \"stage\": 3,\n",
        "        \"offload_optimizer\": {\n",
        "            \"device\": \"cpu\",\n",
        "            \"pin_memory\": true\n",
        "        },\n",
        "        \"offload_param\": {\n",
        "            \"device\": \"cpu\",\n",
        "            \"pin_memory\": true\n",
        "        },\n",
        "        \"overlap_comm\": true,\n",
        "        \"contiguous_gradients\": true,\n",
        "        \"sub_group_size\": 1e9,\n",
        "        \"reduce_bucket_size\": \"auto\",\n",
        "        \"stage3_prefetch_bucket_size\": \"auto\",\n",
        "        \"stage3_param_persistence_threshold\": \"auto\",\n",
        "        \"stage3_max_live_parameters\": 1e9,\n",
        "        \"stage3_max_reuse_distance\": 1e9,\n",
        "        \"stage3_gather_16bit_weights_on_model_save\": true\n",
        "    },\n",
        "\n",
        "    \"gradient_accumulation_steps\": \"auto\",\n",
        "    \"gradient_clipping\": \"auto\",\n",
        "    \"steps_per_print\": 2000,\n",
        "    \"train_batch_size\": \"auto\",\n",
        "    \"train_micro_batch_size_per_gpu\": \"auto\",\n",
        "    \"wall_clock_breakdown\": false\n",
        "}\n",
        "EOT"
      ],
      "metadata": {
        "trusted": true,
        "id": "g65RxL73uQnq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer,BitsAndBytesConfig,BertForSequenceClassification,Trainer,TrainingArguments,DataCollatorWithPadding,AutoModelForSequenceClassification,AutoTokenizer\n",
        "from peft import get_peft_model,LoraConfig,TaskType,prepare_model_for_kbit_training\n",
        "from datasets import load_dataset\n",
        "import torch\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "def main():\n",
        "    checkpoint = [\"bert-base-uncased\",\n",
        "                 \"BioMistral/BioMistral-7B\",\n",
        "                 \"bigscience/bloom-3b\",\n",
        "                 ]\n",
        "#     bnb_config = BitsAndBytesConfig(load_in_8bit=True)\n",
        "    index = 2\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(checkpoint[index],\n",
        "    #                                                            device_map=\"auto\",\n",
        "                                                               num_labels=2,\n",
        "                                                               torch_dtype=torch.float16, #This reduces the gpu onboard vram usage\n",
        "#                                                                quantization_config = bnb_config,\n",
        "                                                          )\n",
        "\n",
        "    peft_config = LoraConfig(\n",
        "                             inference_mode=False,\n",
        "                             r=16,\n",
        "                             lora_alpha = 1024,\n",
        "                             lora_dropout = 0.1,\n",
        "                             bias=\"none\",\n",
        "                             peft_type = \"SEQ_CLS\",\n",
        "                             use_dora=True,\n",
        "                             )\n",
        "\n",
        "    # model = prepare_model_for_kbit_training(model)\n",
        "    model = get_peft_model(model, peft_config)\n",
        "    model.print_trainable_parameters()\n",
        "\n",
        "\n",
        "    def compute_metrics(eval_pred):\n",
        "        logits, labels = eval_pred\n",
        "        logits = torch.from_numpy(logits)\n",
        "        labels = torch.from_numpy(labels)\n",
        "\n",
        "        predictions = torch.argmax(logits, dim=-1)\n",
        "        accuracy = (predictions == labels).float().mean()\n",
        "        return {'accuracy': accuracy.item()}\n",
        "\n",
        "    # Preprocess the dataset\n",
        "    def encode(examples):\n",
        "        outputs = tokenizer(examples['sentence1'], examples['sentence2'], truncation=True, padding='max_length', max_length=128)\n",
        "        outputs['labels'] = examples['label']\n",
        "        return outputs\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(checkpoint[index])\n",
        "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
        "    # model.config.pad_token_id = model.config.eos_token_id\n",
        "    # tokenizer.pad_token = tokenizer.eos_token\n",
        "    # Dataset\n",
        "    dataset = load_dataset('glue', 'mrpc')\n",
        "    dataset = dataset.map(encode, batched=True, num_proc=12)\n",
        "    dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
        "    label_names = dataset['train'].features['label'].names\n",
        "    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "\n",
        "    # Training arguments\n",
        "    torch.set_grad_enabled(True)\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir='./results',\n",
        "        logging_dir='./logs_rslora',\n",
        "        # run_name='run_8',\n",
        "        do_train=True,\n",
        "        do_eval=True,\n",
        "        num_train_epochs=10,\n",
        "        learning_rate=2e-4,\n",
        "        logging_strategy='epoch',\n",
        "        per_device_train_batch_size=16,\n",
        "        per_device_eval_batch_size=16,\n",
        "        save_total_limit=1,\n",
        "        save_strategy=\"epoch\",\n",
        "        eval_strategy=\"epoch\",\n",
        "        label_names = [\"labels\"], #Without this Validation Accuracy and Validation Loss wouldn't be logged\n",
        "        fp16=True,\n",
        "        deepspeed = \"/kaggle/working/ds_config_zero3.json\",\n",
        "#         gradient_checkpointing=True,\n",
        "#         gradient_accumulation_steps=4,\n",
        "#         fsdp=\"full_shard\",\n",
        "#         fsdp_auto_wrap_policy=\"TRANSFORMER_BASED_WRAP\",\n",
        "#         fsdp_transformer_layer_cls_to_wrap = \"BertLayer\",\n",
        "    )\n",
        "\n",
        "    # Initialize Trainer\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=dataset['train'],\n",
        "        eval_dataset=dataset['validation'],\n",
        "        data_collator=data_collator,\n",
        "        compute_metrics=compute_metrics\n",
        "    )\n",
        "\n",
        "    # Train the model\n",
        "    trainer.train()\n",
        "    trainer.evaluate()\n",
        "if __name__ ==\"__main__\":\n",
        "    main()\n",
        "#     from accelerate import notebook_launcher\n",
        "#     notebook_launcher(main, num_processes=2, mixed_precision=\"fp16\")\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "1wZ-nKfXuQnq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install deepspeed"
      ],
      "metadata": {
        "trusted": true,
        "id": "F2GGa6eQuQnr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !torchrun --nproc_per_node=2 load_ddp_model.py\n",
        "!accelerate launch --num_processes=2 --mixed_precision=\"fp16\" --use_deepspeed trainer.py"
      ],
      "metadata": {
        "trusted": true,
        "id": "c_2BiTWVuQns",
        "outputId": "fe0ac4aa-64e9-47af-8911-e6c25de6d60b"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Some weights of BloomForSequenceClassification were not initialized from the model checkpoint at bigscience/bloom-3b and are newly initialized: ['score.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nSome weights of BloomForSequenceClassification were not initialized from the model checkpoint at bigscience/bloom-3b and are newly initialized: ['score.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\ntrainable params: 5,145,600 || all params: 3,007,708,160 || trainable%: 0.1711\ntrainable params: 5,145,600 || all params: 3,007,708,160 || trainable%: 0.1711\nMap (num_proc=12): 100%|████████████| 3668/3668 [00:10<00:00, 335.58 examples/s]\nMap (num_proc=12): 100%|████████████| 3668/3668 [00:11<00:00, 324.19 examples/s]\nMap (num_proc=12): 100%|███████████████| 408/408 [00:09<00:00, 44.69 examples/s]\nMap (num_proc=12): 100%|███████████████| 408/408 [00:10<00:00, 40.50 examples/s]\nMap (num_proc=12): 100%|████████████| 1725/1725 [00:09<00:00, 180.18 examples/s]\nMap (num_proc=12):  92%|███████████ | 1582/1725 [00:09<00:00, 206.42 examples/s][2024-07-29 18:15:30,042] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)\nMap (num_proc=12): 100%|████████████| 1725/1725 [00:10<00:00, 168.30 examples/s]\n[2024-07-29 18:15:30,931] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n\u001b[93m [WARNING] \u001b[0m async_io requires the dev libaio .so object and headers but these were not found.\n\u001b[93m [WARNING] \u001b[0m async_io: please install the libaio-dev package with apt\n\u001b[93m [WARNING] \u001b[0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.\n\u001b[93m [WARNING] \u001b[0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH\n\u001b[93m [WARNING] \u001b[0m async_io requires the dev libaio .so object and headers but these were not found.\n\u001b[93m [WARNING] \u001b[0m async_io: please install the libaio-dev package with apt\n\u001b[93m [WARNING] \u001b[0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.\n\u001b[93m [WARNING] \u001b[0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH\n\u001b[93m [WARNING] \u001b[0m NVIDIA Inference is only supported on Ampere and newer architectures\n\u001b[93m [WARNING] \u001b[0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1\n\u001b[93m [WARNING] \u001b[0m please install triton==1.0.0 if you want to use sparse attention\n\u001b[93m [WARNING] \u001b[0m NVIDIA Inference is only supported on Ampere and newer architectures\n\u001b[93m [WARNING] \u001b[0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1\n\u001b[93m [WARNING] \u001b[0m please install triton==1.0.0 if you want to use sparse attention\n[2024-07-29 18:15:32,197] [INFO] [comm.py:637:init_distributed] cdb=None\n[2024-07-29 18:15:32,729] [INFO] [comm.py:637:init_distributed] cdb=None\n[2024-07-29 18:15:32,729] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\nUsing /root/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...\nCreating extension directory /root/.cache/torch_extensions/py310_cu121/cpu_adam...\nUsing /root/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...\nCreating extension directory /root/.cache/torch_extensions/py310_cu121/cpu_adam...\nEmitting ninja build file /root/.cache/torch_extensions/py310_cu121/cpu_adam/build.ninja...\nBuilding extension module cpu_adam...\nAllowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n[1/3] c++ -MMD -MF cpu_adam.o.d -DTORCH_EXTENSION_NAME=cpu_adam -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1013\\\" -I/opt/conda/lib/python3.10/site-packages/deepspeed/ops/csrc/includes -isystem /opt/conda/lib/python3.10/site-packages/torch/include -isystem /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.10/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.10/site-packages/torch/include/THC -isystem /opt/conda/include/python3.10 -D_GLIBCXX_USE_CXX11_ABI=1 -fPIC -std=c++17 -O3 -std=c++17 -g -Wno-reorder -L/usr/local/cuda/lib64 -lcudart -lcublas -g -march=native -fopenmp -D__AVX512__ -D__ENABLE_CUDA__ -c /opt/conda/lib/python3.10/site-packages/deepspeed/ops/csrc/adam/cpu_adam.cpp -o cpu_adam.o \n[2/3] c++ -MMD -MF cpu_adam_impl.o.d -DTORCH_EXTENSION_NAME=cpu_adam -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1013\\\" -I/opt/conda/lib/python3.10/site-packages/deepspeed/ops/csrc/includes -isystem /opt/conda/lib/python3.10/site-packages/torch/include -isystem /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.10/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.10/site-packages/torch/include/THC -isystem /opt/conda/include/python3.10 -D_GLIBCXX_USE_CXX11_ABI=1 -fPIC -std=c++17 -O3 -std=c++17 -g -Wno-reorder -L/usr/local/cuda/lib64 -lcudart -lcublas -g -march=native -fopenmp -D__AVX512__ -D__ENABLE_CUDA__ -c /opt/conda/lib/python3.10/site-packages/deepspeed/ops/csrc/adam/cpu_adam_impl.cpp -o cpu_adam_impl.o \n[3/3] c++ cpu_adam.o cpu_adam_impl.o -shared -lcurand -L/opt/conda/lib/python3.10/site-packages/torch/lib -lc10 -ltorch_cpu -ltorch -ltorch_python -o cpu_adam.so\nLoading extension module cpu_adam...\nTime to load cpu_adam op: 42.851970911026 seconds\nLoading extension module cpu_adam...\nTime to load cpu_adam op: 42.907750368118286 seconds\nParameter Offload: Total persistent parameters: 1244160 in 275 params\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mnnilayy\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.17.5 is available!  To upgrade, please run:\n\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.17.4\n\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/wandb/run-20240729_181632-2qov05eg\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33m./results\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/nnilayy/huggingface\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/nnilayy/huggingface/runs/2qov05eg\u001b[0m\n  5%|██▏                                      | 62/1150 [02:49<48:29,  2.67s/it]",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!accelerate launch -h"
      ],
      "metadata": {
        "trusted": true,
        "id": "2IOXRhAwuQnt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loads Model on CPUs RAM\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "model = AutoModelForCausalLM.from_pretrained(\"bigscience/bloom-3b\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bigscience/bloom-3b\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "BHVnQXhkuQnt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Trainer?"
      ],
      "metadata": {
        "trusted": true,
        "id": "v6TywmehuQnv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loads Model on 1 GPU Vram\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "model = AutoModelForCausalLM.from_pretrained(\"bigscience/bloom-3b\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bigscience/bloom-3b\")\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)"
      ],
      "metadata": {
        "trusted": true,
        "id": "w7fwg61zuQnv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loads Model on 1 GPU Vram, in 16 precision\n",
        "# Reduce memory footprint by half\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "model = AutoModelForCausalLM.from_pretrained(\"bigscience/bloom-3b\", torch_dtype=torch.float16)\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bigscience/bloom-3b\")\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)"
      ],
      "metadata": {
        "trusted": true,
        "id": "lHUwL8rBuQnw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM\n",
        "torch.set_default_dtype(torch.float16)\n",
        "if torch.cuda.is_available():\n",
        "    torch.set_default_device('cuda:1')\n",
        "model = AutoModelForCausalLM.from_pretrained(\"bigscience/bloom-3b\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "PmZQypBzuQnx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Parallelism: Same Model Gets Loaded On One GPU\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\"bigscience/bloom-3b\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bigscience/bloom-3b\")\n",
        "model = torch.nn.DataParallel(model)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "PuMfpCrsuQny"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\"bigscience/bloom-3b\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bigscience/bloom-3b\")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    # Use all available GPUs\n",
        "    device_ids = list(range(torch.cuda.device_count()))\n",
        "    model = torch.nn.DataParallel(model, device_ids=device_ids)\n",
        "    model.to('cuda')  # Move model to the default device\n",
        "else:\n",
        "    print(\"CUDA is not available.\")\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "qhEEjv1CuQny"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "dataset = load_dataset('nnilayy/pubmedqa_artificial_128')"
      ],
      "metadata": {
        "trusted": true,
        "id": "SbS9jsnHuQny"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_content=\"\"\"\n",
        "from transformers import BertTokenizer,BitsAndBytesConfig,BertForSequenceClassification,Trainer,TrainingArguments,DataCollatorWithPadding,AutoModelForSequenceClassification,AutoTokenizer\n",
        "from peft import get_peft_model,LoraConfig,TaskType,prepare_model_for_kbit_training\n",
        "from datasets import load_dataset\n",
        "import torch\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "def main():\n",
        "    checkpoint = [\"bert-base-uncased\",\n",
        "                 \"BioMistral/BioMistral-7B\",\n",
        "                 \"bigscience/bloom-3b\",\n",
        "                 ]\n",
        "#     bnb_config = BitsAndBytesConfig(load_in_8bit=True)\n",
        "    index = 2\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(checkpoint[index],\n",
        "    #                                                            device_map=\"auto\",\n",
        "                                                               num_labels=2,\n",
        "                                                               torch_dtype=torch.float16, #This reduces the gpu onboard vram usage\n",
        "#                                                                quantization_config = bnb_config,\n",
        "                                                          )\n",
        "\n",
        "    peft_config = LoraConfig(\n",
        "                             inference_mode=False,\n",
        "                             r=16,\n",
        "                             lora_alpha = 1024,\n",
        "                             lora_dropout = 0.1,\n",
        "                             bias=\"none\",\n",
        "                             peft_type = \"SEQ_CLS\",\n",
        "                             use_dora=True,\n",
        "                             )\n",
        "\n",
        "    # model = prepare_model_for_kbit_training(model)\n",
        "    model = get_peft_model(model, peft_config)\n",
        "    model.print_trainable_parameters()\n",
        "\n",
        "\n",
        "    def compute_metrics(eval_pred):\n",
        "        logits, labels = eval_pred\n",
        "        logits = torch.from_numpy(logits)\n",
        "        labels = torch.from_numpy(labels)\n",
        "\n",
        "        predictions = torch.argmax(logits, dim=-1)\n",
        "        accuracy = (predictions == labels).float().mean()\n",
        "        return {'accuracy': accuracy.item()}\n",
        "\n",
        "    # Preprocess the dataset\n",
        "    def encode(examples):\n",
        "        outputs = tokenizer(examples['sentence1'], examples['sentence2'], truncation=True, padding='max_length', max_length=128)\n",
        "        outputs['labels'] = examples['label']\n",
        "        return outputs\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(checkpoint[index])\n",
        "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
        "    # model.config.pad_token_id = model.config.eos_token_id\n",
        "    # tokenizer.pad_token = tokenizer.eos_token\n",
        "    # Dataset\n",
        "    dataset = load_dataset('glue', 'mrpc')\n",
        "    dataset = dataset.map(encode, batched=True, num_proc=12)\n",
        "    dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
        "    label_names = dataset['train'].features['label'].names\n",
        "    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "\n",
        "    # Training arguments\n",
        "    torch.set_grad_enabled(True)\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir='./results',\n",
        "        logging_dir='./logs_rslora',\n",
        "        # run_name='run_8',\n",
        "        do_train=True,\n",
        "        do_eval=True,\n",
        "        num_train_epochs=10,\n",
        "        learning_rate=2e-4,\n",
        "        logging_strategy='epoch',\n",
        "        per_device_train_batch_size=16,\n",
        "        per_device_eval_batch_size=16,\n",
        "        save_total_limit=1,\n",
        "        save_strategy=\"epoch\",\n",
        "        eval_strategy=\"epoch\",\n",
        "        label_names = [\"labels\"], #Without this Validation Accuracy and Validation Loss wouldn't be logged\n",
        "        fp16=True,\n",
        "        deepspeed = \"/kaggle/working/ds_config_zero3.json\",\n",
        "#         gradient_checkpointing=True,\n",
        "#         gradient_accumulation_steps=4,\n",
        "#         fsdp=\"full_shard\",\n",
        "#         fsdp_auto_wrap_policy=\"TRANSFORMER_BASED_WRAP\",\n",
        "#         fsdp_transformer_layer_cls_to_wrap = \"BertLayer\",\n",
        "    )\n",
        "\n",
        "    # Initialize Trainer\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=dataset['train'],\n",
        "        eval_dataset=dataset['validation'],\n",
        "        data_collator=data_collator,\n",
        "        compute_metrics=compute_metrics\n",
        "    )\n",
        "\n",
        "    # Train the model\n",
        "    trainer.train()\n",
        "    trainer.evaluate()\n",
        "if __name__ ==\"__main__\":\n",
        "    main()\n",
        "#     from accelerate import notebook_launcher\n",
        "#     notebook_launcher(main, num_processes=2, mixed_precision=\"fp16\")\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "# Create and write to the file in the /kaggle/working/ directory\n",
        "file_path = \"/kaggle/working/trainer.py\"\n",
        "with open(file_path, \"w\") as file:\n",
        "    file.write(file_content)\n",
        "\n",
        "print(\"File created successfully in /kaggle/working/\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "5nwwzT5GuQny"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !python -m torch.distributed.launch --nproc_per_node=2 trainer.py\n",
        "# !torchrun --nproc_per_node=2 load_ddp_model.py\n",
        "# !accelerate launch --multi_gpu --mixed_precision=\"fp16\" --num_processes=2 trainer.py\n",
        "!python trainer.py"
      ],
      "metadata": {
        "trusted": true,
        "id": "xoAYE230uQnz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import gc\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()"
      ],
      "metadata": {
        "trusted": true,
        "id": "3TC4IUQpuQnz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}