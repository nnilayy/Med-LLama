{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [],
      "dockerImageVersionId": 30733,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nnilayy/MedGPT/blob/main/stress_testing_gpus_code_with_30b_llms.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the read-only \"../input/\" directory\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n",
        "\n",
        "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n",
        "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "id": "LkAp5BR1HAhD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "trusted": true,
        "id": "tJiw-Ti2HAhG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use a pipeline as a high-level helper\n",
        "from transformers import pipeline\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"Who are you?\"},\n",
        "]\n",
        "pipe = pipeline(\"text-generation\", model=\"BioMistral/BioMistral-7B\")\n",
        "# pipe(messages)"
      ],
      "metadata": {
        "trusted": true,
        "id": "sZ9izt_YHAhH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import psutil\n",
        "import platform\n",
        "try:\n",
        "    import GPUtil\n",
        "except ImportError:\n",
        "    print(\"GPUtil module not installed. GPU information will not be available.\")\n",
        "\n",
        "# CPU Information\n",
        "cpus = os.cpu_count()\n",
        "print(\"Logical CPUs:\", cpus)\n",
        "print(\"Physical CPUs:\", psutil.cpu_count(logical=False))\n",
        "\n",
        "# System Memory\n",
        "ram = psutil.virtual_memory()\n",
        "print(\"Total RAM (GB):\", round(ram.total / (1024 ** 3), 2))\n",
        "print(\"Available RAM (GB):\", round(ram.available / (1024 ** 3), 2))\n",
        "print(\"Used RAM (GB):\", round(ram.used / (1024 ** 3), 2))\n",
        "\n",
        "# Disk Information\n",
        "print(\"\\nDisk Information:\")\n",
        "for partition in psutil.disk_partitions():\n",
        "    try:\n",
        "        usage = psutil.disk_usage(partition.mountpoint)\n",
        "        print(f\"  Mountpoint: {partition.mountpoint}\")\n",
        "        print(f\"    Total Size (GB): {round(usage.total / (1024 ** 3), 2)}\")\n",
        "        print(f\"    Used Space (GB): {round(usage.used / (1024 ** 3), 2)}\")\n",
        "        print(f\"    Free Space (GB): {round(usage.free / (1024 ** 3), 2)}\")\n",
        "        print(f\"    Percentage Used: {usage.percent}%\")\n",
        "    except PermissionError:\n",
        "        print(f\"  No Permission to access {partition.mountpoint}\")\n",
        "\n",
        "# GPU Information (if GPUtil is available)\n",
        "if 'GPUtil' in globals():\n",
        "    gpus = GPUtil.getGPUs()\n",
        "    if gpus:\n",
        "        for i, gpu in enumerate(gpus):\n",
        "            print(f\"\\nGPU {i}: {gpu.name}\")\n",
        "            print(f\"  Total VRAM (GB): {round(gpu.memoryTotal / 1024, 2)}\")\n",
        "            print(f\"  Used VRAM (GB): {round(gpu.memoryUsed / 1024, 2)}\")\n",
        "            print(f\"  Free VRAM (GB): {round(gpu.memoryFree / 1024, 2)}\")\n",
        "            print(f\"  GPU Load (%): {gpu.load * 100}\")\n",
        "    else:\n",
        "        print(\"No GPU found or GPUtil cannot find it.\")\n",
        "else:\n",
        "    print(\"GPU information not available due to missing GPUtil.\")\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "m4hXk7iSHAhI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U flash-attn --no-build-isolation"
      ],
      "metadata": {
        "trusted": true,
        "id": "DGwhCiLlHAhI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "checkpoint  = [\n",
        "    \"BioMistral/BioMistral-7B\",\n",
        "    \"uygarkurt/llama-3-merged-linear\",\n",
        "    \"01-ai/Yi-1.5-9B\",\n",
        "    \"facebook/opt-30b\"\n",
        "]\n",
        "model_to_load = 0\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(checkpoint[model_to_load],\n",
        "                                             torch_dtype=torch.float16,\n",
        "                                             device_map='auto',\n",
        "                                             attn_implementation=\"flash_attention_2\"\n",
        "                                            )\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint[model_to_load], return_token_type_ids=False)"
      ],
      "metadata": {
        "trusted": true,
        "id": "ChVGEsheHAhJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.backends.cuda.enable_mem_efficient_sdp(False)\n",
        "torch.backends.cuda.enable_flash_sdp(False)"
      ],
      "metadata": {
        "trusted": true,
        "id": "oGBueTtoHAhJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "# model = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-v0.1\", torch_dtype=torch.float16, attn_implementation=\"flash_attention_2\", device_map=\"auto\")\n",
        "# tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-v0.1\")\n",
        "\n",
        "prompt = \"My favourite condiment is\"\n",
        "\n",
        "model_inputs = tokenizer([prompt], return_tensors=\"pt\", return_token_type_ids=False).to(\"cuda\")\n",
        "# model.to(device)\n",
        "\n",
        "generated_ids = model.generate(**model_inputs,\n",
        "                               max_length=512,\n",
        "                               do_sample=True,\n",
        "                               skip_special_tokens=True,\n",
        "                              )\n",
        "tokenizer.batch_decode(generated_ids)[0]\n",
        "\n",
        "# input_text = \"\"\"\n",
        "# You are a Doctor,  Help the Patient with the query\n",
        "# Patient: HI, Doctor, I am having fever, and itching near my cock can you suggest me something?\n",
        "# Doctor:\n",
        "# \"\"\"\n",
        "# # Example input\n",
        "# encoded_input = tokenizer.encode(input_text, return_tensors='pt')\n",
        "# encoded_input = encoded_input.to(device)  # Move tensor to the correct device\n",
        "# output_sequences = model.generate(\n",
        "#     input_ids=encoded_input,\n",
        "#     do_sample=True,\n",
        "#     max_length=512,  # You can change this as needed\n",
        "#     num_return_sequences=1  # Number of sentences to generate\n",
        "# )\n",
        "\n",
        "# generated_text = tokenizer.decode(output_sequences[0], skip_special_tokens=True)\n",
        "# print(generated_text)"
      ],
      "metadata": {
        "trusted": true,
        "id": "cuqvEIIgHAhJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "hugging_face_token = \"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\"\n",
        "notebook_login()"
      ],
      "metadata": {
        "trusted": true,
        "id": "R-YPy7zVHAhJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.push_to_hub('nnilayy/biomistral-unmodified')\n",
        "tokenizer.push_to_hub('nnilayy/biomistral-unmodified')"
      ],
      "metadata": {
        "trusted": true,
        "id": "b907ELlFHAhK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Clear GPU (Maybe)\n",
        "import torch\n",
        "# del model\n",
        "# del tokenizer\n",
        "# torch.cuda.empty_cache()\n",
        "\n",
        "\n",
        "# Clear CPU's Memory\n",
        "import gc\n",
        "gc.collect()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-05T10:44:45.556938Z",
          "iopub.execute_input": "2024-07-05T10:44:45.557703Z",
          "iopub.status.idle": "2024-07-05T10:44:47.330913Z",
          "shell.execute_reply.started": "2024-07-05T10:44:45.557669Z",
          "shell.execute_reply": "2024-07-05T10:44:47.329887Z"
        },
        "trusted": true,
        "id": "6XMm9yNFHAhK",
        "outputId": "19491111-f992-4ed9-822a-5f133df8064d"
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 1,
          "output_type": "execute_result",
          "data": {
            "text/plain": "0"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading model on CPUs would crash them, instead loaad them on GPUs, using the parameter\n",
        "# device_map =\"cuda\"/\"auto\" in loading pretrained model\n",
        "# By loading model onto gpus, even if we load multiple models onto gpus, the notebook wouldnt crash but just instead free upt the space which was occupied by the previous model\n",
        "\n",
        "# When you are loading a model, you can load the model either on a cpu or a gpu. After loading these models would\n",
        "# persist in the cpu/gpu and not\n",
        "\n",
        "# When device is mapped to auto, it would try to load mjaority of the model on the gpus, but once gpus\n",
        "# reach their limit the rest of model is loaded onto the cpu, but if the model is still big enoughy\n",
        "# the cpu's ram would be filled completely and the notebook would crash, ultimately being not able to load the\n",
        "# model into memory\n",
        "\n",
        "\n",
        "# Interesting Observation, On the first load of the 30 billion paramter model into memory, the notebook crashed\n",
        "# on the second load of the model, the model loaded perfectly into the memory, without using any of the CPU's ram\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "c9hUGA59HAhL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}