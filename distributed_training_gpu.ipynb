{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [],
      "dockerImageVersionId": 30733,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nnilayy/MedGPT/blob/main/distributed_training_gpu.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install accelerate datasets evaluate transformers"
      ],
      "metadata": {
        "_uuid": "b5780f65-0694-4e20-abf8-e0662796d09d",
        "_cell_guid": "57b902e9-7ca5-485b-a684-dd76d62125ad",
        "jupyter": {
          "outputs_hidden": false
        },
        "trusted": true,
        "id": "9FiIXG4tfDKS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from datasets import load_dataset\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, AdamW, DataCollatorWithPadding\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm.auto import tqdm\n",
        "from accelerate import Accelerator"
      ],
      "metadata": {
        "_uuid": "2fe85a82-beee-491a-9442-22c800671656",
        "_cell_guid": "0e12cb30-b313-4962-81b5-39055c59f197",
        "jupyter": {
          "outputs_hidden": false
        },
        "trusted": true,
        "id": "CI1-M_7MfDKU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Device Type and Count\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "num_devices = torch.cuda.device_count()\n",
        "print(f'Using device: {device}')\n",
        "print(f'Number of available devices: {num_devices}')\n",
        "\n",
        "accelerator = Accelerator()\n",
        "\n",
        "checkpoint = 'bert-base-uncased'\n",
        "model = BertForSequenceClassification.from_pretrained(checkpoint, force_download=True, num_labels=2)\n",
        "tokenizer = BertTokenizer.from_pretrained(checkpoint, force_download=True)\n",
        "\n",
        "# model.to(device)\n",
        "\n",
        "dataset = load_dataset('glue', 'mrpc')\n",
        "\n",
        "def encode(examples):\n",
        "    return tokenizer(examples['sentence1'], examples['sentence2'], truncation=True, padding='max_length', max_length=128)\n",
        "\n",
        "dataset = dataset.map(encode, batched=True)\n",
        "dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n",
        "\n",
        "train_dataset = dataset['train']\n",
        "eval_dataset = dataset['validation']\n",
        "\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=data_collator)\n",
        "eval_loader = DataLoader(eval_dataset, batch_size=32, collate_fn=data_collator)\n",
        "\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-4)\n",
        "\n",
        "model, optimizer, train_loader, eval_loader = accelerator.prepare(model, optimizer, train_loader, eval_loader)"
      ],
      "metadata": {
        "_uuid": "8c80cc62-b3da-4e1a-9f60-9d3d170624d2",
        "_cell_guid": "b3956789-ab54-40c3-9a7e-e930b1d06192",
        "jupyter": {
          "outputs_hidden": false
        },
        "trusted": true,
        "id": "pX7Zf7_-fDKU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(epoch, model, loader, optimizer, device):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    correct_predictions = 0\n",
        "    total_predictions = 0\n",
        "    loop = tqdm(loader, desc=f\"Training Epoch {epoch}\")\n",
        "    for batch in loop:\n",
        "        batch = {k: v for k, v in batch.items() if k in ['input_ids', 'attention_mask', 'labels']}\n",
        "        outputs = model(**batch)\n",
        "        loss = outputs.loss\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        # loss.backward()\n",
        "        accelerator.backward(loss)\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        logits = outputs.logits\n",
        "        predictions = torch.argmax(logits, dim=-1)\n",
        "        correct_predictions += (predictions == batch['labels']).sum().item()\n",
        "        total_predictions += batch['labels'].size(0)\n",
        "        loop.set_postfix(loss=loss.item())\n",
        "\n",
        "    train_accuracy = correct_predictions / total_predictions\n",
        "    train_loss = total_loss / len(loader)\n",
        "    return train_accuracy, train_loss\n",
        "\n",
        "# Evaluation function without tqdm\n",
        "def evaluate(model, loader, device):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    correct_predictions = 0\n",
        "    total_predictions = 0\n",
        "    for batch in loader:\n",
        "        batch = {k: v for k, v in batch.items() if k in ['input_ids', 'attention_mask', 'labels']}\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**batch)\n",
        "            logits = outputs.logits\n",
        "            loss = outputs.loss\n",
        "            total_loss += loss.item()\n",
        "            predictions = torch.argmax(logits, dim=-1)\n",
        "            correct_predictions += (predictions == batch['labels']).sum().item()\n",
        "            total_predictions += batch['labels'].size(0)\n",
        "\n",
        "    validation_accuracy = correct_predictions / total_predictions\n",
        "    validation_loss = total_loss / len(loader)\n",
        "    return validation_accuracy, validation_loss"
      ],
      "metadata": {
        "_uuid": "d29e73ac-6d21-4134-be22-23a76442b688",
        "_cell_guid": "e1c061c4-19f6-442d-bdc6-6d7bb47448ad",
        "jupyter": {
          "outputs_hidden": false
        },
        "trusted": true,
        "id": "4Yu-E9mIfDKV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the training and evaluation\n",
        "for epoch in range(1, 6):  # Training for 3 epochs\n",
        "    train_accuracy, train_loss = train(epoch, model, train_loader, optimizer, device)\n",
        "    validation_accuracy, validation_loss = evaluate(model, eval_loader, device)\n",
        "    print(f\"Training Accuracy: {train_accuracy:.4f}, Validation Accuracy: {validation_accuracy:.4f}\")\n",
        "    print(f\"Training Loss: {train_loss:.4f}, Validation Loss: {validation_loss:.4f}\")"
      ],
      "metadata": {
        "_uuid": "c8bb3d43-991f-49c4-88ba-f5853d0b2418",
        "_cell_guid": "65f7b730-86fa-46bd-9f81-3a6098074b1c",
        "jupyter": {
          "outputs_hidden": false
        },
        "trusted": true,
        "id": "rneS_o9cfDKV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the content you want to write to the file\n",
        "file_content = \"\"\"\n",
        "def hello_world():\n",
        "    print(\"Hello, World!\")\n",
        "\n",
        "if __name__==\"__main__\":\n",
        "    hello_world()\n",
        "\"\"\"\n",
        "\n",
        "# Create and write to the file in the /kaggle/working/ directory\n",
        "file_path = \"/kaggle/working/example.py\"\n",
        "with open(file_path, \"w\") as file:\n",
        "    file.write(file_content)\n",
        "\n",
        "print(\"File created successfully in /kaggle/working/\")\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "WNzjNzRkfDKV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python example.py"
      ],
      "metadata": {
        "trusted": true,
        "id": "YbGFMmU1fDKW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_content=\"\"\"\n",
        "\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, AdamW, DataCollatorWithPadding\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm.auto import tqdm\n",
        "from accelerate import Accelerator\n",
        "\n",
        "def main():\n",
        "    accelerator = Accelerator()\n",
        "    print(\"Process {}/{} starting\".format(accelerator.process_index, accelerator.num_processes))\n",
        "\n",
        "    device = accelerator.device\n",
        "    print(f'Using device: {device}', flush=True) if accelerator.is_local_main_process else None\n",
        "\n",
        "    checkpoint = 'bert-base-uncased'\n",
        "    model = BertForSequenceClassification.from_pretrained(checkpoint, num_labels=2)\n",
        "    tokenizer = BertTokenizer.from_pretrained(checkpoint)\n",
        "\n",
        "    dataset = load_dataset('glue', 'mrpc')\n",
        "    def encode(examples):\n",
        "        return tokenizer(examples['sentence1'], examples['sentence2'], truncation=True, padding='max_length', max_length=128)\n",
        "\n",
        "    dataset = dataset.map(encode, batched=True)\n",
        "    dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n",
        "\n",
        "    train_dataset = dataset['train']\n",
        "    eval_dataset = dataset['validation']\n",
        "    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=data_collator)\n",
        "    eval_loader = DataLoader(eval_dataset, batch_size=32, collate_fn=data_collator)\n",
        "\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=5e-4)\n",
        "    model, optimizer, train_loader, eval_loader = accelerator.prepare(model, optimizer, train_loader, eval_loader)\n",
        "\n",
        "    def train(epoch, model, loader, optimizer, device):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        loop = tqdm(loader, desc=f\"Training Epoch {epoch}\", disable=not accelerator.is_local_main_process)\n",
        "        for batch in loop:\n",
        "            outputs = model(**batch)\n",
        "            loss = outputs.loss\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            accelerator.backward(loss)\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            loop.set_postfix(loss=loss.item())\n",
        "\n",
        "        return total_loss / len(loader)\n",
        "\n",
        "    def evaluate(model, loader, device):\n",
        "        model.eval()\n",
        "        total_loss = 0\n",
        "        loop = tqdm(loader, desc=\"Evaluating\", disable=not accelerator.is_local_main_process)\n",
        "        for batch in loop:\n",
        "            with torch.no_grad():\n",
        "                outputs = model(**batch)\n",
        "                loss = outputs.loss\n",
        "                total_loss += loss.item()\n",
        "                loop.set_postfix(loss=loss.item())\n",
        "\n",
        "        return total_loss / len(loader)\n",
        "\n",
        "    for epoch in range(1, 6):  # Training for 5 epochs\n",
        "        train_loss = train(epoch, model, train_loader, optimizer, device)\n",
        "        validation_loss = evaluate(model, eval_loader, device)\n",
        "        if accelerator.is_local_main_process:\n",
        "            print(f\"Training Loss: {train_loss:.4f}, Validation Loss: {validation_loss:.4f}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "# Create and write to the file in the /kaggle/working/ directory\n",
        "file_path = \"/kaggle/working/train_script.py\"\n",
        "with open(file_path, \"w\") as file:\n",
        "    file.write(file_content)\n",
        "\n",
        "print(\"File created successfully in /kaggle/working/\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "qc1jOPpQfDKW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!accelerate config"
      ],
      "metadata": {
        "trusted": true,
        "id": "im-6-R55fDKW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !accelerate launch --multi_gpu --num_processes=2 train_script.py\n",
        "# !accelerate launch --multi_gpu --mixed_precision=fp16 --num_processes=2 train_script.py\n",
        "# !accelerate launch --multi_gpu --mixed_precision=fp16 --num_processes=2 train_script.py"
      ],
      "metadata": {
        "execution": {
          "iopub.status.idle": "2024-06-23T12:37:54.016790Z",
          "shell.execute_reply.started": "2024-06-23T12:34:03.282444Z",
          "shell.execute_reply": "2024-06-23T12:37:54.015640Z"
        },
        "trusted": true,
        "id": "g2UqDGgkfDKW",
        "outputId": "4245aaee-c314-4b5a-e371-6a1a15d7a08b"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Training Epoch 1: 100%|█████████████| 58/58 [00:38<00:00,  1.50it/s, loss=0.897]\nEvaluating: 100%|█████████████████████| 7/7 [00:01<00:00,  4.91it/s, loss=0.598]\nTraining Loss: 0.6477, Validation Loss: 0.6429\nTraining Epoch 2: 100%|█████████████| 58/58 [00:41<00:00,  1.41it/s, loss=0.698]\nEvaluating: 100%|█████████████████████| 7/7 [00:01<00:00,  4.71it/s, loss=0.594]\nTraining Loss: 0.6470, Validation Loss: 0.6327\nTraining Epoch 3: 100%|█████████████| 58/58 [00:39<00:00,  1.45it/s, loss=0.582]\nEvaluating: 100%|█████████████████████| 7/7 [00:01<00:00,  4.84it/s, loss=0.602]\nTraining Loss: 0.6409, Validation Loss: 0.6287\nTraining Epoch 4: 100%|█████████████| 58/58 [00:40<00:00,  1.44it/s, loss=0.519]\nEvaluating: 100%|█████████████████████| 7/7 [00:01<00:00,  4.77it/s, loss=0.594]\nTraining Loss: 0.6387, Validation Loss: 0.6301\nTraining Epoch 5: 100%|█████████████| 58/58 [00:39<00:00,  1.45it/s, loss=0.528]\nEvaluating: 100%|█████████████████████| 7/7 [00:01<00:00,  4.83it/s, loss=0.596]\nTraining Loss: 0.6359, Validation Loss: 0.6403\n",
          "output_type": "stream"
        }
      ]
    }
  ]
}