{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tJiw-Ti2HAhG",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m4hXk7iSHAhI",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import psutil\n",
        "import platform\n",
        "try:\n",
        "    import GPUtil\n",
        "except ImportError:\n",
        "    print(\"GPUtil module not installed. GPU information will not be available.\")\n",
        "\n",
        "# CPU Information\n",
        "cpus = os.cpu_count()\n",
        "print(\"Logical CPUs:\", cpus)\n",
        "print(\"Physical CPUs:\", psutil.cpu_count(logical=False))\n",
        "\n",
        "# System Memory\n",
        "ram = psutil.virtual_memory()\n",
        "print(\"Total RAM (GB):\", round(ram.total / (1024 ** 3), 2))\n",
        "print(\"Available RAM (GB):\", round(ram.available / (1024 ** 3), 2))\n",
        "print(\"Used RAM (GB):\", round(ram.used / (1024 ** 3), 2))\n",
        "\n",
        "# Disk Information\n",
        "print(\"\\nDisk Information:\")\n",
        "for partition in psutil.disk_partitions():\n",
        "    try:\n",
        "        usage = psutil.disk_usage(partition.mountpoint)\n",
        "        print(f\"  Mountpoint: {partition.mountpoint}\")\n",
        "        print(f\"    Total Size (GB): {round(usage.total / (1024 ** 3), 2)}\")\n",
        "        print(f\"    Used Space (GB): {round(usage.used / (1024 ** 3), 2)}\")\n",
        "        print(f\"    Free Space (GB): {round(usage.free / (1024 ** 3), 2)}\")\n",
        "        print(f\"    Percentage Used: {usage.percent}%\")\n",
        "    except PermissionError:\n",
        "        print(f\"  No Permission to access {partition.mountpoint}\")\n",
        "\n",
        "# GPU Information (if GPUtil is available)\n",
        "if 'GPUtil' in globals():\n",
        "    gpus = GPUtil.getGPUs()\n",
        "    if gpus:\n",
        "        for i, gpu in enumerate(gpus):\n",
        "            print(f\"\\nGPU {i}: {gpu.name}\")\n",
        "            print(f\"  Total VRAM (GB): {round(gpu.memoryTotal / 1024, 2)}\")\n",
        "            print(f\"  Used VRAM (GB): {round(gpu.memoryUsed / 1024, 2)}\")\n",
        "            print(f\"  Free VRAM (GB): {round(gpu.memoryFree / 1024, 2)}\")\n",
        "            print(f\"  GPU Load (%): {gpu.load * 100}\")\n",
        "    else:\n",
        "        print(\"No GPU found or GPUtil cannot find it.\")\n",
        "else:\n",
        "    print(\"GPU information not available due to missing GPUtil.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Device Type and Count\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "num_devices = torch.cuda.device_count()\n",
        "print(f'Using device: {device}')\n",
        "print(f'Number of available devices: {num_devices}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import gc\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ChVGEsheHAhJ",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "checkpoint  = [\n",
        "    \"BioMistral/BioMistral-7B\",\n",
        "    \"uygarkurt/llama-3-merged-linear\",\n",
        "    \"01-ai/Yi-1.5-9B\",\n",
        "    \"facebook/opt-30b\"\n",
        "]\n",
        "model_to_load = 0\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(checkpoint[model_to_load],\n",
        "                                             torch_dtype=torch.float16,\n",
        "                                             device_map='auto',\n",
        "                                             attn_implementation=\"flash_attention_2\"\n",
        "                                            )\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint[model_to_load], return_token_type_ids=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oGBueTtoHAhJ",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "torch.backends.cuda.enable_mem_efficient_sdp(False)\n",
        "torch.backends.cuda.enable_flash_sdp(False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# !accelerate launch --multi_gpu --num_processes=2 train_script.py\n",
        "# !accelerate launch --multi_gpu --mixed_precision=fp16 --num_processes=2 train_script.py\n",
        "# !accelerate launch --multi_gpu --mixed_precision=fp16 --num_processes=2 train_script.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c9hUGA59HAhL",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Loading model on CPUs would crash them, instead loaad them on GPUs, using the parameter\n",
        "# device_map =\"cuda\"/\"auto\" in loading pretrained model\n",
        "# By loading model onto gpus, even if we load multiple models onto gpus, the notebook wouldnt crash but just instead free upt the space which was occupied by the previous model\n",
        "\n",
        "# When you are loading a model, you can load the model either on a cpu or a gpu. After loading these models would\n",
        "# persist in the cpu/gpu and not\n",
        "\n",
        "# When device is mapped to auto, it would try to load mjaority of the model on the gpus, but once gpus\n",
        "# reach their limit the rest of model is loaded onto the cpu, but if the model is still big enoughy\n",
        "# the cpu's ram would be filled completely and the notebook would crash, ultimately being not able to load the\n",
        "# model into memory\n",
        "\n",
        "\n",
        "# Interesting Observation, On the first load of the 30 billion paramter model into memory, the notebook crashed\n",
        "# on the second load of the model, the model loaded perfectly into the memory, without using any of the CPU's ram\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [],
      "dockerImageVersionId": 30733,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
