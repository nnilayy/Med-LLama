{"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30761,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## API KEYS","metadata":{}},{"cell_type":"code","source":"import os\nimport wandb\nfrom huggingface_hub import HfApi\nfrom kaggle_secrets import UserSecretsClient\n\n# LOADING API-KEYS\nuser_secrets = UserSecretsClient()\n\n# HUGGINGFACE LOGIN\nhugging_face_token = user_secrets.get_secret(\"HUGGING_FACE_API_KEY\")\napi = HfApi(token=hugging_face_token)\n\n# WANDB LOGIN\nwandb_api_token = user_secrets.get_secret(\"WANDB_API_KEY\")\nwandb.login(key = wandb_api_token)","metadata":{"execution":{"iopub.status.busy":"2024-09-02T16:52:46.834554Z","iopub.execute_input":"2024-09-02T16:52:46.834935Z","iopub.status.idle":"2024-09-02T16:52:49.555175Z","shell.execute_reply.started":"2024-09-02T16:52:46.834898Z","shell.execute_reply":"2024-09-02T16:52:49.554321Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}]},{"cell_type":"markdown","source":"## LOADING MODEL AND TOKENIZERS","metadata":{}},{"cell_type":"code","source":"import torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n# tokenizer.pad_token = tokenizer.eos_token\ncheckpoint = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\ntokenizer = AutoTokenizer.from_pretrained(checkpoint, token=hugging_face_token)\nmodel = AutoModelForCausalLM.from_pretrained(checkpoint, \n                                             device_map=\"auto\",\n                                             torch_dtype=torch.float16,\n                                             token=hugging_face_token,\n                                            )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import TextStreamer\nstreamer = TextStreamer(\n    tokenizer,\n    skip_prompt=True,\n    skip_special_tokens=True,\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from datasets import load_dataset\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\n\n# Load dataset\ndataset = load_dataset(\"pubmed_qa\", \"pqa_labeled\")\ntest_set = dataset[\"train\"]\n\n# Load model and tokenizer\ncheckpoint = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\ntokenizer = AutoTokenizer.from_pretrained(checkpoint, token=hugging_face_token)\nmodel = AutoModelForCausalLM.from_pretrained(checkpoint, \n                                             device_map=\"auto\",\n                                             torch_dtype=torch.float16,\n                                             token=hugging_face_token)\ntokenizer.pad_token = tokenizer.eos_token\ndevice = next(model.parameters()).device\n\ndef collate_fn(batch):\n    input_texts = [f\"Question: {item['question']}\\nAnswer:\" for item in batch]\n    inputs = tokenizer(input_texts, return_tensors=\"pt\", padding=True, truncation=True)\n    return {\n        'inputs': inputs,\n        'true_answers': [item['final_decision'] for item in batch]\n    }\n\nbatch_size = 8  # Adjust based on your GPU memory\ndataloader = DataLoader(test_set, batch_size=batch_size, collate_fn=collate_fn)\n\n# Evaluation loop\nresults = []\ntotal_batches = len(dataloader)\n\nmodel.eval()\nfor batch in tqdm(dataloader, total=total_batches, desc=\"Evaluating\"):\n    inputs = batch['inputs']\n    inputs = {k: v.to(device) for k, v in inputs.items()}\n    \n    with torch.no_grad():\n        outputs = model.generate(**inputs, max_length=1024)\n    \n    generated_answers = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n    \n    for true_answer, generated_answer in zip(batch['true_answers'], generated_answers):\n        results.append({\n            \"true_answer\": true_answer,\n            \"generated_answer\": generated_answer\n        })\n\n# Calculate metrics\ncorrect = sum(1 for r in results if r[\"true_answer\"] in r[\"generated_answer\"])\naccuracy = correct / len(results)\nprint(f\"Accuracy: {accuracy:.2f}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## DATASET HANDLING","metadata":{}},{"cell_type":"code","source":"# from datasets import load_dataset\nclass DataHelper:\n    def __init__(self):\n        self.dataset = None\n        self.tokenizer = tokenizer\n        self.tokenizer.pad_token = self.tokenizer.eos_token\n\n        self.system_instruction = \"You are a Helpful AI Assistant.\"\n        self.user_instruction = \"Please answer the following Question: \"\n        self.user_query = None\n        \n        # Config Columns\n        self.user_query_column = None        \n        self.columns_to_tokenize = None    \n\n# DATASETS CLASS\n    def load_dataset(self, dataset):\n        self.dataset = dataset\n        return self.dataset\n\n    def __len__(self):\n        return len(self.dataset)\n\n    def __getitem__(self, idx):\n        return {\n            'input_ids': self.dataset['input_ids'][idx].unsqueeze(0),\n            'attention_mask': self.dataset['attention_mask'][idx].unsqueeze(0),\n            'token_type_ids': self.dataset.get('token_type_ids', torch.tensor([]))[idx].unsqueeze(0) if 'token_type_ids' in self.dataset else None\n            }\n\n# LOADING IMPORTANT COLUMNS\n    def load_config_columns(self, columns_dictionary):\n        self.user_query_column = columns_dictionary[\"user_query_column\"]\n        self.columns_to_tokenize = columns_dictionary[\"columns_to_tokenize\"]\n        \n# SYSTEM & USER PROMPT\n    def set_system_instruction(self, system_instruction):\n        self.system_instruction = system_instruction\n        return self.system_instruction\n    \n    def set_user_instruction(self, user_instruction):\n        self.user_instruction = user_instruction\n        return self.user_instruction\n    \n    def set_user_query(self, user_query):\n        self.user_query = user_query\n        return self.user_query\n\n# HANDLING INPUT COLUMN\n    def handle_input_columns(self):\n        pass\n\n# LOADING TOKENIZER\n    def load_tokenizer(self, tokenizer):\n        self.tokenizer = tokenizer\n        return self.tokenizer\n    \n# FORMATTING DATASET CODE\n    def convert_input_to_chat_template(self):\n        message = [\n            {\"role\": \"system\", \"content\": self.system_instruction},\n            {\"role\": \"user\", \"content\": self.user_instruction + self.user_query}\n        ]\n        formatted_input = self.tokenizer.apply_chat_template(message,\n                                                                tokenize=False,\n                                                                add_generation_prompt=True,\n                                                                return_tensors=\"pt\"\n                                                            )\n        return formatted_input\n\n    def create_chat_template_dataset(self, example):\n        self.user_query = example[self.user_query_column]\n        example['training_input'] = self.convert_input_to_chat_template()\n        return example\n\n    def format_dataset(self, dataset):        \n        formatted_dataset = dataset.map(self.create_chat_template_dataset)\n        return formatted_dataset\n    \n# TOKENIZATION CODE    \n    def tokenization_function(self, example):\n        return self.tokenizer(example[self.columns_to_tokenize],\n                                padding=True,\n                                max_length=1024,\n                                truncation=True,\n                                return_tensors=\"pt\"\n                                )\n    \n    def tokenize_dataset(self, dataset):\n        tokenized_dataset = dataset.map(self.tokenization_function,\n                                                       batched=True, \n                                                       batch_size=128, \n                                                       num_proc=8\n                                                      )\n        return tokenized_dataset    \n    \n    def clean_up_dataset(self, base_dataset, tokenized_dataset):\n        base_dataset_columns = list(base_dataset.features.keys())\n        tokenized_dataset_columns = list(tokenized_dataset.features.keys())\n        final_columns = list(set(tokenized_dataset_columns) - set(base_dataset_columns))\n        \n        final_dataset = tokenized_dataset.remove_columns(base_dataset_columns)\n        final_dataset.set_format(type='pt', columns=final_columns, output_all_columns=True)\n        return final_dataset","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# LOADING DATASET\nfrom datasets import load_dataset\ndataset = load_dataset(\"medalpaca/medical_meadow_medqa\")\ndataset","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Extracting Datasets\ntrain_dataset = dataset['train']\ntest_dataset = dataset['test']\nvalidation_dataset = dataset['validation']\ndatasets = [train_dataset, test_dataset, validation_dataset]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"datahelper = DataHelper(tokenizer=tokenizer, user_query_column = \"input\", columns_to_tokenize=\"training_input\")\n\nformatted_datasets = [datahelper.format_dataset(dataset) for dataset in datasets]\ntokenized_datasets = [datahelper.tokenize_dataset(dataset) for dataset in formatted_datasets]\n\ntrain_dataset, test_dataset, validation_dataset = tokenized_datasets","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# tokenized_datasets\n# datahelper.clean_up_dataset(base_dataset=dataset['train'], tokenized_dataset=tokenized_dataset)\n# train_dataset = dataset['train'].map(preprocessing, batched=True, batch_size=32)\n# train_dataset = train_dataset.remove_columns(['input', 'instruction', 'output', 'final_text'])\n# train_dataset.set_format(type='pt', columns=['input_ids', 'attention_mask'], output_all_columns=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with tqdm(total=len(train_dataset.batch(batch_size=8)), desc=\"Generating responses\", unit=\"batch\") as pbar:\n    for batch in train_dataset.batch(batch_size=8):\n        batch = {k: v.to(\"cuda:0\") for k, v in batch.items() if isinstance(v, torch.Tensor)}\n        \n        with torch.no_grad():\n            output = model.generate(**batch,max_length=1024)\n            results.append(output)\n    #         decoded_output = tokenizer.decode(output[0], skip_special_tokens=True)\n    #         results.append(decoded_output)\n            pbar.update(1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Trainer Code","metadata":{}},{"cell_type":"code","source":"import torch\nfrom transformers import Trainer, TrainingArguments\n\ntraining_args = TrainingArguments(\n    output_dir='./results',\n    num_train_epochs=1,\n    per_device_train_batch_size=28,\n    save_total_limit=1,\n    eval_strategy=\"steps\",\n    save_strategy=\"epoch\",\n    save_steps = 100,\n    # label_names = ['not_equivalent', 'equivalent'],\n    fp16=torch.cuda.is_available()  # Use mixed precision if GPUs support it\n)\n\n# Initialize Trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=dataset['train'],\n    eval_dataset=dataset['validation'],\n    data_collator=data_collator,\n    compute_metrics=compute_metrics\n)\n\n# Train the model\ntrainer.train()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install evaluate peft transformers==4.42.0","metadata":{"execution":{"iopub.status.busy":"2024-09-02T16:03:08.969307Z","iopub.execute_input":"2024-09-02T16:03:08.970071Z","iopub.status.idle":"2024-09-02T16:03:34.980100Z","shell.execute_reply.started":"2024-09-02T16:03:08.970031Z","shell.execute_reply":"2024-09-02T16:03:34.978959Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Collecting evaluate\n  Downloading evaluate-0.4.2-py3-none-any.whl.metadata (9.3 kB)\nCollecting peft\n  Downloading peft-0.12.0-py3-none-any.whl.metadata (13 kB)\nCollecting transformers==4.42.0\n  Downloading transformers-4.42.0-py3-none-any.whl.metadata (43 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m690.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:--:--\u001b[0m\n\u001b[?25hRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers==4.42.0) (3.15.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/conda/lib/python3.10/site-packages (from transformers==4.42.0) (0.24.6)\nRequirement already satisfied: numpy<2.0,>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.42.0) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers==4.42.0) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.42.0) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.42.0) (2024.5.15)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers==4.42.0) (2.32.3)\nRequirement already satisfied: tokenizers<0.20,>=0.19 in /opt/conda/lib/python3.10/site-packages (from transformers==4.42.0) (0.19.1)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.42.0) (0.4.4)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers==4.42.0) (4.66.4)\nRequirement already satisfied: datasets>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.21.0)\nRequirement already satisfied: dill in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.2.2)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from evaluate) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.70.16)\nRequirement already satisfied: fsspec>=2021.05.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.6.1)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from peft) (5.9.3)\nRequirement already satisfied: torch>=1.13.0 in /opt/conda/lib/python3.10/site-packages (from peft) (2.4.0)\nRequirement already satisfied: accelerate>=0.21.0 in /opt/conda/lib/python3.10/site-packages (from peft) (0.33.0)\nRequirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (16.1.0)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (3.9.5)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.42.0) (4.12.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers==4.42.0) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.42.0) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.42.0) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.42.0) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.42.0) (2024.7.4)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (1.13.2)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.1.4)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2024.1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2024.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.2.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.5)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->peft) (2.1.5)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.13.0->peft) (1.3.0)\nDownloading transformers-4.42.0-py3-none-any.whl (9.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.3/9.3 MB\u001b[0m \u001b[31m26.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading evaluate-0.4.2-py3-none-any.whl (84 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading peft-0.12.0-py3-none-any.whl (296 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m296.4/296.4 kB\u001b[0m \u001b[31m21.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: transformers, peft, evaluate\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.44.0\n    Uninstalling transformers-4.44.0:\n      Successfully uninstalled transformers-4.44.0\nSuccessfully installed evaluate-0.4.2 peft-0.12.0 transformers-4.42.0\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\nimport torch\nimport warnings\nimport numpy as np\nfrom time import time\nfrom evaluate import load\nfrom datasets import load_dataset\nfrom transformers import DataCollatorWithPadding\nfrom peft.utils import get_peft_model_state_dict\nfrom transformers import TrainingArguments, Trainer\nfrom peft import get_peft_model, LoraConfig, TaskType, prepare_model_for_kbit_training\nfrom transformers import AutoTokenizer, BitsAndBytesConfig, AutoModelForCausalLM, AutoModelForSequenceClassification, AutoTokenizer, BertForSequenceClassification\n\n    \nwarnings.filterwarnings(\"ignore\", category=RuntimeWarning)\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n\nfrom transformers import set_seed\nset_seed(42)\n\ndef main():\n    def compute_metrics(eval_pred):\n        logits, labels = eval_pred\n        predictions = np.argmax(logits, axis=-1)\n        return accuracy.compute(predictions=predictions, references=labels)\n\n    def encode(examples):\n        output = tokenizer(examples['sentence1'], \n                           examples['sentence2'], \n                           truncation=True, \n                           padding='max_length', \n                           max_length=128,\n                          )\n        \n        output['labels'] = examples['label']\n        return output\n\n# MODEL\n    checkpoint = \"bert-base-uncased\"\n    model = AutoModelForSequenceClassification.from_pretrained(checkpoint,\n                                                  #torch_dtype=torch.float16,\n                                                 )\n\n#     peft_config = LoraConfig(inference_mode=False,\n#                              r=32,\n#                              lora_alpha = 512,\n#                              lora_dropout = 0.1,\n#                              bias=\"none\",\n#                              peft_type = TaskType.SEQ_CLS, #\" CAUSAL_LM\"\n#                              )\n\n#     model = prepare_model_for_kbit_training(model)\n#     model = get_peft_model(model, peft_config)\n#     model.print_trainable_parameters()\n\n# TYPICAL TRAINING CODE\n    accuracy = load(\"accuracy\")\n    tokenizer = AutoTokenizer.from_pretrained(checkpoint, padding_side=\"right\")\n    tokenizer.pad_token = tokenizer.eos_token\n    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n    \n#   DATASET PREP\n    dataset = load_dataset(\"glue\", \"mrpc\")\n    dataset = dataset.map(encode, batched=True)    \n    dataset = dataset.remove_columns(['sentence1', 'sentence2', 'label', 'idx'])\n    dataset.set_format(type='pt', columns=['input_ids', 'attention_mask', 'labels',], output_all_columns=True)\n    data_collator = DataCollatorWithPadding(tokenizer)\n    \n        \n# TRAINING ARGUMENTS\n    training_args = TrainingArguments(\n    # DIRECTORIES FOR SAVING AND LOGGING\n        output_dir=\"/kaggle/working/glue_model_checkpointing_test-8\",\n        logging_dir =  \"/kaggle/working/logs\", \n    \n    #  BASIC PARAMS\n        num_train_epochs=5,\n        fp16=True,\n        seed=42,\n        data_seed=42,\n        \n    # OPTIMIZER SETUP\n        optim=\"rmsprop\",\n        learning_rate=1e-4,\n        lr_scheduler_type=\"cosine\",\n        #lr_scheduler_kwargs={\"power\": 2.0},\n        warmup_ratio=0.2,\n        #warmup_steps=200,\n        \n    # DATA RELATED ARGUMENTS\n        per_device_train_batch_size=16,\n        per_device_eval_batch_size=16,\n        dataloader_num_workers=4, # Reduces Training time by a decent percentage\n        dataloader_pin_memory=True,\n        dataloader_persistent_workers=True, \n        ddp_find_unused_parameters=False,        \n        \n    # LOGGING\n        logging_strategy=\"epoch\", # Logs the Training Loss\n        label_names = ['labels'], # If Peft is off, keep this off doesnt do anything, if Peft is on, Logs the Validation Loss and Validation Accuracy\n        #report_to = tensorboard\n        \n    # EVALUATION\n        eval_strategy=\"epoch\", # Doesnt Evaluate the model per epoch, Reducing the training time\n        #eval_steps        \n        \n    # SAVING TO HUB\n        save_strategy=\"epoch\",\n        save_total_limit=1,\n        push_to_hub=True,\n        hub_token = hugging_face_token,\n        hub_strategy=\"every_save\",\n        hub_model_id=\"nnilayy/glue_model_checkpointing_test-8\",\n        \n#     SAVING VRAM\n#         torch_empty_cache_steps=40, #Clears vram cache during training after a few steps\n#         gradient_checkpointing=True,\n#         gradient_accumulation_steps=4,\n    )\n\n# TRAINER CONSTRUCTOR\n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        train_dataset=dataset[\"train\"],\n        eval_dataset=dataset[\"test\"],\n        tokenizer=tokenizer,\n        data_collator=data_collator,\n        compute_metrics=compute_metrics,\n    )\n\n# Pushing Tokenizer, Model Card, Label Mapping to Hub \n#     tokenizer.push_to_hub(\"nnilayy/glue_model_checkpointing_test-8\")\n#     model.config.label2id = {'equivalent': 0, 'not_equivalent': 1}\n#     model.config.id2label = {0: 'equivalent', 1: 'not_equivalent'}\n#     model.config.push_to_hub(\"nnilayy/glue_model_checkpointing_test-8\")\n\n    trainer.train()\n    model.save_pretrained(\"/kaggle/working/test-model-5\")\n\nif __name__ == \"__main__\":\n    from accelerate import notebook_launcher\n    notebook_launcher(main, num_processes=2)","metadata":{"execution":{"iopub.status.busy":"2024-09-02T16:53:05.646101Z","iopub.execute_input":"2024-09-02T16:53:05.647161Z","iopub.status.idle":"2024-09-02T16:55:42.762671Z","shell.execute_reply.started":"2024-09-02T16:53:05.647105Z","shell.execute_reply":"2024-09-02T16:55:42.761635Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Launching training on 2 GPUs.\n","output_type":"stream"},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/4.20k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6c7e19f9a2c544f0a27d034039b6d835"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/4.20k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ec5495ea6b2145049b73ea01ceb938fa"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4a2c143c892447778462f881d225a23a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c14a4b3c5bbe40adb531165d69ba1c12"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0eef925a836644aba1300c1182d818e0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading readme:   0%|          | 0.00/35.3k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c09bb2af12a44f11816c282caac9ffda"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/649k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"80575a23739a4add9f3f82197af081c0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/75.7k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a7c053d6e1ca439280540d2081069a57"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/308k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"05b47323eb3f4a09bfd380a1584be0e5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/3668 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8f20ff361dec4659a007032a5e1143e9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/408 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a3b166bcae194ab0a8ce166064e44c63"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/1725 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"495cfb33666a4524b33f5980278a37aa"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/3668 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ecb0a842a7f24d0fa047e2e94ea88f17"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/3668 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3a1a380ccbcd4354a881d9aa9477043a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/408 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fd9ec3582bc74b24b0c0f8f3fb447291"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/408 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c3b11e4b6c4948118c961f6f8c46e574"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1725 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3afb7bd0e1f64f1086c548b9d1f213d3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1725 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b9eadc29beb44be3b343da41668892a7"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:488: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:488: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mnnilayy\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.17.8 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.17.7"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20240902_165319-sjwcsuni</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/nnilayy/huggingface/runs/sjwcsuni' target=\"_blank\">/kaggle/working/glue_model_checkpointing_test-8</a></strong> to <a href='https://wandb.ai/nnilayy/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/nnilayy/huggingface' target=\"_blank\">https://wandb.ai/nnilayy/huggingface</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/nnilayy/huggingface/runs/sjwcsuni' target=\"_blank\">https://wandb.ai/nnilayy/huggingface/runs/sjwcsuni</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='575' max='575' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [575/575 01:59, Epoch 5/5]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.567400</td>\n      <td>0.484929</td>\n      <td>0.780870</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.364900</td>\n      <td>0.445038</td>\n      <td>0.826667</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.149100</td>\n      <td>0.551907</td>\n      <td>0.830725</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.057000</td>\n      <td>0.727788</td>\n      <td>0.838261</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.024200</td>\n      <td>0.696037</td>\n      <td>0.845797</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='575' max='575' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [575/575 01:59, Epoch 5/5]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.567400</td>\n      <td>0.484929</td>\n      <td>0.780870</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.364900</td>\n      <td>0.445038</td>\n      <td>0.826667</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.149100</td>\n      <td>0.551907</td>\n      <td>0.830725</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.057000</td>\n      <td>0.727788</td>\n      <td>0.838261</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.024200</td>\n      <td>0.696037</td>\n      <td>0.845797</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"No files have been modified since last commit. Skipping to prevent empty commit.\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import AutoTokenizer, BitsAndBytesConfig, AutoModelForCausalLM, AutoModelForSequenceClassification, AutoTokenizer, BertForSequenceClassification\nfrom datasets import load_dataset\ncheckpoint = \"bert-base-uncased\"\ntokenizer = AutoTokenizer.from_pretrained(checkpoint, padding_side=\"right\")\n\ndef encode(examples):\n    output = tokenizer(examples['sentence1'], \n                       examples['sentence2'], \n                       truncation=True, \n                       padding='max_length', \n                       max_length=128,\n                      return_tensors=\"pt\")\n    output['labels'] = examples['label']\n    return output\n\ndataset = load_dataset(\"glue\", \"mrpc\")\nval_dataset = dataset['test']\nval_dataset = val_dataset.map(encode, batched=True) \nval_dataset = val_dataset.remove_columns(['sentence1', 'sentence2', 'label', 'idx'])\nval_dataset.set_format(type='pt', columns=['input_ids', 'attention_mask', \"token_type_ids\",'labels',], output_all_columns=True)\nval_dataset","metadata":{"execution":{"iopub.status.busy":"2024-09-02T16:56:00.283823Z","iopub.execute_input":"2024-09-02T16:56:00.284600Z","iopub.status.idle":"2024-09-02T16:56:02.487037Z","shell.execute_reply.started":"2024-09-02T16:56:00.284530Z","shell.execute_reply":"2024-09-02T16:56:02.486116Z"},"trusted":true},"execution_count":11,"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1725 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1fae83e4b7834af4ab5e29c54cc9c2a4"}},"metadata":{}},{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n    num_rows: 1725\n})"},"metadata":{}}]},{"cell_type":"code","source":"# validation_dataset = val_dataset\ntest_dataset = val_dataset","metadata":{"execution":{"iopub.status.busy":"2024-09-02T16:56:03.445863Z","iopub.execute_input":"2024-09-02T16:56:03.446774Z","iopub.status.idle":"2024-09-02T16:56:03.450627Z","shell.execute_reply.started":"2024-09-02T16:56:03.446732Z","shell.execute_reply":"2024-09-02T16:56:03.449745Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"from transformers import set_seed\nfrom peft import PeftConfig, PeftModelForSequenceClassification\n\n# set_seed(42)\n\nbase_model_id = \"bert-base-uncased\"\nfine_tuned_model_id = \"/kaggle/working/test-model-5/\"\nbase_model = AutoModelForSequenceClassification.from_pretrained(base_model_id).to(\"cuda\")\nfine_tuned_model = AutoModelForSequenceClassification.from_pretrained(fine_tuned_model_id).to(\"cuda\")\n","metadata":{"execution":{"iopub.status.busy":"2024-09-02T16:56:07.459047Z","iopub.execute_input":"2024-09-02T16:56:07.459779Z","iopub.status.idle":"2024-09-02T16:56:08.430875Z","shell.execute_reply.started":"2024-09-02T16:56:07.459739Z","shell.execute_reply":"2024-09-02T16:56:08.429856Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"code","source":"from evaluate.visualization import radar_plot\n","metadata":{"execution":{"iopub.status.busy":"2024-09-02T16:06:56.414922Z","iopub.execute_input":"2024-09-02T16:06:56.415512Z","iopub.status.idle":"2024-09-02T16:07:13.637084Z","shell.execute_reply.started":"2024-09-02T16:06:56.415467Z","shell.execute_reply":"2024-09-02T16:07:13.636321Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"class Evaluator:\n    def __init__(self):\n        self.dataset = None\n        self.models_dict = None\n        # self.base_model = None\n        # self.fine_tuned_model = None\n        self.device = None\n        self.metric = None\n        self.current_model = None\n\n\n    # def load_base_model(self, model):\n    #     self.base_model = model\n    #     return self.base_model\n    \n    # def load_fine_tuned_model(self, model):\n    #     self.fine_tuned_model = model\n    #     return self.fine_tuned_model\n\n    def load_models_dict(self, models_dict):\n        self.models_dict = models_dict\n        return self.models_dict\n           \n    def set_device(self, device):\n        self.device = device\n        return self.device\n\n    def load_dataset_to_evaluate(self, dataset):\n        self.dataset = dataset\n        return self.dataset\n    \n    def load_metric(self, metric):\n        self.metric = metric\n        return self.metric\n    \n    def compute_metrics(self):\n        pass\n    \n    def evaluate_qbq(self):\n        self.current_model.eval()\n        for index in tqdm(range(len(self.dataset)), desc=\"Evaluating\"):\n            input_ids = self.dataset['input_ids'][index].unsqueeze(0).to(\"cuda\")\n            attention_mask = self.dataset['attention_mask'][index].unsqueeze(0).to(\"cuda\")    \n            with torch.no_grad():\n                outputs = self.model(input_ids = input_ids, \n                                           attention_mask = attention_mask\n                                          )\n            logits = outputs.logits\n            predictions = torch.argmax(logits, dim=-1)\n            self.metric.add(predictions=predictions, references=val_dataset['labels'][index])\n            \n        results = self.metric.compute()\n        return results\n    \n    def evaluate_batch(self):\n        self.current_model.eval()\n        for batch in tqdm(self.dataset, desc=\"Evaluating\"):\n            inputs = {k:v.to(device) for k,v in batch.items()}\n            with torch.no_grad():\n                outputs = model(**inputs)\n            logits = outputs.logits\n            predictions = torch.argmax(logits, dim=-1)\n            self.metric.add(predictions = predictions, references = batch['labels'])\n        results = self.metric.compute()\n        return results\n\n\n    def evaluate_models(self):\n        evaluation_results = {}\n        for model_name, model in self.models_dict.items():\n            self.current_model = model\n            result = self.evaluate_batch()\n            evaluation_results.update({model_name: result})\n        return evaluation_results\n    \n    \n\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tqdm import tqdm \nclass Evaluator:\n    def __init__(self):\n        self.models_dict = None\n        self.metrics_dict = None\n        self.datasets_dict = None\n\n        self.device = None\n        self.current_model = None\n        self.current_model_name = None\n        self.current_dataset = None\n        self.current_dataset_name = None\n        \n\n        self.labels = None\n        self.all_logits = None\n\n# LOADING FUNCTIONS\n    def load_models_dict(self, models_dict):\n        self.models_dict = models_dict\n        return self.models_dict\n           \n    def set_device(self, device):\n        self.device = device\n        return self.device\n\n    def load_datasets_dict(self, datasets_dict):\n        self.datasets_dict = datasets_dict\n        return self.datasets_dict\n    \n    def load_metrics_dict(self, metrics_dict):\n        self.metrics_dict = metrics_dict\n        return self.metrics_dict\n    \n    \n# METRICS COMPUTATION\n    def compute_metrics(self):\n        computed_metrics = {}\n        for _, metric in self.metrics_dict.items():\n            result = metric.compute(predictions = self.all_logits, references = self.labels)\n            computed_metrics.update(result)\n        return computed_metrics\n\n\n    def evaluate_qbq(self):\n        self.current_model.eval()\n        all_logits = []\n        labels = []\n        for index in tqdm(range(len(self.current_dataset)), desc=f\"Evaluating {self.current_model_name} on {self.current_dataset_name}\"):\n            input_ids = self.current_dataset['input_ids'][index].unsqueeze(0).to(self.device)\n            attention_mask = self.current_dataset['attention_mask'][index].unsqueeze(0).to(self.device)  \n            token_type_ids = self.current_dataset['token_type_ids'][index].unsqueeze(0).to(self.device)\n            with torch.no_grad():\n                outputs = self.current_model(input_ids = input_ids,\n                                             attention_mask = attention_mask,\n                                             token_type_ids = token_type_ids\n                                            )\n            logits = outputs.logits\n            predictions = torch.argmax(logits, dim=-1)[0]\n            label = self.current_dataset['labels'][index].to(self.device)\n            \n            all_logits.append(predictions)\n            labels.append(label)\n        self.all_logits = all_logits\n        self.labels = labels\n        \n        evaluated_metrics = self.compute_metrics()\n        return evaluated_metrics\n\n\n    def evaluate_batch(self):\n        self.current_model.eval()\n        all_logits = []\n        for batch in tqdm(self.current_dataset, desc=\"Evaluating\"):\n            inputs = {k:v.to(self.device) for k,v in batch.items()}\n            with torch.no_grad():\n                outputs = self.current_model(**inputs)\n            logits = outputs.logits\n            all_logits.append(logits)\n        self.all_logits = all_logits        \n        evaluated_metrics = self.compute_metrics()\n        return evaluated_metrics\n\n\n    def evaluate_models(self):\n        evaluation_results = {}\n        for model_name, model in self.models_dict.items():\n            self.current_model = model\n            self.current_model_name = model_name\n            result = self.evaluate_qbq()\n            evaluation_results.update({model_name: result})\n        return evaluation_results\n\n\n    def evaluate_datasets(self):\n        evaluation_results = {}\n        for dataset_name, dataset in self.datasets_dict.items():\n            self.current_dataset = dataset\n            self.current_dataset_name = dataset_name\n            result = self.evaluate_models()\n            evaluation_results.update({dataset_name: result})\n        return evaluation_results\n    ","metadata":{"execution":{"iopub.status.busy":"2024-09-02T16:56:14.788757Z","iopub.execute_input":"2024-09-02T16:56:14.789190Z","iopub.status.idle":"2024-09-02T16:56:14.813523Z","shell.execute_reply.started":"2024-09-02T16:56:14.789110Z","shell.execute_reply":"2024-09-02T16:56:14.812587Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"## DataLoader","metadata":{}},{"cell_type":"code","source":"from tqdm import tqdm \nfrom evaluate import load\nfrom torch.utils.data import DataLoader\n\n# test_dataset = DataHelper(dataset)\ntest_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=True)\n\naccuracy = load(\"accuracy\")\n\nlabels, all_logits = [], []\nfor batch in tqdm(test_dataloader, desc=\"Evaluating\"):\n    inputs = {k:v.to(\"cuda\") for k,v in batch.items()}\n    with torch.no_grad():\n        outputs = fine_tuned_model(**inputs)\n    logits = outputs.logits\n    predictions = torch.argmax(logits, dim=-1)\n    all_logits.append(predictions)\n    labels.append(inputs['labels'])\n\nlabels = torch.cat(labels, dim=0)\nall_logits = torch.cat(all_logits, dim=0)\n\naccuracy.compute(predictions = all_logits, references = labels)","metadata":{"execution":{"iopub.status.busy":"2024-09-02T18:00:23.795054Z","iopub.execute_input":"2024-09-02T18:00:23.795436Z","iopub.status.idle":"2024-09-02T18:00:37.542218Z","shell.execute_reply.started":"2024-09-02T18:00:23.795392Z","shell.execute_reply":"2024-09-02T18:00:37.541304Z"},"trusted":true},"execution_count":62,"outputs":[{"name":"stderr","text":"Evaluating: 100%|██████████| 54/54 [00:12<00:00,  4.34it/s]\n","output_type":"stream"},{"execution_count":62,"output_type":"execute_result","data":{"text/plain":"{'accuracy': 0.8457971014492753}"},"metadata":{}}]},{"cell_type":"code","source":"import torch\n\n# Create two one-dimensional tensors\ntensor1 = torch.tensor([1, 2, 3])\ntensor2 = torch.tensor([4, 5, 6])\n\n# Concatenate the tensors\nresult_tensor = torch.cat([tensor1, tensor2], dim=0)\nresult_tensor","metadata":{"execution":{"iopub.status.busy":"2024-09-02T17:52:57.032457Z","iopub.execute_input":"2024-09-02T17:52:57.033263Z","iopub.status.idle":"2024-09-02T17:52:57.041310Z","shell.execute_reply.started":"2024-09-02T17:52:57.033224Z","shell.execute_reply":"2024-09-02T17:52:57.040522Z"},"trusted":true},"execution_count":47,"outputs":[{"execution_count":47,"output_type":"execute_result","data":{"text/plain":"tensor([1, 2, 3, 4, 5, 6])"},"metadata":{}}]},{"cell_type":"code","source":"# LOADING MODELS\nfrom transformers import set_seed\nfrom peft import PeftConfig, PeftModelForSequenceClassification\n\nbase_model_id = \"bert-base-uncased\"\nfine_tuned_model_id = \"/kaggle/working/test-model-5/\"\nbase_model = AutoModelForSequenceClassification.from_pretrained(base_model_id).to(\"cuda\")\nfine_tuned_model = AutoModelForSequenceClassification.from_pretrained(fine_tuned_model_id).to(\"cuda\")\n\n# LOADING METRICS\nfrom evaluate import load\naccuracy = load(\"accuracy\")\nf1 = load(\"f1\")\nrecall = load(\"recall\")\n","metadata":{"execution":{"iopub.status.busy":"2024-09-02T16:56:19.244219Z","iopub.execute_input":"2024-09-02T16:56:19.245052Z","iopub.status.idle":"2024-09-02T16:56:21.038618Z","shell.execute_reply.started":"2024-09-02T16:56:19.245010Z","shell.execute_reply":"2024-09-02T16:56:21.037709Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/6.77k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b73a3d8e55f243cbbdfbfa251dad2f16"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/7.36k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"27716c3688284d97a380b096a5b64ddd"}},"metadata":{}}]},{"cell_type":"code","source":"\nmodels_dict = {\n    \"fine_tuned_model\": fine_tuned_model,\n    \"base_model\": base_model,\n}\n\nmetrics_dict = {\n    \"accuracy\": accuracy,\n    \"f1\": f1,\n    \"recall\": recall,   \n}\n\ndatasets_dict = {\n#     \"train_dataset\": train_dataset,\n    \"test_dataset\": test_dataset,\n    \"validation_dataset\": validation_dataset,\n}\n\n\nevaluator = Evaluator()\nevaluator.set_device(\"cuda\")\nevaluator.load_models_dict(models_dict)\nevaluator.load_metrics_dict(metrics_dict)\nevaluator.load_datasets_dict(datasets_dict)\n\nresult = evaluator.evaluate_datasets()\nresult","metadata":{"execution":{"iopub.status.busy":"2024-09-02T17:05:19.861788Z","iopub.execute_input":"2024-09-02T17:05:19.862665Z","iopub.status.idle":"2024-09-02T17:06:44.470378Z","shell.execute_reply.started":"2024-09-02T17:05:19.862615Z","shell.execute_reply":"2024-09-02T17:06:44.469274Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stderr","text":"Evaluating fine_tuned_model on test_dataset: 100%|██████████| 1725/1725 [00:36<00:00, 47.75it/s]\nEvaluating base_model on test_dataset: 100%|██████████| 1725/1725 [00:37<00:00, 45.86it/s]\nEvaluating fine_tuned_model on validation_dataset: 100%|██████████| 408/408 [00:05<00:00, 81.19it/s]\nEvaluating base_model on validation_dataset: 100%|██████████| 408/408 [00:04<00:00, 83.06it/s]\n","output_type":"stream"},{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"{'test_dataset': {'fine_tuned_model': {'accuracy': 0.8457971014492753,\n   'f1': 0.888235294117647,\n   'recall': 0.9215344376634699},\n  'base_model': {'accuracy': 0.664927536231884,\n   'f1': 0.7987465181058496,\n   'recall': 1.0}},\n 'validation_dataset': {'fine_tuned_model': {'accuracy': 0.8529411764705882,\n   'f1': 0.894736842105263,\n   'recall': 0.9139784946236559},\n  'base_model': {'accuracy': 0.6838235294117647,\n   'f1': 0.8122270742358079,\n   'recall': 1.0}}}"},"metadata":{}}]},{"cell_type":"code","source":"from evaluate import load\n\naccuracy = load(\"accuracy\")\n\nlogits = []\nfor logit in evaluator.all_logits:\n    prediction = torch.argmax(logit, dim=-1)[0]\n    logits.append(prediction)\n# # logits\n# evaluator.labels\n\naccuracy.compute(predictions=logits, references=evaluator.labels)","metadata":{"execution":{"iopub.status.busy":"2024-09-02T08:33:27.853758Z","iopub.execute_input":"2024-09-02T08:33:27.854745Z","iopub.status.idle":"2024-09-02T08:33:29.420288Z","shell.execute_reply.started":"2024-09-02T08:33:27.854690Z","shell.execute_reply":"2024-09-02T08:33:29.419264Z"},"trusted":true},"execution_count":57,"outputs":[{"execution_count":57,"output_type":"execute_result","data":{"text/plain":"{'accuracy': 0.8529411764705882}"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# test_dict = {\n#     \"param-1\":1,\n#     \"param-2\":2,\n#     \"param-3\":3,\n#     \"param-4\":4,\n#     }\n# for k, v in test_dict.items():\n#     print(k,v)\n\ndef evaluate(num):\n    return num + 1\n\nmodels = {\"model-1\":1, \n          \"model-2\":2, \n          \"model-3\":3,\n          }\nevaluation_results={}\n    \nfor models_name, model in models.items():\n    result = evaluate(model)  \n    evaluation_results.update({models_name:result})\nevaluation_results","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"metrics_dict = {\n    \"accuracy\":1,\n    \"f-1 score\":2,\n    \"recall\":3,\n}\n\ndef evaluate(num):\n    return num + 2\n\ncomputed_metrics={}\nfor metric_name, metric in metrics_dict.items():\n    result = evaluate(metric)\n    computed_metrics.update({metric_name: result})\ncomputed_metrics","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tqdm import tqdm\nimport evaluate\nimport torch\nmetric = evaluate.load(\"accuracy\")\nmodel = fine_tuned_model\n\nmodel.eval()\nfor index in tqdm(range(len(val_dataset)), desc=\"Evaluating\"):\n    input_ids = val_dataset['input_ids'][index].unsqueeze(0).to(\"cuda\")\n    attention_mask = val_dataset['attention_mask'][index].unsqueeze(0).to(\"cuda\")\n    token_type_ids = val_dataset['token_type_ids'][index].unsqueeze(0).to(\"cuda\")\n    with torch.no_grad():\n        outputs = model(input_ids = input_ids, \n                        attention_mask = attention_mask, \n                        token_type_ids = token_type_ids\n                       )\n        \n    logits = outputs.logits\n    predictions = torch.argmax(logits, dim=-1)\n    metric.add(predictions=predictions, references=val_dataset['labels'][index])\n\nmetric.compute()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Evaluate Code","metadata":{}},{"cell_type":"code","source":"from tqdm import tqdm\nimport torch\nimport logging\n\nclass Evaluate:\n    def __init__(self, tokenizer, model):\n        \n        logging.getLogger(\"transformers\").setLevel(logging.ERROR)\n\n        self.tokenizer = tokenizer\n        self.model = model\n        self.streamer = None\n        self.model.generation_config.pad_token_id = self.tokenizer.pad_token_id\n        self.model.generation_config.eos_token_id = self.tokenizer.eos_token_id\n        \n    def single_question_evaluate(self, question, return_prompt=False, stream_response=False):\n        self.model.eval()\n        with torch.no_grad():\n            inputs = self.tokenizer(question, return_tensors=\"pt\").to(\"cuda\")\n            output = self.model.generate(\n                **inputs,\n                max_new_tokens=1000,\n                return_dict_in_generate=True,\n                temperature=0.5,\n                do_sample=True,\n                top_k=50, \n                num_beams=1,\n                early_stopping=False,\n                eos_token_id=self.tokenizer.eos_token_id,\n                pad_token_id=self.tokenizer.pad_token_id,\n                streamer=(self.streamer if stream_response else None),\n            )\n            \n            if return_prompt:\n                response = self.tokenizer.decode(output.sequences[0], skip_special_tokens=True)\n            else:\n                response = self.tokenizer.decode(output.sequences[:, inputs.input_ids.shape[1]:][0], skip_special_tokens=True)  \n                \n            return response\n                \n    def load_streamer(self, streamer):\n        self.streamer = streamer\n        return self.streamer\n    \n    def batch_evaluate(self, dataset, batch_size, return_prompt=False):\n        pass\n    \n    def qbq_evaluate(self, dataset, return_prompt=False):\n        model_responses = []\n        self.model.eval()\n        with tqdm(total=len(dataset), desc=\"Generating responses\", unit=\"question\") as pbar:\n            for index in range(len(dataset)):\n                with torch.no_grad():\n                    question = dataset['training_input'][index]\n                    inputs = self.tokenizer(question, return_tensors=\"pt\").to(\"cuda\")\n                    output = self.model.generate(\n                        **inputs,\n                        max_new_tokens=1000,\n                        return_dict_in_generate=True,\n                        temperature=0.5,\n                        do_sample=True,\n                        top_k=50, \n                        num_beams=1,\n                        early_stopping=False,\n                    )\n                    if return_prompt:\n                        response = self.tokenizer.decode(output.sequences[0], skip_special_tokens=True)\n                    else:\n                        response = self.tokenizer.decode(output.sequences[:, inputs.input_ids.shape[1]:][0], skip_special_tokens=True)\n                        \n                    model_responses.append(response)\n                    pbar.update(1)\n                    \n        return model_responses\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_config_dict = {\n    \"\"\n\n}","metadata":{},"execution_count":null,"outputs":[]}]}