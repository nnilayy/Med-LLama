{"metadata":{"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30761,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## LIBRARY INSTALLATION","metadata":{}},{"cell_type":"code","source":"!pip install evaluate peft transformers==4.42.0","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## API KEYS","metadata":{}},{"cell_type":"code","source":"import os\nimport wandb\nfrom huggingface_hub import HfApi\nfrom kaggle_secrets import UserSecretsClient\n\n# LOADING API-KEYS\nuser_secrets = UserSecretsClient()\n\n# HUGGINGFACE LOGIN\nhugging_face_token = user_secrets.get_secret(\"HUGGING_FACE_API_KEY\")\napi = HfApi(token=hugging_face_token)\n\n# WANDB LOGIN\nwandb_api_token = user_secrets.get_secret(\"WANDB_API_KEY\")\nwandb.login(key = wandb_api_token)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## LOADING MODEL AND TOKENIZERS","metadata":{}},{"cell_type":"code","source":"import torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n# tokenizer.pad_token = tokenizer.eos_token\ncheckpoint = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\ntokenizer = AutoTokenizer.from_pretrained(checkpoint, token=hugging_face_token)\nmodel = AutoModelForCausalLM.from_pretrained(checkpoint, \n                                             device_map=\"auto\",\n                                             torch_dtype=torch.float16,\n                                             token=hugging_face_token,\n                                            )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import TextStreamer\nstreamer = TextStreamer(\n    tokenizer,\n    skip_prompt=True,\n    skip_special_tokens=True,\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from datasets import load_dataset\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\n\n# Load dataset\ndataset = load_dataset(\"pubmed_qa\", \"pqa_labeled\")\ntest_set = dataset[\"train\"]\n\n# Load model and tokenizer\ncheckpoint = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\ntokenizer = AutoTokenizer.from_pretrained(checkpoint, token=hugging_face_token)\nmodel = AutoModelForCausalLM.from_pretrained(checkpoint, \n                                             device_map=\"auto\",\n                                             torch_dtype=torch.float16,\n                                             token=hugging_face_token)\ntokenizer.pad_token = tokenizer.eos_token\ndevice = next(model.parameters()).device\n\ndef collate_fn(batch):\n    input_texts = [f\"Question: {item['question']}\\nAnswer:\" for item in batch]\n    inputs = tokenizer(input_texts, return_tensors=\"pt\", padding=True, truncation=True)\n    return {\n        'inputs': inputs,\n        'true_answers': [item['final_decision'] for item in batch]\n    }\n\nbatch_size = 8  # Adjust based on your GPU memory\ndataloader = DataLoader(test_set, batch_size=batch_size, collate_fn=collate_fn)\n\n# Evaluation loop\nresults = []\ntotal_batches = len(dataloader)\n\nmodel.eval()\nfor batch in tqdm(dataloader, total=total_batches, desc=\"Evaluating\"):\n    inputs = batch['inputs']\n    inputs = {k: v.to(device) for k, v in inputs.items()}\n    \n    with torch.no_grad():\n        outputs = model.generate(**inputs, max_length=1024)\n    \n    generated_answers = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n    \n    for true_answer, generated_answer in zip(batch['true_answers'], generated_answers):\n        results.append({\n            \"true_answer\": true_answer,\n            \"generated_answer\": generated_answer\n        })\n\n# Calculate metrics\ncorrect = sum(1 for r in results if r[\"true_answer\"] in r[\"generated_answer\"])\naccuracy = correct / len(results)\nprint(f\"Accuracy: {accuracy:.2f}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## DATAHELPER CLASS","metadata":{}},{"cell_type":"code","source":"from tqdm import tqdm \nfrom evaluate import load\nfrom torch.utils.data import DataLoader\n\n\nclass DataHelper:\n    def __init__(self):\n        self.datasets_dict = None        \n        self.current_datasets_dict = {}\n        self.formatted_datasets_dict = {}\n        self.tokenized_datasets_dict = {}\n\n        self.dataloaders_dict = {}\n\n        self.tokenizer = None\n#         self.tokenizer.pad_token = None\n\n        self.system_instruction = \"You are a Helpful AI Assistant.\"\n        self.user_instruction = \"Please answer the following Question: \"\n        self.user_query = None\n        \n        #datasets configurations\n        self.batch_size = None\n        self.shuffle = None\n        self.max_length = None\n        self.return_tensors = None\n        self.padding = None\n        self.truncation = None\n        \n        # Config Columns\n        self.user_query_column = None        \n        self.columns_to_tokenize = None  \n        \n        #Number\n        self.number_one = 1\n        self.number_two = 3\n        \n\n# DATASETS CLASS\n    def __len__(self):\n        return len(self.dataset)\n\n    def __getitem__(self, idx):\n        return {\n            'input_ids': self.dataset['input_ids'][idx].unsqueeze(0),\n            'attention_mask': self.dataset['attention_mask'][idx].unsqueeze(0),\n            'token_type_ids': self.dataset.get('token_type_ids', torch.tensor([]))[idx].unsqueeze(0) if 'token_type_ids' in self.dataset else None\n            }\n\n# LOADING DATASETS DICT\n    def load_datasets_dict(self, datasets_dict):\n        self.datasets_dict = datasets_dict\n        return self.datasets_dict\n\n# LOADING DATASET CONFIGURATION\n    def set_dataset_config(self, dataset_configuration):\n        self.batch_size = dataset_configuration['batch_size']\n        self.shuffle = dataset_configuration['shuffle']\n        self.max_length = dataset_configuration['max_length']\n        self.return_tensors = dataset_configuration['return_tensors']\n        self.padding = dataset_configuration['padding']\n        self.truncation = dataset_configuration['truncation']\n\n# LOADING IMPORTANT COLUMNS\n    def load_config_columns(self, columns_configuration):\n        self.user_query_column = columns_configuration[\"user_query_column\"]\n        self.columns_to_tokenize = columns_configuration[\"columns_to_tokenize\"]\n\n# LOADING TOKENIZER\n    def load_tokenizer(self, tokenizer):\n        self.tokenizer = tokenizer\n        return self.tokenizer        \n\n# SYSTEM & USER PROMPT\n    def set_system_instruction(self, system_instruction):\n        self.system_instruction = system_instruction\n        return self.system_instruction\n    \n    def set_user_instruction(self, user_instruction):\n        self.user_instruction = user_instruction\n        return self.user_instruction\n    \n    def set_user_query(self, user_query):\n        self.user_query = user_query\n        return self.user_query\n\n# HANDLING INPUT COLUMN\n    def handle_dataset(self):\n        pass\n\n# CONVERTING DATASETS TO DATALOADER\n    def datasets_to_dataloader(self):\n        if self.tokenized_datasets_dict:\n            self.current_datasets_dict = self.tokenized_datasets_dict\n        elif self.formatted_datasets_dict:\n            self.current_datasets_dict = self.formatted_datasets_dict\n        else:\n            self.current_datasets_dict = self.datasets_dict\n\n        for dataset_name, dataset in self.current_datasets_dict.items():\n            dataloader = DataLoader(dataset, batch_size=self.batch_size, shuffle=self.shuffle)\n            self.dataloaders_dict.update({dataset_name+\"_dataloader\":dataloader})\n        return self.dataloaders_dict\n    \n# FORMATTING DATASET CODE\n    def convert_input_to_chat_template(self):\n        message = [\n            {\"role\": \"system\", \"content\": self.system_instruction},\n            {\"role\": \"user\", \"content\": self.user_instruction + self.user_query}\n        ]\n        formatted_input = self.tokenizer.apply_chat_template(message,\n                                                                tokenize=False,\n                                                                add_generation_prompt=True,\n                                                                return_tensors = self.return_tensors\n                                                            )\n        return formatted_input\n\n    def create_chat_template_dataset(self, example):\n        self.user_query = example[self.user_query_column]\n        example['training_input'] = self.convert_input_to_chat_template()\n        return example\n\n    def format_dataset(self, dataset):\n        self.current_datasets_dict = self.datasets_dict\n        \n        for dataset_name, dataset in self.current_datasets_dict.items():\n            formatted_dataset = dataset.map(self.create_chat_template_dataset)\n            self.formatted_dataset_dict.update({dataset_name+\"_formatted\":formatted_dataset})\n        return self.formatted_dataset_dict\n    \n# TOKENIZATION CODE    \n    def tokenization_function(self, example):\n        return self.tokenizer(example[self.columns_to_tokenize],\n                                  padding=True,\n                                  truncation=True,\n                                  max_length=1024,\n                                  return_tensors = self.return_tensors\n                                 )\n    \n    def tokenize_datasets(self):\n        if self.formatted_datasets_dict:\n            self.current_datasets_dict = self.formatted_datasets_dict\n        else:\n            self.current_datasets_dict = self.datasets_dict\n            \n        for dataset_name, dataset in self.current_datasets_dict.items():\n            tokenized_dataset = dataset.map(self.tokenization_function, batched=True, batch_size=128, num_proc=8)\n            self.tokenized_datasets_dict.update({dataset_name+\"_tokenized\":dataset}) \n        return self.tokenized_datasets_dict    \n    \n    def remove_columns(self):\n        dataset_name = list(self.datasets_dict.keys())[0]\n        base_dataset_columns = list(self.datasets_dict[dataset_name].features.keys())\n        tokenized_dataset_columns = list(self.current_datasets_dict[dataset_name].features.keys())\n        final_columns = list(set(tokenized_dataset_columns) - set(base_dataset_columns))\n        \n#         final_dataset = tokenized_dataset.remove_columns(base_dataset_columns)\n#         final_dataset.set_format(type = self.return_tensors, columns=final_columns, output_all_columns=True)\n#         return final_dataset    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# LOADING DATASET\nfrom datasets import load_dataset\nfrom transformers import AutoTokenizer, BitsAndBytesConfig, AutoModelForCausalLM, AutoModelForSequenceClassification, AutoTokenizer, BertForSequenceClassification\ncheckpoint = \"bert-base-uncased\"\nmodel = AutoModelForSequenceClassification.from_pretrained(checkpoint)\ntokenizer = AutoTokenizer.from_pretrained(checkpoint, padding_side=\"right\")\n\ndataset = load_dataset(\"glue\", \"mrpc\")\ntrain_dataset = dataset['train']\ntest_dataset = dataset['test']\nvalidation_dataset = dataset['validation']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CustomDataHelper(DataHelper):\n    def handle_dataset(self):\n        datasets_dict = self.datasets_dict\n        for dataset_name, dataset in datasets_dict.items:\n            sentence1 = dataset['sentence1']\n            sentence2 = dataset['sentence2']\n            dataset['input'] = dataset['sentence1'] + dataset['sentence2']\n            self.handled_datasets_dict.update({dataset_name+\"_handled\":dataset})\n        return self.handled_datasets_dict            \n\ndatasets_dict = {\n    \"train_dataset\": train_dataset,\n    \"test_dataset\": test_dataset,\n    \"validation_dataset\": validation_dataset\n}\n\ndataset_configuration = {\n    \"batch_size\": 32,\n    \"shuffle\": True,\n    \"return_tensors\": \"pt\",\n    \"max_length\":128,\n    \"padding\": True,\n    \"truncation\": True\n}\n\ncolumn_configuration = {\n    \"user_query_column\": \"input\",\n    \"columns_to_tokenize\":\"sentence1\"\n}\n\n# data_helper = CustomDataHelper()\n# data_helper.handle_dataset()\ndata_helper = DataHelper()\ndata_helper.load_datasets_dict(datasets_dict)\ndata_helper.load_config_columns(column_configuration)\ndata_helper.set_dataset_config(dataset_configuration)\n\n# data_helper.load_tokenizer(tokenizer)\n# tokenized_dataset = data_helper.tokenize_datasets()\ndataloader = data_helper.datasets_to_dataloader()\ndataloader","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Extracting Datasets\ntrain_dataset = dataset['train']\ntest_dataset = dataset['test']\nvalidation_dataset = dataset['validation']\ndatasets = [train_dataset, test_dataset, validation_dataset]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"datahelper = DataHelper(tokenizer=tokenizer, user_query_column = \"input\", columns_to_tokenize=\"training_input\")\n\nformatted_datasets = [datahelper.format_dataset(dataset) for dataset in datasets]\ntokenized_datasets = [datahelper.tokenize_dataset(dataset) for dataset in formatted_datasets]\n\ntrain_dataset, test_dataset, validation_dataset = tokenized_datasets","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# tokenized_datasets\n# datahelper.clean_up_dataset(base_dataset=dataset['train'], tokenized_dataset=tokenized_dataset)\n# train_dataset = dataset['train'].map(preprocessing, batched=True, batch_size=32)\n# train_dataset = train_dataset.remove_columns(['input', 'instruction', 'output', 'final_text'])\n# train_dataset.set_format(type='pt', columns=['input_ids', 'attention_mask'], output_all_columns=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Trainer Code","metadata":{}},{"cell_type":"code","source":"import torch\nfrom transformers import Trainer, TrainingArguments\n\ntraining_args = TrainingArguments(\n    output_dir='./results',\n    num_train_epochs=1,\n    per_device_train_batch_size=28,\n    save_total_limit=1,\n    eval_strategy=\"steps\",\n    save_strategy=\"epoch\",\n    save_steps = 100,\n    # label_names = ['not_equivalent', 'equivalent'],\n    fp16=torch.cuda.is_available()  # Use mixed precision if GPUs support it\n)\n\n# Initialize Trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=dataset['train'],\n    eval_dataset=dataset['validation'],\n    data_collator=data_collator,\n    compute_metrics=compute_metrics\n)\n\n# Train the model\ntrainer.train()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport torch\nimport warnings\nimport numpy as np\nfrom time import time\nfrom evaluate import load\nfrom datasets import load_dataset\nfrom transformers import DataCollatorWithPadding\nfrom peft.utils import get_peft_model_state_dict\nfrom transformers import TrainingArguments, Trainer\nfrom peft import get_peft_model, LoraConfig, TaskType, prepare_model_for_kbit_training\nfrom transformers import AutoTokenizer, BitsAndBytesConfig, AutoModelForCausalLM, AutoModelForSequenceClassification, AutoTokenizer, BertForSequenceClassification\n\n    \nwarnings.filterwarnings(\"ignore\", category=RuntimeWarning)\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n\nfrom transformers import set_seed\nset_seed(42)\n\ndef main():\n    def compute_metrics(eval_pred):\n        logits, labels = eval_pred\n        predictions = np.argmax(logits, axis=-1)\n        return accuracy.compute(predictions=predictions, references=labels)\n\n    def encode(examples):\n        output = tokenizer(examples['sentence1'], \n                           examples['sentence2'], \n                           truncation=True, \n                           padding='max_length', \n                           max_length=128,\n                          )\n        \n        output['labels'] = examples['label']\n        return output\n\n# MODEL\n    checkpoint = \"bert-base-uncased\"\n    model = AutoModelForSequenceClassification.from_pretrained(checkpoint,\n                                                  #torch_dtype=torch.float16,\n                                                 )\n\n#     peft_config = LoraConfig(inference_mode=False,\n#                              r=32,\n#                              lora_alpha = 512,\n#                              lora_dropout = 0.1,\n#                              bias=\"none\",\n#                              peft_type = TaskType.SEQ_CLS, #\" CAUSAL_LM\"\n#                              )\n\n#     model = prepare_model_for_kbit_training(model)\n#     model = get_peft_model(model, peft_config)\n#     model.print_trainable_parameters()\n\n# TYPICAL TRAINING CODE\n    accuracy = load(\"accuracy\")\n    tokenizer = AutoTokenizer.from_pretrained(checkpoint, padding_side=\"right\")\n    tokenizer.pad_token = tokenizer.eos_token\n    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n    \n#   DATASET PREP\n    dataset = load_dataset(\"glue\", \"mrpc\")\n    dataset = dataset.map(encode, batched=True)    \n    dataset = dataset.remove_columns(['sentence1', 'sentence2', 'label', 'idx'])\n    dataset.set_format(type='pt', columns=['input_ids', 'attention_mask', 'labels',], output_all_columns=True)\n    data_collator = DataCollatorWithPadding(tokenizer)\n    \n        \n# TRAINING ARGUMENTS\n    training_args = TrainingArguments(\n    # DIRECTORIES FOR SAVING AND LOGGING\n        output_dir=\"/kaggle/working/glue_model_checkpointing_test-8\",\n        logging_dir =  \"/kaggle/working/logs\", \n    \n    #  BASIC PARAMS\n        num_train_epochs=5,\n        fp16=True,\n        seed=42,\n        data_seed=42,\n        \n    # OPTIMIZER SETUP\n        optim=\"rmsprop\",\n        learning_rate=1e-4,\n        lr_scheduler_type=\"cosine\",\n        #lr_scheduler_kwargs={\"power\": 2.0},\n        warmup_ratio=0.2,\n        #warmup_steps=200,\n        \n    # DATA RELATED ARGUMENTS\n        per_device_train_batch_size=16,\n        per_device_eval_batch_size=16,\n        dataloader_num_workers=4, # Reduces Training time by a decent percentage\n        dataloader_pin_memory=True,\n        dataloader_persistent_workers=True, \n        ddp_find_unused_parameters=False,        \n        \n    # LOGGING\n        logging_strategy=\"epoch\", # Logs the Training Loss\n        label_names = ['labels'], # If Peft is off, keep this off doesnt do anything, if Peft is on, Logs the Validation Loss and Validation Accuracy\n        #report_to = tensorboard\n        \n    # EVALUATION\n        eval_strategy=\"epoch\", # Doesnt Evaluate the model per epoch, Reducing the training time\n        #eval_steps        \n        \n    # SAVING TO HUB\n        save_strategy=\"epoch\",\n        save_total_limit=1,\n        push_to_hub=True,\n        hub_token = hugging_face_token,\n        hub_strategy=\"every_save\",\n        hub_model_id=\"nnilayy/glue_model_checkpointing_test-8\",\n        \n#     SAVING VRAM\n#         torch_empty_cache_steps=40, #Clears vram cache during training after a few steps\n#         gradient_checkpointing=True,\n#         gradient_accumulation_steps=4,\n    )\n\n# TRAINER CONSTRUCTOR\n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        train_dataset=dataset[\"train\"],\n        eval_dataset=dataset[\"test\"],\n        tokenizer=tokenizer,\n        data_collator=data_collator,\n        compute_metrics=compute_metrics,\n    )\n\n# Pushing Tokenizer, Model Card, Label Mapping to Hub \n#     tokenizer.push_to_hub(\"nnilayy/glue_model_checkpointing_test-8\")\n#     model.config.label2id = {'equivalent': 0, 'not_equivalent': 1}\n#     model.config.id2label = {0: 'equivalent', 1: 'not_equivalent'}\n#     model.config.push_to_hub(\"nnilayy/glue_model_checkpointing_test-8\")\n\n    trainer.train()\n    model.save_pretrained(\"/kaggle/working/test-model-5\")\n\nif __name__ == \"__main__\":\n    from accelerate import notebook_launcher\n    notebook_launcher(main, num_processes=2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoTokenizer, BitsAndBytesConfig, AutoModelForCausalLM, AutoModelForSequenceClassification, AutoTokenizer, BertForSequenceClassification\nfrom datasets import load_dataset\ncheckpoint = \"bert-base-uncased\"\ntokenizer = AutoTokenizer.from_pretrained(checkpoint, padding_side=\"right\")\n\ndef encode(examples):\n    output = tokenizer(examples['sentence1'], \n                       examples['sentence2'], \n                       truncation=True, \n                       padding='max_length', \n                       max_length=128,\n                      return_tensors=\"pt\")\n    output['labels'] = examples['label']\n    return output\n\ndataset = load_dataset(\"glue\", \"mrpc\")\nval_dataset = dataset['validation']\nval_dataset = val_dataset.map(encode, batched=True) \nval_dataset = val_dataset.remove_columns(['sentence1', 'sentence2', 'label', 'idx'])\nval_dataset.set_format(type='pt', columns=['input_ids', 'attention_mask', \"token_type_ids\",'labels',], output_all_columns=True)\nval_dataset","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"validation_dataset = val_dataset\n# test_dataset = val_dataset","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import set_seed\nfrom peft import PeftConfig, PeftModelForSequenceClassification\n\n# set_seed(42)\n\nbase_model_id = \"bert-base-uncased\"\nfine_tuned_model_id = \"/kaggle/working/test-model-5/\"\nbase_model = AutoModelForSequenceClassification.from_pretrained(base_model_id).to(\"cuda\")\nfine_tuned_model = AutoModelForSequenceClassification.from_pretrained(fine_tuned_model_id).to(\"cuda\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## EVALUATOR CLASS","metadata":{}},{"cell_type":"code","source":"from tqdm import tqdm \nclass Evaluator:\n    def __init__(self):\n        self.models_dict = None\n        self.metrics_dict = None\n        self.datasets_dict = None\n\n        self.device = None\n        self.current_model = None\n        self.current_model_name = None\n        self.current_dataset = None\n        self.current_dataset_name = None\n        \n        self.labels = None\n        self.all_logits = None\n\n# LOADING FUNCTIONS\n    def load_models_dict(self, models_dict):\n        self.models_dict = models_dict\n        return self.models_dict\n           \n    def set_device(self, device):\n        self.device = device\n        return self.device\n\n    def load_datasets_dict(self, datasets_dict):\n        self.datasets_dict = datasets_dict\n        return self.datasets_dict\n    \n    def load_metrics_dict(self, metrics_dict):\n        self.metrics_dict = metrics_dict\n        return self.metrics_dict\n    \n    \n# METRICS COMPUTATION\n    def compute_metrics(self):\n        computed_metrics = {}\n        for _, metric in self.metrics_dict.items():\n            result = metric.compute(predictions = self.all_logits, references = self.labels)\n            computed_metrics.update(result)\n        return computed_metrics\n\n\n    def evaluate_qbq(self):\n        self.current_model.eval()\n        all_logits = []\n        labels = []\n        for index in tqdm(range(len(self.current_dataset)), desc=f\"Evaluating {self.current_model_name} on {self.current_dataset_name}\"):\n            input_ids = self.current_dataset['input_ids'][index].unsqueeze(0).to(self.device)\n            attention_mask = self.current_dataset['attention_mask'][index].unsqueeze(0).to(self.device)  \n            token_type_ids = self.current_dataset['token_type_ids'][index].unsqueeze(0).to(self.device)\n            with torch.no_grad():\n                outputs = self.current_model(input_ids = input_ids,\n                                             attention_mask = attention_mask,\n                                             token_type_ids = token_type_ids\n                                            )\n            logits = outputs.logits\n            predictions = torch.argmax(logits, dim=-1)[0]\n            label = self.current_dataset['labels'][index].to(self.device)\n            \n            all_logits.append(predictions)\n            labels.append(label)\n        self.all_logits = all_logits\n        self.labels = labels\n        \n        evaluated_metrics = self.compute_metrics()\n        return evaluated_metrics\n\n\n    def evaluate_batch(self):\n        self.current_model.eval()\n        all_logits = []\n        for batch in tqdm(self.current_dataset, desc=\"Evaluating\"):\n            inputs = {k:v.to(self.device) for k,v in batch.items()}\n            with torch.no_grad():\n                outputs = self.current_model(**inputs)\n            logits = outputs.logits\n            all_logits.append(logits)\n        self.all_logits = all_logits        \n        evaluated_metrics = self.compute_metrics()\n        return evaluated_metrics\n\n\n    def evaluate_models(self):\n        evaluation_results = {}\n        for model_name, model in self.models_dict.items():\n            self.current_model = model\n            self.current_model_name = model_name\n            result = self.evaluate_qbq()\n            evaluation_results.update({model_name: result})\n        return evaluation_results\n\n\n    def evaluate_datasets(self):\n        evaluation_results = {}\n        for dataset_name, dataset in self.datasets_dict.items():\n            self.current_dataset = dataset\n            self.current_dataset_name = dataset_name\n            result = self.evaluate_models()\n            evaluation_results.update({dataset_name: result})\n        return evaluation_results\n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tqdm import tqdm \nfrom evaluate import load\nfrom torch.utils.data import DataLoader\n\n# test_dataset = DataHelper(dataset)\ntest_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=True)\n\naccuracy = load(\"accuracy\")\n\nlabels, all_logits = [], []\nfor batch in tqdm(test_dataloader, desc=\"Evaluating\"):\n    inputs = {k:v.to(\"cuda\") for k,v in batch.items()}\n    with torch.no_grad():\n        outputs = fine_tuned_model(**inputs)\n    logits = outputs.logits\n    predictions = torch.argmax(logits, dim=-1)\n    all_logits.append(predictions)\n    labels.append(inputs['labels'])\n\nlabels = torch.cat(labels, dim=0)\nall_logits = torch.cat(all_logits, dim=0)\n\naccuracy.compute(predictions = all_logits, references = labels)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# LOADING MODELS\nfrom transformers import set_seed\nfrom peft import PeftConfig, PeftModelForSequenceClassification\n\nbase_model_id = \"bert-base-uncased\"\nfine_tuned_model_id = \"/kaggle/working/test-model-5/\"\nbase_model = AutoModelForSequenceClassification.from_pretrained(base_model_id).to(\"cuda\")\nfine_tuned_model = AutoModelForSequenceClassification.from_pretrained(fine_tuned_model_id).to(\"cuda\")\n\n# LOADING METRICS\nfrom evaluate import load\naccuracy = load(\"accuracy\")\nf1 = load(\"f1\")\nrecall = load(\"recall\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"models_dict = {\n    \"fine_tuned_model\": fine_tuned_model,\n    \"base_model\": base_model,\n}\n\nmetrics_dict = {\n    \"accuracy\": accuracy,\n    \"f1\": f1,\n    \"recall\": recall,   \n}\n\ndatasets_dict = {\n#     \"train_dataset\": train_dataset,\n    \"test_dataset\": test_dataset,\n    \"validation_dataset\": validation_dataset,\n}\n\n\nevaluator = Evaluator()\nevaluator.set_device(\"cuda\")\nevaluator.load_models_dict(models_dict)\nevaluator.load_metrics_dict(metrics_dict)\nevaluator.load_datasets_dict(datasets_dict)\n\nresult = evaluator.evaluate_datasets()\nresult","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Evaluate Code","metadata":{}},{"cell_type":"code","source":"from tqdm import tqdm\nimport torch\nimport logging\n\nclass Evaluate:\n    def __init__(self, tokenizer, model):\n        \n        logging.getLogger(\"transformers\").setLevel(logging.ERROR)\n\n        self.tokenizer = tokenizer\n        self.model = model\n        self.streamer = None\n        self.model.generation_config.pad_token_id = self.tokenizer.pad_token_id\n        self.model.generation_config.eos_token_id = self.tokenizer.eos_token_id\n        \n    def single_question_evaluate(self, question, return_prompt=False, stream_response=False):\n        self.model.eval()\n        with torch.no_grad():\n            inputs = self.tokenizer(question, return_tensors=\"pt\").to(\"cuda\")\n            output = self.model.generate(\n                **inputs,\n                max_new_tokens=1000,\n                return_dict_in_generate=True,\n                temperature=0.5,\n                do_sample=True,\n                top_k=50, \n                num_beams=1,\n                early_stopping=False,\n                eos_token_id=self.tokenizer.eos_token_id,\n                pad_token_id=self.tokenizer.pad_token_id,\n                streamer=(self.streamer if stream_response else None),\n            )\n            \n            if return_prompt:\n                response = self.tokenizer.decode(output.sequences[0], skip_special_tokens=True)\n            else:\n                response = self.tokenizer.decode(output.sequences[:, inputs.input_ids.shape[1]:][0], skip_special_tokens=True)  \n                \n            return response\n                \n    def load_streamer(self, streamer):\n        self.streamer = streamer\n        return self.streamer\n    \n    def batch_evaluate(self, dataset, batch_size, return_prompt=False):\n        pass\n    \n    def qbq_evaluate(self, dataset, return_prompt=False):\n        model_responses = []\n        self.model.eval()\n        with tqdm(total=len(dataset), desc=\"Generating responses\", unit=\"question\") as pbar:\n            for index in range(len(dataset)):\n                with torch.no_grad():\n                    question = dataset['training_input'][index]\n                    inputs = self.tokenizer(question, return_tensors=\"pt\").to(\"cuda\")\n                    output = self.model.generate(\n                        **inputs,\n                        max_new_tokens=1000,\n                        return_dict_in_generate=True,\n                        temperature=0.5,\n                        do_sample=True,\n                        top_k=50, \n                        num_beams=1,\n                        early_stopping=False,\n                    )\n                    if return_prompt:\n                        response = self.tokenizer.decode(output.sequences[0], skip_special_tokens=True)\n                    else:\n                        response = self.tokenizer.decode(output.sequences[:, inputs.input_ids.shape[1]:][0], skip_special_tokens=True)\n                        \n                    model_responses.append(response)\n                    pbar.update(1)\n                    \n        return model_responses\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}