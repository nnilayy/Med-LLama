{
  "metadata": {
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [],
      "dockerImageVersionId": 30761,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.14",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nnilayy/MedGPT/blob/main/llama_8_1b_instruct.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## API KEYS"
      ],
      "metadata": {
        "id": "47-nT_bkXwQP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import wandb\n",
        "from huggingface_hub import HfApi\n",
        "from kaggle_secrets import UserSecretsClient\n",
        "\n",
        "# LOADING API-KEYS\n",
        "user_secrets = UserSecretsClient()\n",
        "\n",
        "# HUGGINGFACE LOGIN\n",
        "hugging_face_token = user_secrets.get_secret(\"HUGGING_FACE_API_KEY\")\n",
        "api = HfApi(token=hugging_face_token)\n",
        "\n",
        "# WANDB LOGIN\n",
        "wandb_api_token = user_secrets.get_secret(\"WANDB_API_KEY\")\n",
        "wandb.login(key = wandb_api_token)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-08-29T14:33:41.858557Z",
          "iopub.execute_input": "2024-08-29T14:33:41.859348Z",
          "iopub.status.idle": "2024-08-29T14:33:44.846473Z",
          "shell.execute_reply.started": "2024-08-29T14:33:41.859308Z",
          "shell.execute_reply": "2024-08-29T14:33:44.845499Z"
        },
        "trusted": true,
        "id": "pJn0FqA2XwQR",
        "outputId": "c8b227f5-3b74-4416-c2df-0bf0cbb05831"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "text": "\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
          "output_type": "stream"
        },
        {
          "execution_count": 108,
          "output_type": "execute_result",
          "data": {
            "text/plain": "True"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LOADING MODEL AND TOKENIZERS"
      ],
      "metadata": {
        "id": "Von6GeyBXwQS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "# tokenizer.pad_token = tokenizer.eos_token\n",
        "checkpoint = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint, token=hugging_face_token)\n",
        "model = AutoModelForCausalLM.from_pretrained(checkpoint,\n",
        "                                             device_map=\"auto\",\n",
        "                                             torch_dtype=torch.float16,\n",
        "                                             token=hugging_face_token,\n",
        "                                            )"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-08-29T14:33:46.541207Z",
          "iopub.execute_input": "2024-08-29T14:33:46.542286Z",
          "iopub.status.idle": "2024-08-29T14:36:13.456414Z",
          "shell.execute_reply.started": "2024-08-29T14:33:46.542242Z",
          "shell.execute_reply": "2024-08-29T14:36:13.455442Z"
        },
        "trusted": true,
        "id": "6B5nyn6rXwQT",
        "outputId": "9f5dfa44-6e44-46c7-d6fc-45bb25260880",
        "colab": {
          "referenced_widgets": [
            "bb77f02b44374908af7f6e6e9ee60123",
            "5560fbfd939641058db77da5b49e638e",
            "a944d9bb1f2444afbb3afcad7e9b71e5",
            "4b278f39db7a49ad8c7d9020618ad913",
            "c269df06dcc445188843c62fd93a1f61",
            "7b0eb8258b66410c8817b7b22bcfc6d4",
            "bd6e67d90a644524b3c11f4df7973980",
            "417b363c87054c199014be85f24c7dbb",
            "90ac78a05e434dd3b17b6d4ed4b0aea5",
            "cd7b16a7a857470c9ad9420b0b8f1c29",
            "003e1f0e2c7f40458d8f09e9d1fb27e2",
            "fef6a3fe9822408fa3836c5eab46c53a"
          ]
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "tokenizer_config.json:   0%|          | 0.00/55.4k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "bb77f02b44374908af7f6e6e9ee60123"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5560fbfd939641058db77da5b49e638e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "special_tokens_map.json:   0%|          | 0.00/296 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a944d9bb1f2444afbb3afcad7e9b71e5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "config.json:   0%|          | 0.00/855 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4b278f39db7a49ad8c7d9020618ad913"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c269df06dcc445188843c62fd93a1f61"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7b0eb8258b66410c8817b7b22bcfc6d4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "model-00001-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "bd6e67d90a644524b3c11f4df7973980"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "model-00002-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "417b363c87054c199014be85f24c7dbb"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "model-00003-of-00004.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "90ac78a05e434dd3b17b6d4ed4b0aea5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "model-00004-of-00004.safetensors:   0%|          | 0.00/1.17G [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cd7b16a7a857470c9ad9420b0b8f1c29"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "003e1f0e2c7f40458d8f09e9d1fb27e2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "generation_config.json:   0%|          | 0.00/184 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fef6a3fe9822408fa3836c5eab46c53a"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TextStreamer\n",
        "streamer = TextStreamer(\n",
        "    tokenizer,\n",
        "    skip_prompt=True,\n",
        "    skip_special_tokens=True,\n",
        ")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-08-29T14:36:24.472825Z",
          "iopub.execute_input": "2024-08-29T14:36:24.474156Z",
          "iopub.status.idle": "2024-08-29T14:36:24.480303Z",
          "shell.execute_reply.started": "2024-08-29T14:36:24.474101Z",
          "shell.execute_reply": "2024-08-29T14:36:24.479488Z"
        },
        "trusted": true,
        "id": "tmBrT4VzXwQT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Load dataset\n",
        "dataset = load_dataset(\"pubmed_qa\", \"pqa_labeled\")\n",
        "test_set = dataset[\"train\"]\n",
        "\n",
        "# Load model and tokenizer\n",
        "checkpoint = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint, token=hugging_face_token)\n",
        "model = AutoModelForCausalLM.from_pretrained(checkpoint,\n",
        "                                             device_map=\"auto\",\n",
        "                                             torch_dtype=torch.float16,\n",
        "                                             token=hugging_face_token)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "device = next(model.parameters()).device\n",
        "\n",
        "def collate_fn(batch):\n",
        "    input_texts = [f\"Question: {item['question']}\\nAnswer:\" for item in batch]\n",
        "    inputs = tokenizer(input_texts, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "    return {\n",
        "        'inputs': inputs,\n",
        "        'true_answers': [item['final_decision'] for item in batch]\n",
        "    }\n",
        "\n",
        "batch_size = 8  # Adjust based on your GPU memory\n",
        "dataloader = DataLoader(test_set, batch_size=batch_size, collate_fn=collate_fn)\n",
        "\n",
        "# Evaluation loop\n",
        "results = []\n",
        "total_batches = len(dataloader)\n",
        "\n",
        "model.eval()\n",
        "for batch in tqdm(dataloader, total=total_batches, desc=\"Evaluating\"):\n",
        "    inputs = batch['inputs']\n",
        "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(**inputs, max_length=1024)\n",
        "\n",
        "    generated_answers = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
        "\n",
        "    for true_answer, generated_answer in zip(batch['true_answers'], generated_answers):\n",
        "        results.append({\n",
        "            \"true_answer\": true_answer,\n",
        "            \"generated_answer\": generated_answer\n",
        "        })\n",
        "\n",
        "# Calculate metrics\n",
        "correct = sum(1 for r in results if r[\"true_answer\"] in r[\"generated_answer\"])\n",
        "accuracy = correct / len(results)\n",
        "print(f\"Accuracy: {accuracy:.2f}\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "2jUIIdwVXwQT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DATASET HANDLING"
      ],
      "metadata": {
        "id": "Q88y0FJsXwQT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from datasets import load_dataset\n",
        "class DataHelper:\n",
        "    def __init__(self, tokenizer, user_query_column, columns_to_tokenize):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
        "\n",
        "        self.system_instruction = \"You are a Helpful AI Assistant.\"\n",
        "        self.user_instruction = \"Please answer the following Question: \"\n",
        "        self.user_query = None\n",
        "\n",
        "        self.null_values_list = None\n",
        "\n",
        "        self.user_query_column = user_query_column\n",
        "        self.columns_to_tokenize = columns_to_tokenize\n",
        "\n",
        "# SYSTEM & USER PROMPT\n",
        "    def set_system_instruction(self, system_instruction):\n",
        "        self.system_instruction = system_instruction\n",
        "        return self.system_instruction\n",
        "\n",
        "    def set_user_instruction(self, user_instruction):\n",
        "        self.user_instruction = user_instruction\n",
        "        return self.user_instruction\n",
        "\n",
        "    def set_user_query(self, user_query):\n",
        "        self.user_query = user_query\n",
        "        return self.user_query\n",
        "\n",
        "# HANDLING EMPTY ROWS\n",
        "    def set_null_values_list(self, null_values_list):\n",
        "        self.null_values_list = null_values_list\n",
        "        return self.null_values_list\n",
        "\n",
        "    def find_empty_rows(self, hugging_face_dataset):\n",
        "        pandas_df = hugging_face_dataset.to_pandas()\n",
        "        mask = pandas_df.apply(lambda col: col.isin(self.null_values_list)).any(axis=1)\n",
        "        return pandas_df[mask]\n",
        "\n",
        "    def drop_empty_rows(self, hugging_face_dataset):\n",
        "        pandas_df = hugging_face_dataset.to_pandas()\n",
        "        mask = pandas_df.apply(lambda col: col.isin(self.null_values_list)).any(axis=1)\n",
        "        final_df = pandas_df[~mask].reset_index(drop=True)\n",
        "        hugging_face_dataset = Dataset.from_pandas(final_df)\n",
        "        return hugging_face_dataset\n",
        "\n",
        "# FORMATTING DATASET CODE\n",
        "    def convert_input_to_chat_template(self):\n",
        "        message = [\n",
        "            {\"role\": \"system\", \"content\": self.system_instruction},\n",
        "            {\"role\": \"user\", \"content\": self.user_instruction + self.user_query}\n",
        "        ]\n",
        "        formatted_input = self.tokenizer.apply_chat_template(message,\n",
        "                                                                tokenize=False,\n",
        "                                                                add_generation_prompt=True,\n",
        "                                                                return_tensors=\"pt\"\n",
        "                                                            )\n",
        "        return formatted_input\n",
        "\n",
        "    def create_chat_template_dataset(self, example):\n",
        "        self.user_query = example[self.user_query_column]\n",
        "        example['training_input'] = self.convert_input_to_chat_template()\n",
        "        return example\n",
        "\n",
        "    def format_dataset(self, dataset):\n",
        "        if dataset:\n",
        "            formatted_dataset = dataset.map(self.create_chat_template_dataset)\n",
        "            return formatted_dataset\n",
        "\n",
        "# TOKENIZATION CODE\n",
        "    def tokenization_function(self, example):\n",
        "        return self.tokenizer(example[self.columns_to_tokenize],\n",
        "                                padding=True,\n",
        "                                max_length=1024,\n",
        "                                truncation=True,\n",
        "                                return_tensors=\"pt\"\n",
        "                                )\n",
        "\n",
        "    def tokenize_dataset(self, dataset):\n",
        "        if dataset:\n",
        "            tokenized_dataset = dataset.map(self.tokenization_function,\n",
        "                                            batched=True,\n",
        "                                            batch_size=128,\n",
        "                                            num_proc=8\n",
        "                                           )\n",
        "            return tokenized_dataset\n",
        "\n",
        "    def clean_up_dataset(self, base_dataset, tokenized_dataset):\n",
        "        base_dataset_columns = list(base_dataset.features.keys())\n",
        "        tokenized_dataset_columns = list(tokenized_dataset.features.keys())\n",
        "        final_columns = list(set(tokenized_dataset_columns) - set(base_dataset_columns))\n",
        "\n",
        "        final_dataset = tokenized_dataset.remove_columns(base_dataset_columns)\n",
        "        final_dataset.set_format(type='pt', columns=final_columns, output_all_columns=True)\n",
        "        return final_dataset\n",
        "#"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-08-29T14:36:28.039878Z",
          "iopub.execute_input": "2024-08-29T14:36:28.040806Z",
          "iopub.status.idle": "2024-08-29T14:36:28.057954Z",
          "shell.execute_reply.started": "2024-08-29T14:36:28.040764Z",
          "shell.execute_reply": "2024-08-29T14:36:28.056882Z"
        },
        "trusted": true,
        "id": "_gFntOstXwQT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Example DataFrame\n",
        "null_value_list = [None, \"\", pd.NA, float('nan')]\n",
        "\n",
        "data = {\n",
        "    'A': [\"\", \"2\", None, \"4\", \"\", \"1\"],\n",
        "    'B': [\"3\", None, \"7\", \"8\", pd.NA, \"2\"],\n",
        "    'C': [\"9\", \"10\", \"11\", None, pd.NA, \"3\"]\n",
        "}\n",
        "df = pd.DataFrame(data)\n",
        "dataset = Dataset.from_pandas(df)\n",
        "dataset"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-08-29T14:52:40.493840Z",
          "iopub.execute_input": "2024-08-29T14:52:40.494220Z",
          "iopub.status.idle": "2024-08-29T14:52:40.511369Z",
          "shell.execute_reply.started": "2024-08-29T14:52:40.494184Z",
          "shell.execute_reply": "2024-08-29T14:52:40.510534Z"
        },
        "trusted": true,
        "id": "PT7t7n9cXwQU",
        "outputId": "ad7c14db-264c-44cc-ceec-736b46f77966"
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 133,
          "output_type": "execute_result",
          "data": {
            "text/plain": "Dataset({\n    features: ['A', 'B', 'C'],\n    num_rows: 6\n})"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "datahelper = DataHelper(tokenizer=tokenizer,\n",
        "                        user_query_column = \"input\",\n",
        "                        columns_to_tokenize=\"training_input\"\n",
        "                       )\n",
        "\n",
        "datahelper.set_null_values_list([None, \"\", float('nan'), pd.NA])\n",
        "datahelper.find_empty_rows(dataset)\n",
        "datahelper.drop_empty_rows(dataset)\n",
        "\n",
        "datahelper.set_system_instruction(\"You are a helpful AI Assistant\")\n",
        "datahelper.set_user_instruction(\"Please Answer the following Questions:\\n\")\n",
        "datahelper.set_user_query(\"What is 2*2*x derivate\")\n",
        "datahelper.convert_input_to_chat_template()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-08-29T14:53:05.585136Z",
          "iopub.execute_input": "2024-08-29T14:53:05.585555Z",
          "iopub.status.idle": "2024-08-29T14:53:05.606938Z",
          "shell.execute_reply.started": "2024-08-29T14:53:05.585516Z",
          "shell.execute_reply": "2024-08-29T14:53:05.606042Z"
        },
        "trusted": true,
        "id": "AvF4zpYmXwQU",
        "outputId": "680767cd-b6a2-4f36-9c6b-a73d285eac8d"
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 135,
          "output_type": "execute_result",
          "data": {
            "text/plain": "Dataset({\n    features: ['A', 'B', 'C'],\n    num_rows: 1\n})"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "dataset = load_dataset(\"medalpaca/medical_meadow_medqa\")\n",
        "dataset"
      ],
      "metadata": {
        "trusted": true,
        "id": "8c34VDwhXwQU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# EXTRACTING DATASET\n",
        "train_dataset = dataset['train']\n",
        "test_dataset = None\n",
        "validation_dataset = None\n",
        "\n",
        "datasets = [train_dataset, test_dataset, validation_dataset]\n",
        "datasets"
      ],
      "metadata": {
        "trusted": true,
        "id": "1a6pi59IXwQU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# INSTANTIATING DATAHELPER CLASS\n",
        "datahelper = DataHelper(tokenizer=tokenizer,\n",
        "                        user_query_column = \"input\",\n",
        "                        columns_to_tokenize=\"training_input\"\n",
        "                       )\n",
        "\n",
        "datahelper.set_system_instruction(\"You are a helpful AI Assistant\")\n",
        "datahelper.set_user_instruction(\"Please Answer the following Questions:\\n\")\n",
        "datahelper.set_user_query(\"What is 2*2*x derivate\")\n",
        "datahelper.convert_input_to_chat_template()"
      ],
      "metadata": {
        "trusted": true,
        "id": "OZNekBEyXwQV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# FORMATTING DATASET\n",
        "formatted_datasets = [datahelper.format_dataset(dataset) for dataset in datasets]\n",
        "tokenized_datasets = [datahelper.tokenize_dataset(dataset) for dataset in formatted_datasets]\n",
        "\n",
        "train_dataset, test_dataset, validation_dataset = formatted_datasets"
      ],
      "metadata": {
        "trusted": true,
        "id": "kDrnc-9tXwQV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenized_datasets\n",
        "# datahelper.clean_up_dataset(base_dataset=dataset['train'], tokenized_dataset=tokenized_dataset)\n",
        "# train_dataset = dataset['train'].map(preprocessing, batched=True, batch_size=32)\n",
        "# train_dataset = train_dataset.remove_columns(['input', 'instruction', 'output', 'final_text'])\n",
        "# train_dataset.set_format(type='pt', columns=['input_ids', 'attention_mask'], output_all_columns=True)"
      ],
      "metadata": {
        "id": "cFoe5vS4XwQV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with tqdm(total=len(train_dataset.batch(batch_size=8)), desc=\"Generating responses\", unit=\"batch\") as pbar:\n",
        "    for batch in train_dataset.batch(batch_size=8):\n",
        "        batch = {k: v.to(\"cuda:0\") for k, v in batch.items() if isinstance(v, torch.Tensor)}\n",
        "\n",
        "        with torch.no_grad():\n",
        "            output = model.generate(**batch,max_length=1024)\n",
        "            results.append(output)\n",
        "    #         decoded_output = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "    #         results.append(decoded_output)\n",
        "            pbar.update(1)"
      ],
      "metadata": {
        "trusted": true,
        "id": "2XxLrIaJXwQV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Trainer Code"
      ],
      "metadata": {
        "id": "Deu6zQMBXwQV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import Trainer, TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    num_train_epochs=1,\n",
        "    per_device_train_batch_size=28,\n",
        "    save_total_limit=1,\n",
        "    eval_strategy=\"steps\",\n",
        "    save_strategy=\"epoch\",\n",
        "    save_steps = 100,\n",
        "    # label_names = ['not_equivalent', 'equivalent'],\n",
        "    fp16=torch.cuda.is_available()  # Use mixed precision if GPUs support it\n",
        ")\n",
        "\n",
        "# Initialize Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=dataset['train'],\n",
        "    # eval_dataset=dataset['validation'],\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "trainer.train()"
      ],
      "metadata": {
        "jupyter": {
          "source_hidden": true
        },
        "trusted": true,
        "id": "I-1cGgUfXwQV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluate Code"
      ],
      "metadata": {
        "id": "G2xuo_lsXwQV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "import torch\n",
        "import logging\n",
        "\n",
        "class Evaluate:\n",
        "    def __init__(self, tokenizer, model):\n",
        "\n",
        "        logging.getLogger(\"transformers\").setLevel(logging.ERROR)\n",
        "\n",
        "        self.tokenizer = tokenizer\n",
        "        self.model = model\n",
        "        self.streamer = None\n",
        "        self.model.generation_config.pad_token_id = self.tokenizer.pad_token_id\n",
        "        self.model.generation_config.eos_token_id = self.tokenizer.eos_token_id\n",
        "\n",
        "    def single_question_evaluate(self, question, return_prompt=False, stream_response=False):\n",
        "        self.model.eval()\n",
        "        with torch.no_grad():\n",
        "            inputs = self.tokenizer(question, return_tensors=\"pt\").to(\"cuda\")\n",
        "            output = self.model.generate(\n",
        "                **inputs,\n",
        "                max_new_tokens=1000,\n",
        "                return_dict_in_generate=True,\n",
        "                temperature=0.5,\n",
        "                do_sample=True,\n",
        "                top_k=50,\n",
        "                num_beams=1,\n",
        "                early_stopping=False,\n",
        "                eos_token_id=self.tokenizer.eos_token_id,\n",
        "                pad_token_id=self.tokenizer.pad_token_id,\n",
        "                streamer=(self.streamer if stream_response else None),\n",
        "            )\n",
        "\n",
        "            if return_prompt:\n",
        "                response = self.tokenizer.decode(output.sequences[0], skip_special_tokens=True)\n",
        "            else:\n",
        "                response = self.tokenizer.decode(output.sequences[:, inputs.input_ids.shape[1]:][0], skip_special_tokens=True)\n",
        "\n",
        "            return response\n",
        "\n",
        "    def load_streamer(self, streamer):\n",
        "        self.streamer = streamer\n",
        "        return self.streamer\n",
        "\n",
        "    def batch_evaluate(self, dataset, batch_size, return_prompt=False):\n",
        "        pass\n",
        "\n",
        "    def qbq_evaluate(self, dataset, return_prompt=False):\n",
        "        model_responses = []\n",
        "        self.model.eval()\n",
        "        with tqdm(total=len(dataset), desc=\"Generating responses\", unit=\"question\") as pbar:\n",
        "            for index in range(len(dataset)):\n",
        "                with torch.no_grad():\n",
        "                    question = dataset['training_input'][index]\n",
        "                    inputs = self.tokenizer(question, return_tensors=\"pt\").to(\"cuda\")\n",
        "                    output = self.model.generate(\n",
        "                        **inputs,\n",
        "                        max_new_tokens=1000,\n",
        "                        return_dict_in_generate=True,\n",
        "                        temperature=0.5,\n",
        "                        do_sample=True,\n",
        "                        top_k=50,\n",
        "                        num_beams=1,\n",
        "                        early_stopping=False,\n",
        "                    )\n",
        "                    if return_prompt:\n",
        "                        response = self.tokenizer.decode(output.sequences[0], skip_special_tokens=True)\n",
        "                    else:\n",
        "                        response = self.tokenizer.decode(output.sequences[:, inputs.input_ids.shape[1]:][0], skip_special_tokens=True)\n",
        "\n",
        "                    model_responses.append(response)\n",
        "                    pbar.update(1)\n",
        "\n",
        "        return model_responses\n"
      ],
      "metadata": {
        "jupyter": {
          "source_hidden": true
        },
        "trusted": true,
        "id": "gBCs5pEsXwQV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}