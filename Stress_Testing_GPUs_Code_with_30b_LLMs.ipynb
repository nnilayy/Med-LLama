{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [],
      "dockerImageVersionId": 30733,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "name": "Stress Testing GPUs Code with 30b LLMs",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nnilayy/MedGPT/blob/main/Stress_Testing_GPUs_Code_with_30b_LLMs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the read-only \"../input/\" directory\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n",
        "\n",
        "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n",
        "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "id": "zYqXKBUfimgi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "trusted": true,
        "id": "MO4Ks-93imgl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use a pipeline as a high-level helper\n",
        "from transformers import pipeline\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"Who are you?\"},\n",
        "]\n",
        "pipe = pipeline(\"text-generation\", model=\"BioMistral/BioMistral-7B\")\n",
        "# pipe(messages)"
      ],
      "metadata": {
        "trusted": true,
        "id": "Y4ab5_4gimgm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import psutil\n",
        "import platform\n",
        "try:\n",
        "    import GPUtil\n",
        "except ImportError:\n",
        "    print(\"GPUtil module not installed. GPU information will not be available.\")\n",
        "\n",
        "# CPU Information\n",
        "cpus = os.cpu_count()\n",
        "print(\"Logical CPUs:\", cpus)\n",
        "print(\"Physical CPUs:\", psutil.cpu_count(logical=False))\n",
        "\n",
        "# System Memory\n",
        "ram = psutil.virtual_memory()\n",
        "print(\"Total RAM (GB):\", round(ram.total / (1024 ** 3), 2))\n",
        "print(\"Available RAM (GB):\", round(ram.available / (1024 ** 3), 2))\n",
        "print(\"Used RAM (GB):\", round(ram.used / (1024 ** 3), 2))\n",
        "\n",
        "# Disk Information\n",
        "print(\"\\nDisk Information:\")\n",
        "for partition in psutil.disk_partitions():\n",
        "    try:\n",
        "        usage = psutil.disk_usage(partition.mountpoint)\n",
        "        print(f\"  Mountpoint: {partition.mountpoint}\")\n",
        "        print(f\"    Total Size (GB): {round(usage.total / (1024 ** 3), 2)}\")\n",
        "        print(f\"    Used Space (GB): {round(usage.used / (1024 ** 3), 2)}\")\n",
        "        print(f\"    Free Space (GB): {round(usage.free / (1024 ** 3), 2)}\")\n",
        "        print(f\"    Percentage Used: {usage.percent}%\")\n",
        "    except PermissionError:\n",
        "        print(f\"  No Permission to access {partition.mountpoint}\")\n",
        "\n",
        "# GPU Information (if GPUtil is available)\n",
        "if 'GPUtil' in globals():\n",
        "    gpus = GPUtil.getGPUs()\n",
        "    if gpus:\n",
        "        for i, gpu in enumerate(gpus):\n",
        "            print(f\"\\nGPU {i}: {gpu.name}\")\n",
        "            print(f\"  Total VRAM (GB): {round(gpu.memoryTotal / 1024, 2)}\")\n",
        "            print(f\"  Used VRAM (GB): {round(gpu.memoryUsed / 1024, 2)}\")\n",
        "            print(f\"  Free VRAM (GB): {round(gpu.memoryFree / 1024, 2)}\")\n",
        "            print(f\"  GPU Load (%): {gpu.load * 100}\")\n",
        "    else:\n",
        "        print(\"No GPU found or GPUtil cannot find it.\")\n",
        "else:\n",
        "    print(\"GPU information not available due to missing GPUtil.\")\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "QZeNnxLoimgm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "checkpoint  = [\n",
        "    \"BioMistral/BioMistral-7B\",\n",
        "#     \"uygarkurt/llama-3-merged-linear\",\n",
        "#     \"01-ai/Yi-1.5-9B\",\n",
        "#     \"facebook/opt-30b\"\n",
        "]\n",
        "model_to_load = 0\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = AutoModelForCausalLM.from_pretrained(checkpoint[model_to_load],\n",
        "                                             torch_dtype=torch.float16,\n",
        "                                             device_map='auto',\n",
        "                                            )\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint[model_to_load], return_token_type_ids=False)"
      ],
      "metadata": {
        "trusted": true,
        "id": "IvQUGjL2imgm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.backends.cuda.enable_mem_efficient_sdp(False)\n",
        "torch.backends.cuda.enable_flash_sdp(False)"
      ],
      "metadata": {
        "trusted": true,
        "id": "Q3Fh-X1dimgn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Datasets"
      ],
      "metadata": {
        "id": "LLqtuVOlimgn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset"
      ],
      "metadata": {
        "trusted": true,
        "id": "8Sdoitpuimgo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "hugging_face_token = \"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\"\n",
        "notebook_login()"
      ],
      "metadata": {
        "trusted": true,
        "id": "5BmjFUMmimgo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "wandb_api_token = \"1a6a95ba4f084dedd64528953348896560a68bfe\"\n",
        "wandb.login(key = wandb_api_token)"
      ],
      "metadata": {
        "trusted": true,
        "id": "2TYCYB9Fimgp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PubmedQA: https://huggingface.co/datasets/qiaojin/PubMedQA\n",
        "pubmedqa_artificial = load_dataset(\"qiaojin/PubMedQA\", \"pqa_artificial\")\n",
        "pubmedqa_labeled = load_dataset(\"qiaojin/PubMedQA\", \"pqa_labeled\")\n",
        "pubmedqa_unlabeled = load_dataset(\"qiaojin/PubMedQA\", \"pqa_unlabeled\")\n",
        "pubmedqa_artificial"
      ],
      "metadata": {
        "trusted": true,
        "id": "7rCQKJUMimgp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = pubmedqa_artificial['train']\n",
        "dataset = dataset.map(lambda x: {'finalized_context': \" \".join(x['context']['contexts'])})\n",
        "print(dataset)"
      ],
      "metadata": {
        "trusted": true,
        "id": "-T1eMtxximgq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments, DataCollatorWithPadding\n",
        "checkpoint = 'BioMistral/BioMistral-7B'\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "label_to_idx = {'yes': 1, 'no': 0}\n",
        "def encode(examples):\n",
        "    outputs = tokenizer(examples['question'], examples['finalized_context'],truncation=True, padding='max_length', max_length=1024)\n",
        "    outputs['labels'] = [label_to_idx[label] for label in examples['final_decision']]\n",
        "    return outputs\n",
        "\n",
        "dataset = dataset.map(encode, batched=True, batch_size=1000, num_proc=16, keep_in_memory=True)\n",
        "\n",
        "# Set the format for PyTorch\n",
        "dataset = dataset.remove_columns(['pubid', 'question', 'long_answer', 'final_decision','finalized_context', 'context'])\n",
        "dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
        "print(dataset)"
      ],
      "metadata": {
        "trusted": true,
        "id": "vTAXM6uAimgq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.push_to_hub(\"nnilayy/pubmedqa_artificial_preprocessed\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "EAWInF6ximgq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "dataset = load_dataset(\"nnilayy/pubmedqa_artificial_preprocessed\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "Q_06tPFXimgq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments, DataCollatorWithPadding\n",
        "checkpoint = 'BioMistral/BioMistral-7B'\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "example = dataset[0]\n",
        "tokenizer(example['question'], example['finalized_context'],truncation=True, padding='max_length', max_length=1024)"
      ],
      "metadata": {
        "trusted": true,
        "id": "hPEZsdVLimgr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments, DataCollatorWithPadding\n",
        "import torch\n",
        "checkpoint = 'BioMistral/BioMistral-7B'\n",
        "model = AutoModelForCausalLM.from_pretrained(checkpoint,\n",
        "                                             torch_dtype=torch.float16,\n",
        "                                             device_map='auto',\n",
        "                                             num_labels=2\n",
        "                                            )\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    logits = torch.from_numpy(logits)\n",
        "    labels = torch.from_numpy(labels)\n",
        "\n",
        "    predictions = torch.argmax(logits, dim=-1)\n",
        "    accuracy = (predictions == labels).float().mean()\n",
        "    return {'accuracy': accuracy.item()}\n",
        "\n",
        "data_collator = DataCollatorWithPadding(tokenizer = tokenizer)\n",
        "\n",
        "# Training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir ='./results',\n",
        "    run_name = 'run_8',\n",
        "    num_train_epochs = 1,\n",
        "    per_device_train_batch_size = 2,\n",
        "    save_total_limit = 1,\n",
        "    eval_strategy = \"epoch\",\n",
        "    save_strategy = \"epoch\",\n",
        "#     label_names = ['no', 'yes'],\n",
        "    load_best_model_at_end = False,\n",
        "    fp16 = torch.cuda.is_available()\n",
        ")\n",
        "\n",
        "\n",
        "trainer = Trainer(\n",
        "    model = model,\n",
        "    args = training_args,\n",
        "    train_dataset = dataset,\n",
        "#     eval_dataset = dataset['validation'],\n",
        "    data_collator = data_collator,\n",
        "    compute_metrics = compute_metrics\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "# evaluation_results = trainer.evaluate()\n",
        "# print(evaluation_results)"
      ],
      "metadata": {
        "trusted": true,
        "id": "GOG_k9syimgr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# MedMCQA: https://huggingface.co/datasets/openlifescienceai/medmcqa\n",
        "medmcqa = load_dataset(\"openlifescienceai/medmcqa\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "o7kkPnWXimgr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "medmcqa_train = medmcqa['train']\n",
        "medmcqa_test = medmcqa['test']\n",
        "medmcqa_validation = medmcqa['validation']\n",
        "medmcqa"
      ],
      "metadata": {
        "trusted": true,
        "id": "cHoBj8vOimgr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# MedQA: https://huggingface.co/datasets/GBaker/MedQA-USMLE-4-options-hf\n",
        "medqa = load_dataset(\"GBaker/MedQA-USMLE-4-options-hf\")\n",
        "medqa_train = medqa['train']\n",
        "medqa_test = medqa['test']\n",
        "medqa_validation = medqa['validation']\n",
        "medqa_test"
      ],
      "metadata": {
        "trusted": true,
        "id": "89JO5S7pimgr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inferencing Code"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-19T04:12:15.253782Z",
          "iopub.execute_input": "2024-07-19T04:12:15.254065Z",
          "iopub.status.idle": "2024-07-19T04:12:15.260807Z",
          "shell.execute_reply.started": "2024-07-19T04:12:15.25404Z",
          "shell.execute_reply": "2024-07-19T04:12:15.259965Z"
        },
        "id": "L1Uh8Q9simgr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# pubmedqa_artificial['train'][0]\n",
        "pubmedqa_artificial['train'].features"
      ],
      "metadata": {
        "trusted": true,
        "id": "y4pPDNz7imgs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pubmedqa_artificial['train'][0]\n",
        "pubmedqa_artificial['train'][0]['final_decision']"
      ],
      "metadata": {
        "trusted": true,
        "id": "F04i-9Tmimgs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "index = 121\n",
        "question = pubmedqa_artificial['train'][index]['question']\n",
        "context = \" \".join(pubmedqa_artificial['train'][index]['context']['contexts'])\n",
        "answer = pubmedqa_artificial['train'][index]['final_decision']\n",
        "\n",
        "print(f\"\"\"\n",
        "Question: {question}\n",
        "Answer: {answer}\n",
        "\"\"\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "zHJMiL6Dimgs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_text = f\"\"\"\n",
        "You Are a Soldier Going to Write an Exam. You have to read the Question very Properly and Answer the Question,\n",
        "to your best Knowledge. If you fail to answer anything apart from \"yes\", \"no\", \"maybe\" options\n",
        "Your Entire Family Dies. Answer the Following Question with just one word. Remember Just One Word\n",
        "\n",
        "Here are Four examples as to how you should answer the mentioned Question:\n",
        "Question: Are group 2 innate lymphoid cells ( ILC2s ) increased in chronic rhinosinusitis with nasal polyps or eosinophilia?\n",
        "Options:\n",
        "(a)yes\n",
        "(b)no\n",
        "(c)maybe\n",
        "Answer: (a)yes\n",
        "\n",
        "Question: Does vagus nerve contribute to the development of steatohepatitis and obesity in phosphatidylethanolamine N-methyltransferase deficient mice?\n",
        "Options:\n",
        "(a)yes\n",
        "(b)no\n",
        "(c)maybe\n",
        "Answer: (a)yes\n",
        "\n",
        "Question: Does 1+2=2?\n",
        "Options:\n",
        "(a)yes\n",
        "(b)no\n",
        "(c)maybe\n",
        "Answer: (b)no\n",
        "\n",
        "Question: Does Pythagoras Theorem indicate that in a Right Angled Triange a^2+b^2=c^2 where c is the hypoteneuse?\n",
        "Options:\n",
        "(a)yes\n",
        "(b)no\n",
        "(c)maybe\n",
        "Answer: (a)yes\n",
        "\n",
        "Now Like the Above question, Answer the following question with just one answer, yes, no maybe to your correct knowledge\n",
        "\n",
        "Question: {question}\n",
        "Options:\n",
        "(a)yes\n",
        "(b)no\n",
        "(c)maybe\n",
        "\"\"\"\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "# Example input\n",
        "encoded_input = tokenizer.encode(input_text, return_tensors='pt').to(device)\n",
        "output_sequences = model.generate(\n",
        "    pad_token_id=tokenizer.pad_token_id,\n",
        "    input_ids=encoded_input,\n",
        "    do_sample=True,\n",
        "    max_length=1024,\n",
        "    num_return_sequences=1\n",
        ")\n",
        "\n",
        "generated_text = tokenizer.decode(output_sequences[0], skip_special_tokens=True)\n",
        "output = generated_text.replace(input_text, \"\")\n",
        "print(output)"
      ],
      "metadata": {
        "trusted": true,
        "id": "IMiHgXdNimgs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "hugging_face_token = \"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\"\n",
        "notebook_login()"
      ],
      "metadata": {
        "trusted": true,
        "id": "oAyT_15Fimgs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.push_to_hub('nnilayy/biomistral-unmodified')\n",
        "tokenizer.push_to_hub('nnilayy/biomistral-unmodified')"
      ],
      "metadata": {
        "trusted": true,
        "id": "2wbbe1WMimgs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Clear GPU (Maybe)\n",
        "import torch\n",
        "# del model\n",
        "# del tokenizer\n",
        "# torch.cuda.empty_cache()\n",
        "\n",
        "\n",
        "# Clear CPU's Memory\n",
        "import gc\n",
        "gc.collect()"
      ],
      "metadata": {
        "trusted": true,
        "id": "BmiQulCAimgs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading model on CPUs would crash them, instead loaad them on GPUs, using the parameter\n",
        "# device_map =\"cuda\"/\"auto\" in loading pretrained model\n",
        "# By loading model onto gpus, even if we load multiple models onto gpus, the notebook wouldnt crash but just instead free upt the space which was occupied by the previous model\n",
        "\n",
        "# When you are loading a model, you can load the model either on a cpu or a gpu. After loading these models would\n",
        "# persist in the cpu/gpu and not\n",
        "\n",
        "# When device is mapped to auto, it would try to load mjaority of the model on the gpus, but once gpus\n",
        "# reach their limit the rest of model is loaded onto the cpu, but if the model is still big enoughy\n",
        "# the cpu's ram would be filled completely and the notebook would crash, ultimately being not able to load the\n",
        "# model into memory\n",
        "\n",
        "\n",
        "# Interesting Observation, On the first load of the 30 billion paramter model into memory, the notebook crashed\n",
        "# on the second load of the model, the model loaded perfectly into the memory, without using any of the CPU's ram\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "wNRCpfl2imgt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}