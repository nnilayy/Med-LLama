{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30761,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install evaluate","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from datasets import load_dataset\npubmedqa = load_dataset(\"nnilayy/pubmedqa-qoccl\")\nmedmcqa = load_dataset(\"nnilayy/medmcqa-qoccs\")\nmedqa = load_dataset(\"nnilayy/medqa-qoc\")\nmmlu_medical = load_dataset(\"nnilayy/mmlu-medical-qoc\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nhugging_face_api_key = user_secrets.get_secret(\"HUGGING_FACE_API_KEY\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\ncheckpoint = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\ntokenizer = AutoTokenizer.from_pretrained(checkpoint, \n                                          padding_side=\"left\",\n                                          token=hugging_face_api_key\n                                         )\ntokenizer.pad_token = tokenizer.eos_token\n# model = AutoModelForCausalLM.from_pretrained(checkpoint, \n#                                              device_map=\"auto\",\n#                                              torch_dtype=torch.float16,\n#                                              token=hugging_face_api_key,\n#                                             )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\n\nclass CustomDataset(torch.utils.data.Dataset):\n    def __init__(self, dataset):\n        self.dataset = dataset\n    \n    def __len__(self):\n        return len(self.dataset)\n    \n    def __getitem__(self, idx):\n        # Convert lists or numpy arrays to tensors\n        item = {\n            'input_ids': torch.tensor(self.dataset['input_ids'][idx]),\n            'attention_mask': torch.tensor(self.dataset['attention_mask'][idx]),\n            'labels': self.dataset['Correct Answer'][idx],\n        }        \n        return item","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tqdm import tqdm \nfrom evaluate import load\nfrom torch.utils.data import DataLoader\n\n\nclass DataHelper:\n    def __init__(self):\n        self.datasets_dict = None        \n        self.current_datasets_dict = {}\n        self.formatted_datasets_dict = {}\n        self.tokenized_datasets_dict = {}\n\n        self.dataloaders_dict = {}\n\n        self.tokenizer = None\n#         self.tokenizer.pad_token = None\n\n        self.system_instruction = \"You are a Helpful AI Assistant.\"\n        self.user_instruction = \"Please answer the following Question: \"\n        self.user_query = None\n        \n        #datasets configurations\n        self.batch_size = None\n        self.shuffle = None\n        self.max_length = None\n        self.return_tensors = None\n        self.padding = None\n        self.truncation = None\n        \n        # Config Columns\n        self.user_query_column = None        \n        self.column_to_tokenize = None  \n        \n# LOADING DATASETS DICT\n    def load_datasets_dict(self, datasets_dict):\n        self.datasets_dict = datasets_dict\n        self.current_datasets_dict = self.datasets_dict        \n        return self.datasets_dict\n\n# LOADING DATASET CONFIGURATION\n    def set_dataset_config(self, dataset_configuration):\n        self.batch_size = dataset_configuration['batch_size']\n        self.shuffle = dataset_configuration['shuffle']\n        self.max_length = dataset_configuration['max_length']\n        self.return_tensors = dataset_configuration['return_tensors']\n        self.padding = dataset_configuration['padding']\n        self.truncation = dataset_configuration['truncation']\n        self.column_to_tokenize = dataset_configuration['column_to_tokenize']\n\n# LOADING TOKENIZER\n    def load_tokenizer(self, tokenizer):\n        self.tokenizer = tokenizer\n        return self.tokenizer        \n\n# SYSTEM & USER PROMPT\n    def set_system_instruction(self, system_instruction):\n        self.system_instruction = system_instruction.strip()\n        return self.system_instruction\n    \n    def set_user_instruction(self, user_instruction):\n        self.user_instruction = user_instruction.strip()\n        return self.user_instruction\n\n# CONVERTING DATASETS TO DATALOADER\n    def datasets_to_dataloader(self):\n        if self.tokenized_datasets_dict:\n            self.current_datasets_dict = self.tokenized_datasets_dict\n        elif self.formatted_datasets_dict:\n            self.current_datasets_dict = self.formatted_datasets_dict\n        else:\n            self.current_datasets_dict = self.datasets_dict\n\n        for dataset_name, dataset in self.current_datasets_dict.items():\n            dataset = CustomDataset(dataset)\n            dataloader = DataLoader(dataset, batch_size=self.batch_size, shuffle=self.shuffle)\n            self.dataloaders_dict.update({dataset_name+\"_dataloader\":dataloader})\n        return self.dataloaders_dict\n    \n# TOKENIZATION CODE    \n    def tokenization_function(self, example):\n        return self.tokenizer(text=example[self.column_to_tokenize],\n                              padding=self.padding,\n                              truncation=self.truncation,\n                              max_length=self.max_length,\n                              return_tensors = self.return_tensors,\n                             )\n    \n    def tokenize_datasets(self):\n        for dataset_name, dataset in self.current_datasets_dict.items():\n            tokenized_dataset = dataset.map(self.tokenization_function, batched = True)\n            self.tokenized_datasets_dict.update({dataset_name+\"_tokenized\":tokenized_dataset}) \n        return self.tokenized_datasets_dict    \n    \n    def remove_columns(self):\n        dataset_name = list(self.datasets_dict.keys())[0]\n        base_dataset_columns = list(self.datasets_dict[dataset_name].features.keys())\n        tokenized_dataset_columns = list(self.current_datasets_dict[dataset_name].features.keys())\n        final_columns = list(set(tokenized_dataset_columns) - set(base_dataset_columns))\n        pass","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from helper_classes.datahelper import DataHelper\nclass CustomDataHelper(DataHelper):\n    def __init__(self):\n        super().__init__()\n        self.current_question = None\n        self.current_options = None\n\n    def processing_fn_format(self, example):\n        if self.tokenizer:\n            self.current_question = example['Question']\n            self.current_options = example['Options']\n            chat_message = [\n                {\"role\": \"system\", \"content\": self.system_instruction},\n                {\"role\": \"user\", \"content\": self.user_instruction + \"\\nQuestion: \" + str(self.current_question) + \"\\nOptions:\\n\"+ str(self.current_options)}\n            ]            \n            example['message'] = self.tokenizer.apply_chat_template(chat_message,\n                                                                    tokenize=False,\n                                                                    add_generation_prompt=True,\n                                                                    return_tensors = self.return_tensors\n                                                                   )\n            return example\n        else:\n            raise Exception(\"Tokenizer has not been provided. Please load a tokenizer\")\n    \n    def format_datasets(self):\n        self.current_datasets_dict = self.datasets_dict\n        for dataset_name, dataset in self.current_datasets_dict.items():\n            formatted_dataset = dataset.map(self.processing_fn_format)\n            self.formatted_datasets_dict.update({dataset_name+\"_formatted\":formatted_dataset})\n            self.current_datasets_dict = self.formatted_datasets_dict\n        return self.current_datasets_dict","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"datasets_dict = {\n    \"anatomy\": mmlu_medical['anatomy'],\n    \"clinical_kg\": mmlu_medical['clinical_kg'],\n    \"college_biology\": mmlu_medical['college_biology']\n}\n\ndataset_configuration = {\n    \"batch_size\": 1,\n    \"shuffle\": True,\n    \"return_tensors\": \"pt\",\n    \"max_length\":300,\n    \"padding\": \"max_length\",\n    \"truncation\": True,\n    \"column_to_tokenize\": \"message\",\n}\n\n# data_helper = CustomDataHelper()\ndata_helper = CustomDataHelper()\ndata_helper.set_system_instruction(\"\"\"\nYou are a USMLE Passed Medical Doctor and a Helpful AI-Assitant.\n\"\"\")\n\ndata_helper.set_user_instruction(\"\"\"\nYou will be presented queries in the format of Multiple Choice Question and Answer. \nUnderstand the question and depth and from the mentioned \"Option\" select the one that is \nseems to be the correct Answer. Do not provide any explanation and just return \nthe answer as \"Final Answer: (A/B/C/D)\".\n\"\"\")\n\ndata_helper.load_datasets_dict(datasets_dict)\ndata_helper.set_dataset_config(dataset_configuration)\n\ndata_helper.load_tokenizer(tokenizer)\n\ndata_helper.format_datasets()\ndata_helper.tokenize_datasets()\n# dataloader = data_helper.datasets_to_dataloader()\n# dataloader","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tqdm import tqdm \nfrom evaluate import load\nfrom torch.utils.data import DataLoader\n\ndemo_dataset = CustomDataset(data_helper.tokenized_datasets_dict['anatomy_formatted_tokenized'])\ndataloader = DataLoader(demo_dataset, batch_size=10, shuffle=False)\n\nmodel.eval()\nlabels, all_logits = [], []\nfor batch in tqdm(dataloader, desc=\"Evaluating\"):\n    inputs = {k:v.to(\"cuda\") for k,v in batch.items() if k!=\"labels\"}\n    with torch.no_grad():\n        outputs = model.generate(**inputs,\n                                 max_new_tokens=300,\n                                 pad_token_id=tokenizer.eos_token_id\n                                )\n#     sent = tokenizer.batch_decode(sequences=outputs, skip_special_tokens=True)\n    sent = tokenizer.batch_decode(sequences=outputs[:, inputs['input_ids'].shape[1]:], \n                                  skip_special_tokens=True)\n    break\n    \n#     logits = outputs.logits\n#     predictions = torch.argmax(logits, dim=-1)\n#     all_logits.append(predictions)\n#     labels.append(inputs['labels'])\n\n# labels = torch.cat(labels, dim=0)\n# all_logits = torch.cat(all_logits, dim=0)\n\n# accuracy.compute(predictions = all_logits, references = labels)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Evaluator","metadata":{}},{"cell_type":"code","source":"from tqdm import tqdm \nclass Evaluator:\n    def __init__(self):\n        self.models_dict = None\n        self.metrics_dict = None\n        self.datasets_dict = None\n\n        self.device = None\n        self.current_model = None\n        self.current_model_name = None\n        self.current_dataset = None\n        self.current_dataset_name = None\n        \n        self.labels = None\n        self.all_logits = None\n\n# LOADING FUNCTIONS\n    def load_models_dict(self, models_dict):\n        self.models_dict = models_dict\n        return self.models_dict\n           \n    def set_device(self, device):\n        self.device = device\n        return self.device\n\n    def load_datasets_dict(self, datasets_dict):\n        self.datasets_dict = datasets_dict\n        return self.datasets_dict\n    \n    def load_metrics_dict(self, metrics_dict):\n        self.metrics_dict = metrics_dict\n        return self.metrics_dict\n    \n    \n# METRICS COMPUTATION\n    def compute_metrics(self):\n        computed_metrics = {}\n        for _, metric in self.metrics_dict.items():\n            result = metric.compute(predictions = self.all_logits, references = self.labels)\n            computed_metrics.update(result)\n        return computed_metrics\n\n\n    def evaluate_qbq(self):\n        self.current_model.eval()\n        all_logits = []\n        labels = []\n        for index in tqdm(range(len(self.current_dataset)), desc=f\"Evaluating {self.current_model_name} on {self.current_dataset_name}\"):\n            input_ids = self.current_dataset['input_ids'][index].unsqueeze(0).to(self.device)\n            attention_mask = self.current_dataset['attention_mask'][index].unsqueeze(0).to(self.device)  \n            token_type_ids = self.current_dataset['token_type_ids'][index].unsqueeze(0).to(self.device)\n            with torch.no_grad():\n                outputs = self.current_model(input_ids = input_ids,\n                                             attention_mask = attention_mask,\n                                             token_type_ids = token_type_ids\n                                            )\n            logits = outputs.logits\n            predictions = torch.argmax(logits, dim=-1)[0]\n            label = self.current_dataset['labels'][index].to(self.device)\n            \n            all_logits.append(predictions)\n            labels.append(label)\n        self.all_logits = all_logits\n        self.labels = labels\n        \n        evaluated_metrics = self.compute_metrics()\n        return evaluated_metrics\n\n\n    def evaluate_batch(self):\n        self.current_model.eval()\n        all_logits = []\n        for batch in tqdm(self.current_dataset, desc=\"Evaluating\"):\n            inputs = {k:v.to(self.device) for k,v in batch.items()}\n            with torch.no_grad():\n                outputs = self.current_model(**inputs)\n            logits = outputs.logits\n            all_logits.append(logits)\n        self.all_logits = all_logits        \n        evaluated_metrics = self.compute_metrics()\n        return evaluated_metrics\n\n\n    def evaluate_models(self):\n        evaluation_results = {}\n        for model_name, model in self.models_dict.items():\n            self.current_model = model\n            self.current_model_name = model_name\n            result = self.evaluate_qbq()\n            evaluation_results.update({model_name: result})\n        return evaluation_results\n\n\n    def evaluate_datasets(self):\n        evaluation_results = {}\n        for dataset_name, dataset in self.datasets_dict.items():\n            self.current_dataset = dataset\n            self.current_dataset_name = dataset_name\n            result = self.evaluate_models()\n            evaluation_results.update({dataset_name: result})\n        return evaluation_results\n    ","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tqdm import tqdm \nfrom itertools import chain\nclass Evaluator:\n    def __init__(self):\n        self.models_dict = None\n        self.metrics_dict = None\n        self.datasets_dict = None\n\n        self.device = None\n        self.current_model = None\n        self.current_model_name = None\n        self.current_dataset = None\n        self.current_dataset_name = None\n        \n        self.all_labels = None\n        self.all_logits = None\n        self.logits_and_labels = {}\n        \n        self.model_generation_type = None        \n\n        #Generation Configurations\n        self.tokenizer = None\n        self.max_new_tokens = None\n        self.pad_token_id = None\n\n# LOADING FUNCTIONS\n    def load_models_dict(self, models_dict):\n        self.models_dict = models_dict\n        return self.models_dict\n           \n    def set_device(self, device):\n        self.device = device\n        return self.device\n\n    def load_datasets_dict(self, datasets_dict):\n        self.datasets_dict = datasets_dict\n        return self.datasets_dict\n    \n    def load_metrics_dict(self, metrics_dict):\n        self.metrics_dict = metrics_dict\n        return self.metrics_dict\n    \n    def set_model_generation_type(self, model_generation_type):\n        self.model_generation_type = model_generation_type\n        return self.model_generation_type\n    \n    def set_generation_config(self, generation_config):\n        self.tokenizer = generation_config['tokenizer']\n        self.max_new_tokens = generation_config['max_new_tokens']\n        self.pad_token_id = generation_config['pad_token_id']\n        \n        \n# METRICS COMPUTATION\n    def compute_metrics(self):\n        if self.metrics_dict:\n            computed_metrics = {}\n            for _, metric in self.metrics_dict.items():\n                result = metric.compute(predictions = self.all_logits, references = self.labels)\n                computed_metrics.update(result)\n            return computed_metrics\n\n# PROCESSING FUNCTIONS TO FORMAT AND CLEAN LOGITS\n    def process_logits_labels(self):\n        \n        self.all_logits = list(chain.from_iterable(self.all_logits))\n        self.all_labels = list(chain.from_iterable(self.all_labels))\n        \n        logits_labels_dict_name = self.current_model_name+\"-\"+self.current_dataset_name+\"-logits-labels\"\n        self.logits_and_labels.update({\n            logits_labels_dict_name:{\n            \"logits\":self.all_logits,\n            \"labels\":self.all_labels\n            }})\n# EVALUATING\n#     def evaluate_qbq(self):\n#         self.current_model.eval()\n#         all_logits = []\n#         labels = []\n#         for index in tqdm(range(len(self.current_dataset)), desc=f\"Evaluating {self.current_model_name} on {self.current_dataset_name}\"):\n#             input_ids = self.current_dataset['input_ids'][index].unsqueeze(0).to(self.device)\n#             attention_mask = self.current_dataset['attention_mask'][index].unsqueeze(0).to(self.device)  \n#             token_type_ids = self.current_dataset['token_type_ids'][index].unsqueeze(0).to(self.device)\n#             with torch.no_grad():\n#                 outputs = self.current_model(input_ids = input_ids,\n#                                              attention_mask = attention_mask,\n#                                              token_type_ids = token_type_ids\n#                                             )\n#             logits = outputs.logits\n#             predictions = torch.argmax(logits, dim=-1)[0]\n#             label = self.current_dataset['labels'][index].to(self.device)\n            \n#             all_logits.append(predictions)\n#             labels.append(label)\n#         self.all_logits = all_logits\n#         self.labels = labels\n        \n#         evaluated_metrics = self.compute_metrics()\n#         return evaluated_metrics\n\n\n    def evaluate(self):\n        all_labels, all_logits = [], []\n\n        self.current_model.eval()        \n        for batch in tqdm(self.current_dataset, desc=f\"Evaluating {self.current_model_name} on {self.current_dataset_name}\"):\n            inputs = {k: v.to(self.device) for k, v in batch.items() if k != 'labels'}\n            \n            with torch.no_grad():\n                if self.model_generation_type == \"default\":\n                    outputs = self.current_model(**inputs)\n                    logits = outputs.logits\n                elif self.model_generation_type == \"generate\":\n                    outputs = self.current_model.generate(**inputs,\n                                                          max_new_tokens = self.max_new_tokens,\n                                                          pad_token_id =  self.pad_token_id,)\n                    logits = self.tokenizer.batch_decode(sequences=outputs[:, inputs['input_ids'].shape[1]:],\n                                                         skip_special_tokens=True)\n                else:\n                    raise Exception(\"Not a Valid Model Generation Type. Please set a valid generation type.\")\n\n            labels = batch['labels']\n            all_logits.append(logits)\n            all_labels.append(labels) \n        \n        self.all_logits = all_logits\n        self.all_labels = all_labels\n        \n        #Process logits and labels before evaluation\n        self.process_logits_labels()\n        \n        # Calculate Metrics\n        return self.compute_metrics()\n\n\n    def evaluate_models(self):\n        if not self.models_dict:\n            raise Exception(\"No Models were provided. Please Provide a Model\")\n        evaluation_results = {}\n        for model_name, model in self.models_dict.items():\n            self.current_model = model\n            self.current_model_name = model_name\n            result = self.evaluate()\n            evaluation_results.update({model_name: result})\n        return evaluation_results\n\n\n    def evaluate_datasets(self):\n        if not self.datasets_dict:\n            raise Exception(\"No Datasets were provided. Please Provide a Dataset\")\n        evaluation_results = {}\n        for dataset_name, dataset in self.datasets_dict.items():\n            self.current_dataset = dataset\n            self.current_dataset_name = dataset_name\n            result = self.evaluate_models()\n            evaluation_results.update({dataset_name: result})\n        return evaluation_results\n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# LOADING MODELS\nfrom transformers import set_seed\nfrom peft import PeftConfig, PeftModelForSequenceClassification\n\nbase_model_id = \"bert-base-uncased\"\nfine_tuned_model_id = \"/kaggle/working/test-model-5/\"\nbase_model = AutoModelForSequenceClassification.from_pretrained(base_model_id).to(\"cuda\")\nfine_tuned_model = AutoModelForSequenceClassification.from_pretrained(fine_tuned_model_id).to(\"cuda\")\n\n# LOADING METRICS\nfrom evaluate import load\naccuracy = load(\"accuracy\")\nf1 = load(\"f1\")\nrecall = load(\"recall\")\n","metadata":{"jupyter":{"source_hidden":true}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"models_dict = {\n    'llama':model,\n}\n\nmetrics_dict = None\n\ndataloader_dict = {\n    \"demo-1\":dataloader,\n    \"demo-2\":dataloader,\n}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"evaluator = Evaluator()\nevaluator.load_models_dict(models_dict)\nevaluator.load_metrics_dict(metrics_dict)\nevaluator.load_datasets_dict(dataloader_dict)\n\nevaluator.set_device(\"cuda\")\nevaluator.set_model_generation_type(\"generate\")\nevaluator.set_generation_config({\n    'tokenizer':tokenizer,\n    'max_new_tokens':300,\n    'pad_token_id': tokenizer.eos_token_id\n})\n\nevaluator.evaluate_datasets()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"evaluator.logits_labels_dict","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}